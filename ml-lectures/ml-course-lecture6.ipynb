{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25293088-revised",
   "metadata": {},
   "source": [
    "# Лекция 6: Основы Supervised Learning и Линейная Регрессия\n",
    "\n",
    "**Цели лекции:**\n",
    "1.  Сформировать представление об основных парадигмах машинного обучения и месте обучения с учителем.\n",
    "2.  Детально изучить стандартный жизненный цикл ML-проекта (CRISP-DM).\n",
    "3.  Понять важность разделения данных и масштабирования признаков как ключевых этапов подготовки данных.\n",
    "4.  Глубоко изучить математические основы и интерпретацию метода линейной регрессии.\n",
    "5.  Освоить концепцию функции потерь (MSE) и познакомиться с основными метриками качества регрессионных моделей (MAE, RMSE, R²)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b5e914",
   "metadata": {},
   "source": [
    "## 1. Карта мира Machine Learning\n",
    "\n",
    "Прежде чем углубляться в конкретный метод, давайте посмотрим на машинное обучение (ML) с высоты птичьего полета, чтобы вы понимали, где мы находиемся и куда движемся. \n",
    "\n",
    "**Машинное обучение** — это область искусственного интеллекта, которая даёт компьютерам способность обучаться на основе данных, выявлять в них закономерности и принимать решения без явного программирования под каждую конкретную задачу.\n",
    "\n",
    "### 1.1. Классификация методов ML\n",
    "\n",
    "Все методы можно условно разделить на три большие парадигмы:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde011f0",
   "metadata": {},
   "source": [
    "#### 1.1.1. Обучение с учителем (Supervised Learning)\n",
    "\n",
    "*   **Суть:** У нас есть набор данных, где для каждого объекта (**признаки**, `X`) известен правильный ответ (**целевая переменная**, `y`). Модель, как прилежный ученик, учится на этих примерах находить зависимость `y ≈ f(X)`.\n",
    "*   **Аналогия:** Ученику дают стопку фотографий животных (`X`) с подписанными названиями (`y`). После обучения он должен уметь называть животное на новой, ранее не виданной фотографии.\n",
    "\n",
    "**Основные задачи Supervised Learning:**\n",
    "\n",
    "1.  **Регрессия (Regression):**\n",
    "    *   **Цель:** Предсказать **непрерывную** (числовую) целевую переменную `y`.\n",
    "    *   **Примеры:**\n",
    "        *   Предсказание цены квартиры по её площади, району, числу комнат.\n",
    "        *   Прогнозирование температуры воздуха на завтра.\n",
    "        *   Оценка спроса на товар в следующем месяце.\n",
    "    *   **Наше место на карте:** **Это именно то, чем мы занимаемся сегодня.**\n",
    "\n",
    "2.  **Классификация (Classification):**\n",
    "    *   **Цель:** Предсказать **категориальную** целевую переменную `y` (метку класса).\n",
    "    *   **Примеры:**\n",
    "        *   Определить, является ли email спамом или нет (2 класса).\n",
    "        *   Распознать цифру на изображении (10 классов: от 0 до 9).\n",
    "        *   Диагностировать заболевание по медицинским анализам (несколько классов: здоров, болен типом А, болен типом Б)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a2d3ae",
   "metadata": {},
   "source": [
    "#### 1.1.2. Обучение без учителя (Unsupervised Learning)\n",
    "\n",
    "*   **Суть:** У нас есть только данные об объектах (`X`), без каких-либо \"правильных ответов\". Модель сама пытается найти в данных внутренние структуры, скрытые закономерности или аномалии.\n",
    "*   **Аналогия:** Ученику дают ту же стопку фотографий животных, но без подписей. Он не знает, кто такие \"кошки\" или \"собаки\", но может рассортировать их на группы похожих друг на друга существ.\n",
    "\n",
    "**Основные задачи Unsupervised Learning:**\n",
    "\n",
    "1.  **Кластеризация (Clustering):**\n",
    "    *   **Цель:** Разделить все объекты на группы (кластеры) так, чтобы объекты в одной группе были максимально похожи, а объекты из разных групп — максимально различны.\n",
    "    *   **Пример:** Сегментация клиентов интернет-магазина по их поведению для проведения целевых маркетинговых кампаний.\n",
    "\n",
    "2.  **Понижение размерности (Dimensionality Reduction):**\n",
    "    *   **Цель:** Уменьшить количество признаков, сохранив при этом как можно больше полезной информации.\n",
    "    *   **Пример:** Визуализация многомерных данных на 2D-графике или сжатие изображений."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c47d1e",
   "metadata": {},
   "source": [
    "#### 1.1.3. Обучение с подкреплением (Reinforcement Learning)\n",
    "\n",
    "*   **Суть:** Модель (агент) учится, взаимодействуя с некой средой. За правильные действия она получает \"награду\" (reward), за неправильные — \"штраф\" (penalty). Цель агента — выработать стратегию поведения, которая максимизирует итоговую награду.\n",
    "*   **Аналогия:** Дрессировка собаки. За выполненную команду (\"сидеть\") она получает лакомство (награда), за невыполненную — ничего.\n",
    "\n",
    "**Примеры:**\n",
    "*   Обучение автопилота для автомобиля.\n",
    "*   Создание алгоритмов для игры в шахматы или го (AlphaGo).\n",
    "*   Оптимизация управления роботом-манипулятором на заводе."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9521b8c",
   "metadata": {},
   "source": [
    "## 2. Жизненный цикл ML-проекта (на основе CRISP-DM)\n",
    "\n",
    "Любой серьезный ML-проект — это не хаотичный набор действий, а структурированный, итеративный процесс. Одной из самых популярных методологий его организации является **CRISP-DM (Cross-Industry Standard Process for Data Mining)**. Она помогает обеспечить предсказуемость, управляемость и высокое качество конечного продукта."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3675dd-revised",
   "metadata": {},
   "source": [
    "#### Этапы жизненного цикла:\n",
    "\n",
    "**1. Business Understanding (Понимание бизнес-задачи):**\n",
    "*   **Вопросы:** Какую проблему мы решаем для бизнеса? Как выглядит успех? Какими метриками его можно измерить?\n",
    "*   **Пример:** Задача не \"построить модель регрессии\", а \"создать сервис для риэлторов, который прогнозирует рыночную стоимость квартиры с точностью до 10%, чтобы ускорить процесс оценки\".\n",
    "*   **Результат:** Четко сформулированная цель проекта и критерии успеха.\n",
    "\n",
    "**2. Data Understanding (Анализ данных):**\n",
    "*   **Действия:** Сбор данных, первичное исследование (EDA), статистики, распределения, корреляции, визуализации.\n",
    "*   **Цель:** Понять, что за данные у нас есть, какого они качества, есть ли в них аномалии и достаточно ли их для решения задачи.\n",
    "\n",
    "**3. Data Preparation (Подготовка данных):**\n",
    "*   **Важность:** Это самый трудоемкий этап, занимающий до 80% времени проекта.\n",
    "*   **Действия:**\n",
    "    *   *Очистка:* обработка пропусков, удаление дубликатов, работа с выбросами.\n",
    "    *   *Инжиниринг признаков (Feature Engineering):* создание новых, более информативных признаков из существующих.\n",
    "    *   *Отбор признаков (Feature Selection):* выбор наиболее значимых признаков.\n",
    "    *   *Преобразование данных:* кодирование категориальных признаков, **масштабирование числовых (о чем мы поговорим подробнее)**.\n",
    "\n",
    "**4. Modeling (Моделирование):**\n",
    "*   **Действия:** Выбор алгоритма, разделение данных на обучающую и тестовую выборки, обучение модели, подбор гиперпараметров.\n",
    "\n",
    "**5. Evaluation (Оценка модели):**\n",
    "*   **Действия:** Оценка качества обученной модели на отложенной выборке с помощью технических метрик и сравнение их с бизнес-критериями.\n",
    "\n",
    "**6. Deployment (Внедрение):**\n",
    "*   **Действия:** Интеграция готовой модели в существующую IT-инфраструктуру.\n",
    "\n",
    "**Важно:** Этот процесс **итеративен**. Часто после этапа оценки приходится возвращаться к подготовке данных или даже к пониманию бизнес-задачи, чтобы улучшить результат."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454ff012-8de7-4f9d-bac6-0ee711aa6454-revised",
   "metadata": {},
   "source": [
    "### 2.1. Стандартный рабочий процесс в машинном обучении\n",
    "\n",
    "Следующая диаграмма подчеркивает критическую важность разделения данных для объективной оценки модели.\n",
    "\n",
    "1. **Разделение данных:** Исходные данные (`X` и `y`) делятся на **обучающий** и **тестовый** наборы.  \n",
    "2. **Итеративный цикл обучения и настройки:**  \n",
    "    * Модель **обучается** исключительно на *обучающем наборе*.  \n",
    "    * Качество модели **оценивается** (часто на валидационной части обучающего набора).  \n",
    "    * На основе оценки происходит **настройка** гиперпараметров модели.  \n",
    "    * Этот цикл (`Обучение -> Оценка -> Настройка`) повторяется до тех пор, пока не будет найдена лучшая модель.  \n",
    "3. **Финальная оценка:** После завершения настройки, лучшая модель **только один раз** проверяется на *тестовом наборе*. Эти данные модель никогда не видела в процессе обучения или настройки. Это дает нам честную, непредвзятую оценку её производительности.  \n",
    "4. **Внедрение:** Если результаты на тесте удовлетворительны, модель готова к внедрению.\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[\"Данные<br/>X и y\"] --> B[\"Обучающий<br/>набор данных\"]\n",
    "    A --> C[\"Тестовый<br/>набор данных\"]\n",
    "    \n",
    "    B --> D[\"Обучение<br/>модели<br/>(fit/train)\"]\n",
    "    D --> F[\"Оценка<br/>работы<br/>модели\"]\n",
    "    F -->|итерация| E[\"Настройка<br/>модели\"]\n",
    "    E --> D\n",
    "    \n",
    "    C -->|финальная проверка| F\n",
    "    E -->|лучшая модель| G[\"Внедрение<br/>модели\"]\n",
    "    \n",
    "    %% Определение стилей\n",
    "    classDef data fill:#f8c471,stroke:#333,stroke-width:2px,color:#000\n",
    "    classDef training fill:#e5b4c4,stroke:#333,stroke-width:2px,color:#000\n",
    "    classDef eval fill:#c8e6c9,stroke:#333,stroke-width:2px,color:#000\n",
    "    classDef deploy fill:#d1c4e9,stroke:#333,stroke-width:2px,color:#000\n",
    "\n",
    "    %% Применение стилей\n",
    "    class A,B,C data\n",
    "    class D,E training\n",
    "    class F eval\n",
    "    class G deploy\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6bda40-cd98-4079-97d2-f5de2e4e03d5-moved",
   "metadata": {},
   "source": [
    "### 2.2. Важный шаг: Масштабирование признаков (Feature Scaling)\n",
    "\n",
    "Масштабирование — один из самых важных шагов в подготовке данных. В большинстве случаев применение моделей машинного обучения к \"сырым\", немасштабированным данным является ошибкой.\n",
    "\n",
    "#### Зачем это нужно?\n",
    "\n",
    "Представьте, что мы предсказываем цену квартиры по двум признакам:\n",
    "*   $x_1$: Количество комнат (значения от 1 до 5)\n",
    "*   $x_2$: Площадь в квадратных метрах (значения от 30 до 200)\n",
    "\n",
    "Диапазон значений площади гораздо больше, чем у количества комнат. Многие алгоритмы будут ошибочно считать признак \"Площадь\" более важным просто из-за его большого масштаба.\n",
    "\n",
    "**Масштабирование приводит все признаки к сопоставимому диапазону значений, чтобы ни один из них не доминировал над другими по причине своего масштаба.**\n",
    "\n",
    "Это критически важно для:\n",
    "1.  **Алгоритмов, использующих градиентный спуск (включая линейные модели):** Если признаки имеют разный масштаб, \"ландшафт\" функции потерь становится вытянутым. Градиентному спуску потребуется гораздо больше шагов, чтобы найти минимум.\n",
    "2.  **Моделей с регуляризацией (Ridge, Lasso), которые мы изучим позже:** Штраф зависит от величины коэффициентов, и без масштабирования он будет некорректно применяться к разным признакам.\n",
    "3.  **Алгоритмов, основанных на расстоянии (например, KNN, SVM):** Расстояние будет почти полностью определяться признаком с наибольшим масштабом.\n",
    "\n",
    "#### Способы масштабирования\n",
    "\n",
    "1. **Стандартизация (Standardization)**\n",
    "\n",
    "Преобразует данные так, чтобы они имели **среднее значение 0 и стандартное отклонение 1**.\n",
    "$$ z = \\frac{x - \\mu}{\\sigma} $$\n",
    "*   **В `scikit-learn`:** `StandardScaler()`\n",
    "*   **Свойства:** Является **методом выбора по умолчанию** для большинства задач, включая линейные модели.\n",
    "\n",
    "2. **Нормализация (Normalization)**\n",
    "\n",
    "\"Сжимает\" данные в диапазон **от 0 до 1**.\n",
    "$$ x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}} $$\n",
    "*   **В `scikit-learn`:** `MinMaxScaler()`\n",
    "*   **Свойства:** Полезен для некоторых алгоритмов (например, нейронных сетей), но очень чувствителен к выбросам.\n",
    "\n",
    "#### Золотое правило масштабирования\n",
    "\n",
    "Параметры для масштабирования (среднее, std, min/max) должны вычисляться **только на обучающей выборке** и затем применяться и к обучающей, и к тестовой выборкам. Это предотвращает **утечку данных (data leakage)**.\n",
    "\n",
    "```python\n",
    "# Пример правильного масштабирования\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Далее модель обучается на X_train_scaled и оценивается на X_test_scaled\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e0f3e6",
   "metadata": {},
   "source": [
    "## 3. Линейная регрессия: Глубокое погружение\n",
    "\n",
    "**Задача:** Предсказать значение непрерывной целевой переменной `y` на основе набора признаков `X`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa52f3",
   "metadata": {},
   "source": [
    "### 3.1. Математическая постановка задачи\n",
    "\n",
    "Пусть у нас есть обучающая выборка из `n` объектов. Для каждого объекта `i` известны `m` признаков $(x_{i1}, x_{i2}, ..., x_{im})$ и значение целевой переменной $y_i$. Мы хотим найти такую функцию $\\hat{y} = f(x_1, ..., x_m)$, которая бы наилучшим образом приближала $y$.\n",
    "\n",
    "В линейной регрессии мы делаем сильное, но очень полезное предположение, что эта зависимость **линейна**:\n",
    "$$ \\hat{y} = w_0 + w_1x_1 + w_2x_2 + ... + w_mx_m $$ \n",
    "\n",
    "*   $\\hat{y}$ — предсказанное (модельное) значение целевой переменной.\n",
    "*   $w_0$ — **свободный член (intercept)**, или **смещение (bias)**. Это базовое значение $\\hat{y}$, когда все признаки равны нулю.\n",
    "*   $w_1, ..., w_m$ — **веса (коэффициенты) модели**. Каждый вес $w_j$ показывает, на сколько в среднем изменится $\\hat{y}$ при увеличении признака $x_j$ на единицу, **при условии, что все остальные признаки остаются неизменными**.\n",
    "\n",
    "В векторной форме это записывается компактнее. Если мы добавим к нашим признакам фиктивный признак $x_0=1$, то формула примет вид:\n",
    "$$ \\hat{y} = \\sum_{j=0}^{m} w_j x_j = w^T x $$\n",
    "\n",
    "А для всей выборки сразу:\n",
    "$$ \\hat{Y} = Xw $$\n",
    "\n",
    "Сгенерируем синтетические данные для наглядности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a242d-f329-44d8-869e-4cde36979ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Сгенерируем простые данные для демонстрации\n",
    "np.random.seed(42)\n",
    "X_simple = 2 * np.random.rand(100, 1)\n",
    "y_simple = 4 + 3 * X_simple + np.random.randn(100, 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_simple, y_simple)\n",
    "plt.title('Пример данных для задачи регрессии')\n",
    "plt.xlabel('Признак X')\n",
    "plt.ylabel('Целевая переменная y')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vis_intro_lr_1",
   "metadata": {},
   "source": [
    "В идеальном мире точки данных лежали бы на одной прямой. Наша задача — предсказать значение Y по новому значению X.\n",
    "\n",
    "![Идеальная линейная зависимость](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec6-1.png)\n",
    "\n",
    "Но в реальности данные всегда содержат \"шум\". Мы не можем провести линию через все точки, но можем найти такую, которая проходит к ним **ближе всего**.\n",
    "\n",
    "![Реальные данные с шумом](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec6-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e758be-875f-4b4b-9282-44d3897a5df5-revised",
   "metadata": {},
   "source": [
    "### 3.2. Поиск \"наилучших\" весов: функция потерь и Метод Наименьших Квадратов (МНК)\n",
    "\n",
    "Как найти \"наилучшие\" веса $w$? В машинном обучении этот вопрос решается через минимизацию **функции потерь (Loss Function)**. Она измеряет, насколько сильно предсказания модели ($\\hat{y}$) отличаются от реальных значений ($y$) на обучающих данных. Задача алгоритма — подобрать такие веса `w`, чтобы значение этой функции было минимальным.\n",
    "\n",
    "Для классической линейной регрессии основной функцией потерь является **Среднеквадратичная ошибка (Mean Squared Error, MSE)**.\n",
    "\n",
    "$$ L(w) = MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2  \\rightarrow \\min_{w} $$\n",
    "\n",
    "Процесс поиска весов `w`, которые минимизируют MSE, называется **Методом Наименьших Квадратов (МНК)** или **Ordinary Least Squares (OLS)**.\n",
    "\n",
    "**Почему именно MSE?**\n",
    "1.  **Дифференцируемость:** MSE — гладкая, дифференцируемая функция, что позволяет легко находить ее минимум с помощью производных.\n",
    "2.  **Штраф за большие ошибки:** Возведение в квадрат сильно \"наказывает\" модель за большие промахи.\n",
    "3.  **Единственный минимум:** MSE является выпуклой функцией, что гарантирует существование единственного глобального минимума, к которому можно сойтись."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vis_residuals_1",
   "metadata": {},
   "source": [
    "Идея МНК — минимизировать общее расстояние между точками и линией. Расстояние от каждой точки до линии — это **ошибка** или **остаток**.\n",
    "\n",
    "![Ошибки как расстояние до линии](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec6-3.png)\n",
    "\n",
    "Чтобы ошибки с разными знаками не компенсировали друг друга, мы возводим их в квадрат. Наша цель — минимизировать сумму площадей этих квадратов.\n",
    "\n",
    "![Сумма квадратов ошибок](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec6-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e6034b-2359-4e6a-b479-d3aee23f2eef",
   "metadata": {},
   "source": [
    "Продемонстрируем результат работы линейной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ee0c25-ee79-415c-af27-ad383ed83545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_simple, y_simple)\n",
    "w0, w1 = lin_reg.intercept_[0], lin_reg.coef_[0][0]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_simple, y_simple)\n",
    "plt.plot(X_simple, lin_reg.predict(X_simple), color='red', linewidth=3, label='Линия регрессии')\n",
    "plt.title(f'Результат работы линейной регрессии (y = {w0:.2f} + {w1:.2f}x)')\n",
    "plt.xlabel('Признак X')\n",
    "plt.ylabel('Целевая переменная y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9409394d-b4b5-43fc-916a-8aab988038ff",
   "metadata": {},
   "source": [
    "### 3.3. Метрики для оценки качества модели\n",
    "\n",
    "Модель обучена. Теперь нам нужно понять, хорошая ли она. Для этого используются **метрики качества (Evaluation Metrics)**. В отличие от функции потерь, которая нужна алгоритму для обучения, метрики нужны **человеку для интерпретации** результата.\n",
    "\n",
    "Оценка производится на данных, которые модель не видела в процессе обучения (тестовых).\n",
    "\n",
    "1.  **MAE (Mean Absolute Error):** Средняя абсолютная ошибка. \n",
    "    $$ MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\n",
    "    *   **Интерпретация:** \"В среднем наша модель ошибается на X единиц\". Легко понять и объяснить заказчику.\n",
    "\n",
    "2.  **RMSE (Root Mean Squared Error):** Корень из среднеквадратичной ошибки. \n",
    "    $$ RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $$\n",
    "    *   **Интерпретация:** Самая популярная метрика. Измеряется в тех же единицах, что и целевая переменная, но при этом сильно штрафует за большие ошибки.\n",
    "\n",
    "3.  **R² (Коэффициент детерминации):** Показывает, какую долю дисперсии целевой переменной `y` способна объяснить наша модель. \n",
    "    $$ R^2 = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2} $$\n",
    "    *   **Интерпретация:** Значение R² от 0 до 1. Чем ближе к 1, тем лучше модель описывает данные. $R^2 = 0.75$ означает, что модель объясняет 75% изменчивости целевой переменной. Является относительной метрикой.\n",
    "    *   **Важно:** R² — это **только метрика качества**. Ее не используют как функцию потерь."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf5b2cb-ee83-49bf-b0e6-d974e29410f5",
   "metadata": {},
   "source": [
    "### 3.4. Сравнение Функции потерь и Метрик качества\n",
    "\n",
    "Важно четко понимать разницу между этими двумя понятиями.\n",
    "\n",
    "| Характеристика      | Функция Потерь (Loss Function)                             | Метрика Качества (Evaluation Metric)                             |\n",
    "| ------------------- | ---------------------------------------------------------- | ---------------------------------------------------------------- |\n",
    "| **Назначение**      | Руководство процессом **обучения** модели                  | Оценка производительности **обученной** модели                   |\n",
    "| **Кто \"использует\"** | **Алгоритм** оптимизации (например, градиентный спуск)     | **Человек** (Data Scientist, заказчик)                           |\n",
    "| **Ключевое свойство** | Должна быть **дифференцируемой**                           | Должна быть **интерпретируемой** и понятной                      |\n",
    "| **Когда**            | **Во время** подбора весов (`model.fit()`)                         | **После** обучения, для оценки на новых данных (`model.score()`) |\n",
    "| **Пример для ЛР**   | **MSE** — удобна для математической оптимизации.           | **RMSE** и **R²** — понятны для оценки бизнес-результата.        |\n",
    "\n",
    "**Итог:** Алгоритм линейной регрессии **минимизирует MSE** во время обучения. Мы, как люди, **смотрим на RMSE и R²**, чтобы оценить, насколько хорошо обученная модель будет работать в реальной жизни."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fffc464",
   "metadata": {},
   "source": [
    "### 3.5. Важные допущения линейной регрессии\n",
    "\n",
    "Чтобы оценки коэффициентов $w$, полученные с помощью МНК, были несмещенными и эффективными, а статистические выводы (доверительные интервалы, p-value) — корректными, должны выполняться несколько допущений:\n",
    "\n",
    "1.  **Линейность:** Среднее значение целевой переменной линейно зависит от признаков.\n",
    "2.  **Независимость ошибок:** Остатки $e_i$ должны быть независимы друг от друга.\n",
    "3.  **Гомоскедастичность (постоянство дисперсии ошибок):** Дисперсия остатков должна быть одинаковой для всех значений признаков.\n",
    "4.  **Нормальность распределения ошибок:** Остатки $e_i$ должны быть распределены нормально с математическим ожиданием, равным нулю.\n",
    "5.  **Отсутствие мультиколлинеарности:** Признаки в матрице $X$ не должны сильно коррелировать друг с другом.\n",
    "\n",
    "На практике эти допущения часто нарушаются, но модель все равно может давать приемлемые по качеству предсказания. Однако для построения надежных и интерпретируемых моделей важно проверять эти условия."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6279fc7d-revised",
   "metadata": {},
   "source": [
    "## Заключение по лекции 6\n",
    "\n",
    "Сегодня мы заложили прочный фундамент для изучения supervised learning:\n",
    "1.  Увидели, какое место задача регрессии занимает в мире ML.\n",
    "2.  Поняли, что ML-проект — это структурированный инженерный процесс (CRISP-DM), и осознали важность правильной подготовки данных: разделения на выборки и масштабирования.\n",
    "3.  Разобрали математику и логику работы линейной регрессии — модели, которая ищет наилучшую линейную зависимость путем минимизации среднеквадратичной ошибки (MSE).\n",
    "4.  Научились отличать функцию потерь (для машины) от метрик качества (для человека) и познакомились с основными из них: MAE, RMSE и R².\n",
    "\n",
    "На следующей лекции мы усложним нашу модель, чтобы описывать нелинейные зависимости, и столкнемся с ключевой проблемой машинного обучения — переобучением."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
