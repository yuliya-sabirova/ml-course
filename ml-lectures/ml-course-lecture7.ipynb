{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5223ceb8-revised",
   "metadata": {},
   "source": [
    "# Лекция 7: Сложность Модели, Переобучение и Методы Борьбы с ним\n",
    "\n",
    "**Цели лекции:**\n",
    "1.  Изучить полиномиальную регрессию как способ моделирования нелинейных зависимостей.\n",
    "2.  Глубоко разобраться в центральной проблеме ML: компромиссе между смещением и дисперсией (bias-variance tradeoff), и ее проявлениях — недообучении и переобучении.\n",
    "3.  Освоить инструменты диагностики моделей: визуальный анализ кривых обучения и остатков.\n",
    "4.  Изучить кросс-валидацию как надежный метод оценки обобщающей способности модели.\n",
    "5.  Понять идею регуляризации (L1 и L2) как метода борьбы с переобучением путем контроля сложности модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b01804e-48b8-4494-840a-3a79fd08bbe9-moved",
   "metadata": {},
   "source": [
    "## 1. Полиномиальная регрессия: Когда линейности недостаточно\n",
    "\n",
    "На прошлой лекции мы изучили линейную регрессию. Но что если зависимость в данных нелинейная? Простая прямая линия не сможет хорошо описать сложные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac858ab-cce4-40a5-b3ac-28b7bd5a5d20-moved",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Сгенерируем нелинейные (квадратичные) данные\n",
    "np.random.seed(0)\n",
    "m = 100\n",
    "X_poly_data = 6 * np.random.rand(m, 1) - 3\n",
    "y_poly_data = 0.5 * X_poly_data**2 + X_poly_data + 2 + np.random.randn(m, 1)\n",
    "\n",
    "# Попробуем описать их простой линейной регрессией\n",
    "plain_lin_reg = LinearRegression()\n",
    "plain_lin_reg.fit(X_poly_data, y_poly_data)\n",
    "\n",
    "plt.scatter(X_poly_data, y_poly_data, alpha=0.7, label='Исходные данные')\n",
    "plt.plot(X_poly_data, plain_lin_reg.predict(X_poly_data), color='red', linewidth=2, label='Простая линейная регрессия')\n",
    "plt.title('Ограничения простой линейной регрессии')\n",
    "plt.xlabel('Признак X')\n",
    "plt.ylabel('Целевая переменная y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bec1fb4-6355-4928-84a3-d937d6c9ff07-moved",
   "metadata": {},
   "source": [
    "### 1.1. Идея: Усложняем модель через признаки\n",
    "\n",
    "Мы можем заставить линейную модель описывать кривую, если **искусственно создадим новые признаки** из существующих. Этот подход называется **Полиномиальная регрессия**.\n",
    "\n",
    "Мы \"обманываем\" модель, подавая ей на вход не только исходный признак $x_1$, но и его степени ($x_1^2, x_1^3, ...$). Уравнение линейной регрессии для признаков $x_1$ и $x_2 = x_1^2$ будет выглядеть так:\n",
    "\n",
    "$$ \\hat{y} = w_0 + w_1x_1 + w_2x_2 $$\n",
    "\n",
    "Если подставить обратно $x_2 = x_1^2$, мы получим:\n",
    "\n",
    "$$ \\hat{y} = w_0 + w_1x_1 + w_2x_1^2 $$\n",
    "\n",
    "Это уравнение параболы! Для самой модели линейной регрессии ничего не изменилось: она по-прежнему ищет *линейную* комбинацию признаков. Но благодаря тому, что мы преобразовали признаки, итоговая функция становится нелинейной."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46b9a7d-d57c-4d42-90a6-24e2511f2ebe-moved",
   "metadata": {},
   "source": [
    "### 1.2. Взаимодействие признаков (Interaction Terms)\n",
    "\n",
    "Когда у нас несколько исходных признаков (например, $x_1$ и $x_2$), генератор полиномиальных признаков создает не только их степени ($x_1^2, x_2^2, ...$), но и их **произведения ($x_1x_2, ...$)**. Их смысл — уловить **синергетический эффект**, когда влияние одного признака зависит от значения другого.\n",
    "\n",
    "**Пример из жизни 1: Рекламные кампании**\n",
    "\n",
    "Представим, что мы предсказываем продажи (`y`) на основе бюджета на рекламу в Instagram ($x_1$) и на уличных баннерах ($x_2$).\n",
    "Простая модель $y = w_0 + w_1x_1 + w_2x_2$ предполагает, что каждый канал вносит независимый вклад. Но что, если человек увидел рекламу в Instagram, а потом баннер на улице \"напомнил\" ему о товаре и **усилил** эффект? Этот совместный эффект моделируется слагаемым $w_3(x_1x_2)$.\n",
    "\n",
    "**Пример из жизни 2: Сельское хозяйство**\n",
    "\n",
    "Предсказываем урожайность (`y`) на основе количества солнечных дней ($x_1$) и количества осадков ($x_2$). Огромное количество солнца без дождей (засуха) или постоянные дожди без солнца (гниение) — плохо. Наилучший урожай будет при **балансе** этих факторов, который как раз и улавливает признак взаимодействия $x_1 \\cdot x_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef1d09a-2437-450a-a945-484da5a19f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "\n",
    "# Создаем конвейер (pipeline): сначала добавляем полиномиальные признаки, затем обучаем линейную регрессию\n",
    "poly_reg_model = make_pipeline(PolynomialFeatures(degree=2, include_bias=False), \n",
    "                               LinearRegression())\n",
    "\n",
    "# Обучаем модель\n",
    "poly_reg_model.fit(X_poly_data, y_poly_data)\n",
    "\n",
    "# Готовим данные для построения гладкой кривой\n",
    "X_plot = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y_plot = poly_reg_model.predict(X_plot)\n",
    "\n",
    "# Визуализируем результат\n",
    "plt.scatter(X_poly_data, y_poly_data, alpha=0.7, label='Исходные данные')\n",
    "plt.plot(X_plot, y_plot, color='red', linewidth=2, label='Полиномиальная регрессия (2 степень)')\n",
    "plt.title('Результат работы полиномиальной регрессии')\n",
    "plt.xlabel('Признак X')\n",
    "plt.ylabel('Целевая переменная y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d0d5d0-c534-456f-9228-09132c3e8264-moved",
   "metadata": {},
   "source": [
    "## 2. Проблема переобучения и недообучения\n",
    "\n",
    "Полиномиальные признаки — это мощный инструмент. Однако он порождает новый важный вопрос: **а где остановиться?** Какую степень полинома выбрать? Слишком большая сложность приведет к **переобучению**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee034f5d-9344-4aed-8e4b-f95a9f56b91c-moved",
   "metadata": {},
   "source": [
    "### 2.1. Смещение (Bias) и Дисперсия (Variance)\n",
    "\n",
    "Это центральная проблема в машинном обучении.\n",
    "\n",
    "*   **Недообучение (Underfitting, High Bias):** Модель слишком проста и не способна уловить основные закономерности в данных. Она будет показывать **плохое качество и на обучающих, и на новых данных**.\n",
    "*   **Переобучение (Overfitting, High Variance):** Модель слишком сложна. Она не просто выучила закономерности, но и \"запомнила\" случайный шум обучающей выборки. Она будет показывать **отличное качество на обучающих данных, но очень плохое на новых**.\n",
    "\n",
    "**Золотая середина:** Нам нужна модель, которая хорошо обобщает закономерности и будет работать одинаково хорошо как на старых, так и на новых данных. Это называется **хорошей обобщающей способностью**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vis_overfitting_1",
   "metadata": {},
   "source": [
    "Визуально это можно представить так:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec7-11.png\" alt=\"Хорошая модель\" width=\"400\"></td>\n",
    "    <td><img src=\"https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec7-1.png\" alt=\"Переобученная модель\" width=\"400\"></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3dedc-f4d5-48ca-8f44-3bcda17d08fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "degrees = [1, 2, 20]\n",
    "for i, degree in enumerate(degrees):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    \n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model.fit(X_poly_data, y_poly_data)\n",
    "    \n",
    "    X_plot = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "    y_plot = model.predict(X_plot)\n",
    "    \n",
    "    plt.scatter(X_poly_data, y_poly_data, alpha=0.5)\n",
    "    plt.plot(X_plot, y_plot, color='red', linewidth=2)\n",
    "    plt.title(f'Степень полинома = {degree}')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.ylim(0, 10)\n",
    "    \n",
    "    if degree == 1:\n",
    "        plt.text(0.5, 8, \"Недообучение (High Bias)\", fontsize=12)\n",
    "    if degree == 2:\n",
    "        plt.text(0, 8, \"Оптимальная модель\", fontsize=12)\n",
    "    if degree == 20:\n",
    "        plt.text(-2.5, 1, \"Переобучение (High Variance)\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970f6d9b-a4b5-4f40-a3ea-bb99613095c9-moved",
   "metadata": {},
   "source": [
    "## 3. Диагностика и Оценка Модели\n",
    "\n",
    "**Главная проблема:** Как понять, что модель переобучилась, и найти оптимальную сложность?\n",
    "\n",
    "**Ключевой принцип:** Никогда не оценивать финальное качество модели на тех же данных, на которых она обучалась. Для этого данные делят на части."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1216520-bf8f-4a7b-8079-9aa7d5283f8e-moved",
   "metadata": {},
   "source": [
    "### 3.1. Разбиение данных: Train / Validation / Test\n",
    "\n",
    "*   **Обучающая выборка (train set, ~60-80%):** Используется для обучения модели (подбора весов `w`).\n",
    "*   **Валидационная выборка (validation set, ~10-20%):** Используется для **выбора модели и ее гиперпараметров** (например, оптимальной степени полинома). Мы выбираем ту модель, которая показывает наименьшую ошибку на *валидационных* данных.\n",
    "*   **Тестовая выборка (test set, ~10-20%):** \"Неприкосновенный запас\". Используется **только один раз** для финальной, честной оценки качества выбранной модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vis_learning_curves_1",
   "metadata": {},
   "source": [
    "Поведение ошибок на обучающей и тестовой выборках в зависимости от сложности модели — это классический способ диагностики.\n",
    "\n",
    "![Кривые обучения](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec7-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790188b4-3de1-4bb8-8e37-336bc80eee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_poly_data, y_poly_data, test_size=0.3, random_state=10)\n",
    "\n",
    "train_errors, val_errors = [], []\n",
    "degrees = range(1, 15)\n",
    "\n",
    "for degree in degrees:\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_predict = model.predict(X_train)\n",
    "    y_val_predict = model.predict(X_val)\n",
    "    \n",
    "    train_errors.append(np.sqrt(mean_squared_error(y_train, y_train_predict)))\n",
    "    val_errors.append(np.sqrt(mean_squared_error(y_val, y_val_predict)))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(degrees, train_errors, 'r-+', linewidth=2, label='Ошибка на обучении (Train)')\n",
    "plt.plot(degrees, val_errors, 'b-', linewidth=3, label='Ошибка на валидации (Validation)')\n",
    "plt.legend(loc='upper right', fontsize=14)\n",
    "plt.xlabel('Сложность модели (степень полинома)', fontsize=14)\n",
    "plt.ylabel('RMSE', fontsize=14)\n",
    "plt.title('Кривые обучения', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5f47f1-3962-4fba-bc2d-9fd08378b773-moved",
   "metadata": {},
   "source": [
    "**Вывод из графика (Кривые обучения):**\n",
    "*   Ошибка на **обучении** постоянно падает с ростом сложности — модель все лучше подстраивается под данные, которые видит.\n",
    "*   Ошибка на **валидации** сначала падает, достигает минимума (в нашем случае при степени 2-3), а затем начинает расти. Это точка, где модель начинает переобучаться.\n",
    "\n",
    "**Наша цель — найти модель со сложностью, соответствующей минимуму на валидационной кривой.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c347348-38ed-48aa-9070-9da91ee58ed0-moved",
   "metadata": {},
   "source": [
    "### 3.2. Визуальный анализ остатков\n",
    "\n",
    "Остатки ($y_{True} - y_{pred}$) — это ошибки нашей модели. В идеале, они должны быть случайным шумом без закономерностей.\n",
    "\n",
    "**Что мы ищем на графике остатков:**\n",
    "1.  **Случайное распределение:** Точки хаотично разбросаны вокруг нулевой линии.\n",
    "2.  **Отсутствие паттернов:** Если видна структура (например, парабола), значит, модель не уловила какую-то зависимость.\n",
    "3.  **Гомоскедастичность:** Разброс точек должен быть одинаковым по всей оси X. Если разброс увеличивается (форма воронки), это называется гетероскедастичностью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76164ff-65ad-4493-85bd-beac1244a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возьмем модель с оптимальной степенью = 2\n",
    "import seaborn as sns\n",
    "optimal_model = make_pipeline(PolynomialFeatures(2), LinearRegression())\n",
    "optimal_model.fit(X_train, y_train)\n",
    "y_val_pred = optimal_model.predict(X_val)\n",
    "residuals = y_val.flatten() - y_val_pred.flatten()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_val_pred.flatten(), y=residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('График остатков (Residual Plot)')\n",
    "plt.xlabel('Предсказанные значения')\n",
    "plt.ylabel('Остатки (ошибки)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vis_anscombe",
   "metadata": {},
   "source": [
    "Анализ остатков помогает обнаружить проблемы, которые не видны по одним только метрикам. Классический пример — **квартет Энскомба**: четыре набора данных с одинаковыми статистиками и одинаковой линией регрессии, но совершенно разной структурой.\n",
    "\n",
    "![Квартет Энскомба](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec7-3.png)\n",
    "\n",
    "Только для первого набора данных (вверху слева) линейная регрессия является адекватной моделью. В остальных случаях анализ остатков показал бы явные проблемы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ca34c-3fe2-4632-9b88-0b2f0398726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "\n",
    "# Возьмем модель с оптимальной степенью = 2\n",
    "optimal_model = make_pipeline(PolynomialFeatures(2), LinearRegression())\n",
    "optimal_model.fit(X_train, y_train)\n",
    "y_val_pred = optimal_model.predict(X_val)\n",
    "residuals = y_val.flatten() - y_val_pred.flatten()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_val_pred.flatten(), y=residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('График остатков (Residual Plot)')\n",
    "plt.xlabel('Предсказанные значения')\n",
    "plt.ylabel('Остатки (ошибки)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96688695-0d03-415d-9ddb-9ba8b405538d-moved",
   "metadata": {},
   "source": [
    "### 3.3. Метод кросс-валидации (Cross-Validation)\n",
    "\n",
    "Проблема обычного Train/Validation Split в том, что оценка сильно зависит от случайного разделения. **K-Fold Cross-Validation** — это более надежный метод оценки:\n",
    "\n",
    "1.  Обучающая выборка разбивается на K непересекающихся частей (\"фолдов\"), например, K=5.\n",
    "2.  Запускается цикл K раз. На каждой итерации `i`:\n",
    "    *   Блок `i` используется как **валидационный**.\n",
    "    *   Оставшиеся K-1 блоков используются для **обучения**.\n",
    "    *   Оценивается качество на валидационном блоке `i`.\n",
    "3.  Усредняем K полученных оценок качества.\n",
    "\n",
    "#### Общий процесс настройки и оценки\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[\"Данные<br/>X и y\"] --> B[\"Обучающий (+ Валидационный)<br/>набор данных\"]\n",
    "    A --> C[\"Тестовый<br/>(отложенный) набор\"]\n",
    "    \n",
    "    subgraph Итеративный цикл настройки\n",
    "        B --> D[\"Обучение<br/>модели на Train\"]\n",
    "        D --> F[\"Оценка<br/>на Validation\"]\n",
    "        F -->|Результат оценки| E[\"Настройка<br/>гиперпараметров\"]\n",
    "        E --> D\n",
    "    end\n",
    "    \n",
    "    C -->|Финальная проверка| G[\"Оценка лучшей<br/>модели на Test\"]\n",
    "    E -- лучшая модель --> G\n",
    "    G --> H[\"Внедрение<br/>модели\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b2dc34-c010-424c-b8f2-d33726c3805a",
   "metadata": {},
   "source": [
    "![K-Fold CV](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n",
    "\n",
    "**Преимущества:**\n",
    "*   **Надежность:** Оценка меньше зависит от случайности разбиения.\n",
    "*   **Эффективное использование данных:** Каждый объект побывает в роли тестового ровно один раз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c1872a-5785-4d61-ba27-afec4b7e2a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Данные X_poly_data и y_poly_data должны быть определены ранее в ноутбуке,\n",
    "# как в предыдущих ячейках. Для полноты примера, определим их здесь снова.\n",
    "np.random.seed(0)\n",
    "m = 100\n",
    "X_poly_data = 6 * np.random.rand(m, 1) - 3\n",
    "y_poly_data = 0.5 * X_poly_data**2 + X_poly_data + 2 + np.random.randn(m, 1)\n",
    "\n",
    "\n",
    "# Возьмем нашу оптимальную модель 2-й степени\n",
    "model_deg_2 = make_pipeline(PolynomialFeatures(2), LinearRegression())\n",
    "\n",
    "# Запускаем 10-fold кросс-валидацию. \n",
    "# cv=10 означает, что данные будут разбиты на 10 частей.\n",
    "# scoring='neg_mean_squared_error' вычисляет отрицательный MSE.\n",
    "scores = cross_val_score(model_deg_2, X_poly_data, y_poly_data, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Преобразуем отрицательный MSE в положительный RMSE\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "print(\"Ошибки (RMSE) на каждом из 10 фолдов:\")\n",
    "print(rmse_scores.round(2))\n",
    "print(\"\\nСредний RMSE на кросс-валидации: {:.2f}\".format(rmse_scores.mean()))\n",
    "print(\"Стандартное отклонение RMSE: {:.2f}\".format(rmse_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed30556-3594-480b-a314-f20414e905ec-moved",
   "metadata": {},
   "source": [
    "## 4. Регуляризация: Борьба с переобучением\n",
    "\n",
    "Переобученная модель часто имеет очень большие по модулю веса $w$. Она становится слишком \"нервной\", чувствительной к малейшим изменениям во входных данных.\n",
    "\n",
    "**Идея регуляризации:** Добавить в функцию потерь **штраф за большие веса**. Модель теперь будет вынуждена искать компромисс: хорошо описывать данные и при этом сохранять веса небольшими.\n",
    "\n",
    "$$ L_{new}(w) = L_{old}(w) + \\alpha \\cdot P(w) = \\text{MSE} + \\text{штраф} $$ \n",
    "\n",
    "*   $P(w)$ — **регуляризационный член**.\n",
    "*   $\\alpha \\ge 0$ — **коэффициент регуляризации**, гиперпараметр, контролирующий силу штрафа."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e4b71-a34e-4172-993b-d6285bcce18d",
   "metadata": {},
   "source": [
    "### 4.1. L2-регуляризация (Ridge Regression, Гребневая регрессия)\n",
    "\n",
    "В качестве штрафа используется L2-норма вектора весов (сумма их квадратов).\n",
    "$$ L_{Ridge}(w) = \\sum_{i=1}^{n}(y_i - w^T x_i)^2 + \\alpha \\sum_{j=1}^{m} w_j^2 $$ \n",
    "(свободный член $w_0$ обычно не регуляризуется)\n",
    "\n",
    "*   **Эффект:** \"Сжимает\" все веса к нулю, но, как правило, **не обнуляя их полностью**.\n",
    "*   **Когда полезна:** Хорошо работает, когда большинство признаков вносят свой вклад в результат."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa13dd6-e238-49d4-92b2-c077927600df",
   "metadata": {},
   "source": [
    "### 4.2. L1-регуляризация (Lasso Regression, Лассо)\n",
    "\n",
    "В качестве штрафа используется L1-норма вектора весов (сумма их модулей).\n",
    "$$ L_{Lasso}(w) = \\sum_{i=1}^{n}(y_i - w^T x_i)^2 + \\alpha \\sum_{j=1}^{m} |w_j| $$ \n",
    "\n",
    "*   **Эффект:** Может **полностью обнулять веса** некоторых наименее важных признаков.\n",
    "*   **Ключевое свойство:** Производит **автоматический отбор признаков (feature selection)**, делая модель более простой и интерпретируемой.\n",
    "*   **Когда полезна:** Когда мы предполагаем, что многие признаки являются \"мусорными\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vis_regularization_1",
   "metadata": {},
   "source": [
    "Геометрическая интерпретация помогает понять, почему L1-регуляризация обнуляет веса, а L2 — нет. Область допустимых значений весов для L1 — это ромб, а для L2 — круг. Линии уровня функции потерь (эллипсы) с большей вероятностью коснутся ромба в одной из его вершин (где один из коэффициентов равен нулю).\n",
    "\n",
    "![Геометрическая интерпретация L1 и L2 регуляризации](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec7-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6779327c-09bb-4acb-974b-3eef78cb3bb1",
   "metadata": {},
   "source": [
    "Давайте визуально посмотрим на эффект. Обучим 3 модели на полиномиальных данных 10-й степени (которые склонны к переобучению): простую, Ridge и Lasso.\n",
    "\n",
    "**Важно:** Перед применением регуляризации **необходимо масштабировать признаки** (например, с помощью `StandardScaler`), иначе штраф будет применяться некорректно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd78315-db83-43a2-8aab-4430d3265771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Создаем конвейер: масштабирование, полиномиальные признаки, модель\n",
    "pipe_lr = make_pipeline(StandardScaler(), PolynomialFeatures(degree=10, include_bias=False), LinearRegression())\n",
    "pipe_ridge = make_pipeline(StandardScaler(), PolynomialFeatures(degree=10, include_bias=False), Ridge(alpha=1))\n",
    "pipe_lasso = make_pipeline(StandardScaler(), PolynomialFeatures(degree=10, include_bias=False), Lasso(alpha=0.1))\n",
    "\n",
    "# Обучаем\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "pipe_ridge.fit(X_train, y_train)\n",
    "pipe_lasso.fit(X_train, y_train)\n",
    "\n",
    "# Извлекаем коэффициенты\n",
    "coeffs_df = pd.DataFrame({\n",
    "    'Linear Regression': pipe_lr.named_steps['linearregression'].coef_.flatten(),\n",
    "    'Ridge (alpha=1)': pipe_ridge.named_steps['ridge'].coef_.flatten(),\n",
    "    'Lasso (alpha=0.1)': pipe_lasso.named_steps['lasso'].coef_.flatten()\n",
    "})\n",
    "\n",
    "coeffs_df.plot(kind='bar', figsize=(15, 7))\n",
    "plt.title('Влияние регуляризации на коэффициенты модели (степень=10)')\n",
    "plt.ylabel('Значение коэффициента')\n",
    "plt.ylim(-5, 5) # Ограничим ось Y для наглядности\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c27f47d-8cbd-4a06-bde7-190959470ed6-revised",
   "metadata": {},
   "source": [
    "**Вывод:**\n",
    "*   **Linear Regression:** Коэффициенты имеют огромные значения (здесь мы их ограничили для наглядности), что является явным признаком переобучения.\n",
    "*   **Ridge:** Все коэффициенты стали значительно меньше. Модель стала более стабильной.\n",
    "*   **Lasso:** Большинство коэффициентов стали равны нулю. Модель произвела отбор наиболее важных признаков.\n",
    "\n",
    "### 4.3. ElasticNet и подбор гиперпараметра `alpha`\n",
    "\n",
    "*   **ElasticNet** — это комбинация L1 и L2 регуляризаций. Хорошо работает, когда есть группы сильно скоррелированных признаков.\n",
    "*   **Подбор `alpha`:** Оптимальное значение коэффициента регуляризации $\\alpha$ подбирается с помощью **кросс-валидации**. Мы просто перебираем несколько значений (например, `[0.01, 0.1, 1, 10]`) и выбираем то, которое дает наилучшее среднее качество на кросс-валидации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c12da6-3cbe-426d-b68d-17885af1576a-moved",
   "metadata": {},
   "source": [
    "## 5. Продвинутые темы: Функции Потерь и Методы Оптимизации\n",
    "\n",
    "В первой лекции мы познакомились с MSE как стандартной функцией потерь. Теперь, понимая контекст, рассмотрим альтернативы.\n",
    "\n",
    "#### 5.1. Альтернативные Функции Потерь\n",
    "\n",
    "Выбор функции потерь зависит от свойств данных и целей модели.\n",
    "\n",
    "*   **MSE (L2 Loss):** $(y_i - \\hat{y}_i)^2$\n",
    "    *   **Свойство:** Сильно штрафует за большие ошибки. **Чувствительна к выбросам**.\n",
    "\n",
    "*   **MAE (L1 Loss):** $|y_i - \\hat{y}_i|$\n",
    "    *   **Свойство:** Штрафует за ошибки линейно. Более **устойчива (робастна) к выбросам**.\n",
    "\n",
    "*   **Потеря Хьюбера (Huber Loss):** Гибрид L1 и L2.\n",
    "    *   **Свойство:** Ведет себя как MSE для маленьких ошибок и как MAE для больших. Сочетает стабильность MSE и устойчивость к выбросам MAE.\n",
    "\n",
    "#### 5.2. Методы Оптимизации: Как найти минимум?\n",
    "\n",
    "*   **Аналитическое решение (Нормальное уравнение):**\n",
    "    $$ w = (X^T X)^{-1} X^T y $$\n",
    "    *   **Плюсы:** Точное решение без итераций.\n",
    "    *   **Минусы:** Вычислительно дорого ($O(m^3)$) при большом количестве признаков. Неприменимо ко многим моделям (например, Lasso).\n",
    "\n",
    "*   **Численное решение (Градиентный спуск):**\n",
    "    *   **Интуиция:** Итеративные шаги в направлении самого крутого спуска (антиградиента) функции потерь: $w := w - \\alpha \\cdot ∇L(w)$.\n",
    "\n",
    "    ![Аналогия с градиентным спуском](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec7-6.png)\n",
    "    ![Аналогия с градиентным спуском 1](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec7-8.png)\n",
    "    ![Аналогия с градиентным спуском 1](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec7-7.png)\n",
    "\n",
    "    *   **Плюсы:** Универсальный и масштабируемый подход. Основа современного ML.\n",
    "    *   **Минусы:** Находит приближенное решение, требует подбора скорости обучения $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49cedd5-b6dc-4e04-b110-2cf746033075-revised",
   "metadata": {},
   "source": [
    "## Заключение по лекции 7\n",
    "\n",
    "Сегодня мы рассмотрели ключевые концепции, связанные с обобщающей способностью моделей:\n",
    "1.  **Полиномиальная регрессия** позволяет описывать нелинейные зависимости, но несет риск **переобучения**.\n",
    "2.  Мы научились диагностировать переобучение и недообучение с помощью **кривых обучения** и **анализа остатков**.\n",
    "3.  Для надежной оценки модели и подбора гиперпараметров мы используем **кросс-валидацию**.\n",
    "4.  **Регуляризация (Ridge, Lasso)** — мощный метод борьбы с переобучением, который штрафует модель за излишнюю сложность (большие веса).\n",
    "5.  Мы также вернулись к функциям потерь и методам оптимизации, чтобы увидеть, как их выбор влияет на свойства модели, такие как робастность к выбросам."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
