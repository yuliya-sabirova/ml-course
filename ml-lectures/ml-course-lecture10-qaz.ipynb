{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-дәріс: Логикалық әдістер және Шешімдер Ағаштары\n",
    "\n",
    "**Дәріс мақсаты:**\n",
    "1. Классификацияның **логикалық әдістерінің** интуициясымен танысу.\n",
    "2. **Шешімдер ағаштарының** құрылымын, оқыту принципін және қолданылуын тереңінен талдау.\n",
    "3. Ағаштардың күшті жақтарын (интерпретациялануы) көрсету және олардың негізгі әлсіздігін (артық оқытуға бейімділігін) анықтау, осылайша ансамбльдерді зерттеуге негіз дайындау."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-бөлім: Логикалық әдістерге кіріспе\n",
    "\n",
    "Біздің көптеген пайымдауларымыздың негізінде логика жатыр. Біз жиі қарапайым сұрақтарға бірізді жауап беру арқылы шешім қабылдаймыз. Машиналық оқыту да осы принципін **классификацияның логикалық әдістерінде** қолданады.\n",
    "\n",
    "Адамның профилін анықтауға көмектесетін қарапайым схеманы елестетіп көріңіз:\n",
    "\n",
    "```mermaid\n",
    "graph TD;\n",
    "    A{ChatGPT қолданады ма?} -->|Иә| B{Саналы түрде қолданады ма?};\n",
    "    A -->|Жоқ| C{Жалпы тапсырмаларды орындайды ма?};\n",
    "    B -->|Иә| D[<font color=green><b>Емтиханды тапсырады</b></font>];\n",
    "    B -->|Жоқ| E[<font color=red><b>Емтиханды тапсыра алмайды</b></font>];\n",
    "    C -->|Иә| F[<font color=green><b>Емтиханды тапсырады</b></font>];\n",
    "    C -->|Жоқ| G[<font color=red><b>Емтиханды тапсыра алмайды</b></font>];\n",
    "\n",
    "    style D fill:#d4edda,stroke:#155724\n",
    "    style F fill:#d4edda,stroke:#155724\n",
    "    style E fill:#f8d7da,stroke:#721c24\n",
    "    style G fill:#f8d7da,stroke:#721c24\n",
    "```\n",
    "\n",
    "Бұл схема — қарапайым ережелер жиынтығы. Математикалық тұрғыдан бұл ережелер жиі бірнеше шарттың **конъюнкциясы** (логикалық \"ЖӘНЕ\") болып табылады. Мысалы, емтиханнан сүріну жолдарының бірін былай жазуға болады:\n",
    "\n",
    "**ЕГЕР** (\"ChatGPT қолданады ма?\" = ИӘ) **ЖӘНЕ** (\"Саналы түрде қолданады ма?\" = ЖОҚ) **ОНДА** (профиль = \"Емтиханды тапсыра алмайды\")\n",
    "\n",
    "Формула түрінде бұл былай көрінеді:\n",
    "$$ R(x) = [f_1(x)=1] \\land [f_2(x)=0] \\rightarrow \\text{Емтиханды тапсыра алмайды} $$\n",
    "\n",
    "мұндағы $f_1(x)$ — \"ChatGPT қолданады ма?\" белгісі, ал $f_2(x)$ — \"Саналы түрде қолданады ма?\" белгісі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Бинарлық ережелерден нақты белгілерге көшу\n",
    "\n",
    "Нақты деректерде белгілер сирек бинарлық (`иә/жоқ`) болады. Көбінесе олар нақты (температура, бой, баға) болады. Бұл жағдайда логикалық ереже белгіні белгілі бір **шектік мәнмен** салыстыру арқылы құрылады.\n",
    "\n",
    "Мысалы, иристерді классификациялау ережесі былай көрінуі мүмкін: **ЕГЕР** (\"Күлте жапырағының ұзындығы\" <= 2.45 см) **ЖӘНЕ** (\"Күлте жапырағының ені\" > 1.8 см) **ОНДА** (түрі = \"Virginica\").\n",
    "\n",
    "Математикалық тұрғыдан бұл Айверсон нотациясын қолдану арқылы жазылады:\n",
    "$$ [f_j(x) \\le a_j] $$\n",
    "Бұл өрнек, егер $x$ объектісінің $j$-ші белгісінің $f_j(x)$ мәні $a_j$ шегінен кем немесе тең болса, `1` (ақиқат) мәніне, ал кері жағдайда `0` (жалған) мәніне тең болады. Осындай қарапайым ережелерді біріктіре отырып, біз **шешімдер ағаштарын** құрамыз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Балама логикалық тәсілдер\n",
    "\n",
    "Шешімдер ағаштары жалғыз логикалық әдіс емес екенін түсіну маңызды. Дәрігерлік практикадан алынған тағы бір танымал тәсіл — **синдромдық тәсіл**.\n",
    "\n",
    "**Интуиция:** Дәрігер диагнозды бір симптом бойынша емес, олардың жиынтығы бойынша қояды. Егер пациентте белгілі бір ауруға тән симптомдардың жеткілікті саны (`d`) байқалса, сәйкес диагноз қойылады.\n",
    "\n",
    "**Мысал:** **ЕГЕР** ((Температура > 38) + (Жөтел бар) + (Тамақ ауырады) >= 2) **ОНДА** (диагноз = \"ЖРВИ\").\n",
    "\n",
    "Математикалық тұрғыдан бұл былай жазылады: біз $x$ объектісінің қанша белгісі \"қалыпты\" жағдайдан тыс екенін (белгілі бір аралыққа түсетінін) санаймыз, және егер бұл сан $d$ шегінен асып кетсе, ереже іске қосылады.\n",
    "\n",
    "$$ R(x) = \\left[ \\sum_{j \\in J} [a_j \\le f_j(x) \\le b_j] \\ge d \\right] $$\n",
    "\n",
    "Мұндай әдістер бар болғанымен, **шешімдер ағаштары** өздерінің интерпретациялануы мен тиімділігінің арқасында ең кең таралған, сондықтан біз әрі қарай соларға назар аударамыз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-бөлім: Шешімдер Ағашының Анатомиясы\n",
    "\n",
    "**Шешімдер ағашы** — бұл төңкерілген ағашқа ұқсайтын, интуитивті түсінікті модель, ациклдік граф болып табылады.\n",
    "\n",
    "![tree](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/tree.png)\n",
    "\n",
    "**Негізгі компоненттер:**\n",
    "*   **Түбір (Root Node):** Ең жоғарғы түйін, бөлу осыдан басталады. Оқыту таңдамасының барлық объектілері осы түйінге түседі.\n",
    "*   **Ішкі түйін (Internal Node):** \"Сұрақ\" қоятын (шартты тексеретін) және деректерді екі немесе одан да көп тармаққа бөлетін түйін.\n",
    "*   **Тармақ (Branch):** Түйіндерді қосатын сызық. Әрбір тармақ шартты тексеру нәтижелерінің біріне сәйкес келеді (мысалы, \"иә\" немесе \"жоқ\").\n",
    "*   **Жапырақ (Leaf / Terminal Node):** Қорытынды шешімді қамтитын соңғы түйін (классификация үшін класс белгісі немесе регрессия үшін орташа мән).\n",
    "\n",
    "**Классификация процесі:** Жаңа объект ағаш бойымен түбірден жапыраққа дейін \"өтеді\", әр түйінде өз белгілеріне байланысты тармақты таңдайды. Ол түскен жапырақта көрсетілген класс модельдің болжамы болып табылады."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-бөлім: Ағаш қалай оқиды? Бөлу сапасының критерийлері\n",
    "\n",
    "Ағашты құру кезіндегі басты сұрақ: **Әр түйінде бөлу үшін ең жақсы белгі мен ең жақсы шекті қалай табуға болады?**\n",
    "\n",
    "Жауап: еншілес түйіндерді мүмкіндігінше **\"таза\"** (кластар құрамы бойынша біртекті) ететін ережені таңдау керек.\n",
    "\n",
    "#### \"Белгісіздік\" (Impurity) ұғымы\n",
    "\n",
    "\"Белгісіздік\" — бұл бір түйіндегі кластардың қаншалықты араласқанының өлшемі.\n",
    "\n",
    "![impurity](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/impurity_examples_qaz.png)\n",
    "\n",
    "Белгісіздікті сандық өлшеу үшін арнайы критерийлер қолданылады. Ең танымал екеуі — **Джини критерийі** және **энтропия**.\n",
    "\n",
    "**1. Джини критерийі (Gini Impurity):**\n",
    "Түйіннен кездейсоқ таңдалған объектіге сол түйіндегі кездейсоқ белгіні тағайындағанда, оның дұрыс классификацияланбау ықтималдығын көрсетеді.\n",
    "$$ G = 1 - \\sum_{k=1}^{K} p_k^2 $$\n",
    "мұндағы $p_k$ — түйіндегі $k$ класына жататын объектілердің үлесі.\n",
    "- $G=0$ толығымен \"таза\" түйін үшін.\n",
    "- $G$ аралас кластары бар түйін үшін максимумға ұмтылады.\n",
    "\n",
    "**2. Энтропия (Entropy):**\n",
    "Ақпарат теориясынан алынған ұғым, жүйедегі хаосты немесе белгісіздікті өлшейді.\n",
    "$$ S = -\\sum_{k=1}^{K} p_k \\log_2 p_k $$\n",
    "- $S=0$ толығымен \"таза\" түйін үшін.\n",
    "- $S$ барлық кластар теңдей ұсынылғанда максималды болады.\n",
    "\n",
    "![impurity](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/metrics_qaz.png)\n",
    "\n",
    "#### Ақпараттық өсім (Information Gain)\n",
    "Ағаш ең жақсы бөлу туралы шешімді **ақпараттық өсімді** максимумға жеткізу арқылы қабылдайды:\n",
    "$$ IG = \\text{Impurity}(\\text{ата-ана}) - \\sum_{j=1}^{m} \\frac{N_j}{N} \\text{Impurity}(\\text{еншілес}_j) $$\n",
    "Алгоритм барлық мүмкін белгілер мен шектерді қарастырып, әрқайсысы үшін `IG` есептейді және максималды мән беретінін таңдайды."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Мысал: Теннис ойнауға барамыз ба?\n",
    "\n",
    "Бізде 14 күндік бақылау бар деп есептейік, және біз теннис ойнауға баратынымызды болжағымыз келеді. Түбірлік түйінде (ешқандай бөлуге дейін) бізде барлық 14 күн бар:\n",
    "*   **9 күн**, біз ойнауға барғанда (`Play=Yes`)\n",
    "*   **5 күн**, біз ойнауға бармағанда (`Play=No`)\n",
    "\n",
    "**1. Түбір үшін Джини критерийін (Gini Impurity) есептеу:**\n",
    "Кездейсоқ таңдалған күнге қолда бар белгілерден кездейсоқ біреуін тағайындағанда қателесу ықтималдығын көрсетеді.\n",
    "$$ G = 1 - \\sum_{k=1}^{K} p_k^2 $$\n",
    "Біздің мысал үшін:\n",
    "$$ G_{root} = 1 - ( (9/14)^2 + (5/14)^2 ) \\approx 1 - (0.41 + 0.13) = 0.46 $$\n",
    "Бұл біздің бастапқы белгісіздігіміз.\n",
    "\n",
    "**2. Түбір үшін Энтропияны (Entropy) есептеу:**\n",
    "Жүйедегі хаос немесе белгісіздік өлшемі.\n",
    "$$ S = -\\sum_{k=1}^{K} p_k \\log_2 p_k $$\n",
    "$$ S_{root} = - ( (9/14) \\log_2(9/14) + (5/14) \\log_2(5/14) ) \\approx 0.94 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Алгоритм ең жақсы сплитті қалай табады? Ақпараттық өсім (Сараң тәсіл)\n",
    "\n",
    "Алгоритм **Ақпараттық өсімді (Information Gain)** максимумға жеткізеді. Бұл бөлуден кейін белгісіздіктің қаншалықты азайғанын көрсететін көрсеткіш. **Алгоритм мұны ӘРБІР мүмкін сплит үшін (барлық белгілер мен барлық шектер бойынша) жасайды және `IG` максималды болғанын таңдайды!**. Толығырақ қарастырайық. \n",
    "\n",
    "Әрбір түйінде ағаш құру алгоритмі **\"сараң\" (greedy)** принципі бойынша жұмыс істейді. Бөлу үшін ең жақсы ережені табу үшін ол толық, бірақ тиімді іздеуді орындайды:\n",
    "\n",
    "1.  **Барлық белгілер бойынша цикл:** Алгоритм әрбір қолжетімді белгіні бірізді түрде қарастырады.\n",
    "2.  **Белгі үшін ең жақсы шекті іздеу:** Ағымдағы белгі үшін ол барлық мүмкін шектік мәндерді (сандық белгілер үшін) немесе категорияларды бөлудің барлық мүмкін комбинацияларын (категориялық белгілер үшін) тексереді. Әрбір осындай потенциалды сплит үшін ол **Ақпараттық өсімді (Information Gain)** есептейді.\n",
    "3.  **Белгінің \"чемпионын\" таңдау:** Осы белгі үшін максималды `IG` берген сплит (ереже) есте сақталады.\n",
    "4.  **Абсолютті жеңімпазды таңдау:** **Барлық** белгілер үшін \"чемпиондар\" табылғаннан кейін, алгоритм олардың `IG` мәндерін өзара салыстырып, **жалғыз ең жақсы ережені** — `IG` ең жоғары болғанын таңдайды.\n",
    "\n",
    "Дәл осы ереже ағымдағы түйінді бөлу үшін қолданылады. Содан кейін бүкіл процесс пайда болған әрбір еншілес түйін үшін рекурсивті түрде қайталанады."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Мысал: Теннис ойнауға барамыз ба?**\n",
    "\n",
    "Болашақ ағашымыздың **түбірлік түйінінде** тұрмыз деп елестетіңіз. Онда барлық 14 күндік бақылау бар: **9 күн**, біз ойнауға бардық (`Play=Yes`), және **5 күн**, бармадық (`Play=No`).\n",
    "\n",
    "Осы ата-аналық түйіннің Gini бойынша белгісіздігі:\n",
    "$$ G_{root} = 1 - ( (9/14)^2 + (5/14)^2 ) \\approx 1 - (0.413 + 0.128) = 0.459 $$\n",
    "\n",
    "Енді алгоритм осы 14 күнді қай белгі бойынша бөлу керектігін таңдауы керек. Екі нұсқаны салыстырып көрейік: \"Жел\" және \"Ылғалдылық\".\n",
    "\n",
    "**1-нұсқа: \"Жел\" белгісі бойынша бөлу**\n",
    "\n",
    "```mermaid\n",
    "graph TD;\n",
    "    A[<b>Түбір</b> <br> 14 күн <br> 9 Yes, 5 No <br> Gini=0.459] -->|Жел=Әлсіз| B[<b>Еншілес 1</b> <br> 8 күн <br> 6 Yes, 2 No <br> Gini=0.375];\n",
    "    A -->|Жел=Күшті| C[<b>Еншілес 2</b> <br> 6 күн <br> 3 Yes, 3 No <br> Gini=0.5];\n",
    "```\n",
    "\n",
    "*   $G_{weak} = 1 - ((6/8)^2 + (2/8)^2) = 0.375$\n",
    "*   $G_{strong} = 1 - ((3/6)^2 + (3/6)^2) = 0.5$\n",
    "\n",
    "Еншілес түйіндердің орташа салмақталған белгісіздігі:\n",
    "$$ \\text{Weighted\\_Gini}_{wind} = (\\frac{8}{14}) \\times 0.375 + (\\frac{6}{14}) \\times 0.5 \\approx 0.214 + 0.214 = 0.428 $$\n",
    "\n",
    "\"Жел\" үшін ақпараттық өсім:\n",
    "$$ IG_{wind} = 0.459 - 0.428 = \\mathbf{0.031} $$\n",
    "\n",
    "**1-нұсқа бойынша қорытынды:** Интуитивті түрде бұл бөлу өте сәтті емес сияқты. Еншілес түйіндердің бірі (`Wind=Strong`) 50/50 сияқты \"ластанған\", ал екіншісі ата-анасынан сәл ғана \"тазарақ\". `IG=0.031` шағын мәні осыны растайды.\n",
    "\n",
    "---\n",
    "\n",
    "**2-нұсқа: \"Ылғалдылық\" белгісі бойынша бөлу**\n",
    "\n",
    "\"Ылғалдылық\" белгісі сол 14 күнді басқа екі топқа бөледі делік:\n",
    "\n",
    "```mermaid\n",
    "graph TD;\n",
    "    A[<b>Түбір</b> <br> 14 күн <br> 9 Yes, 5 No <br> Gini=0.459] -->|Ылғалдылық=Жоғары| B[<b>Еншілес 1</b> <br> 7 күн <br> 3 Yes, 4 No <br> Gini=0.49];\n",
    "    A -->|Ылғалдылық=Қалыпты| C[<b>Еншілес 2</b> <br> 7 күн <br> 6 Yes, 1 No <br> Gini=0.245];\n",
    "```\n",
    "\n",
    "*   $G_{high} = 1 - ((3/7)^2 + (4/7)^2) \\approx 0.49$\n",
    "*   $G_{normal} = 1 - ((6/7)^2 + (1/7)^2) \\approx 0.245$\n",
    "\n",
    "Еншілес түйіндердің орташа салмақталған белгісіздігі:\n",
    "$$ \\text{Weighted\\_Gini}_{humidity} = (\\frac{7}{14}) \\times 0.49 + (\\frac{7}{14}) \\times 0.245 \\approx 0.245 + 0.122 = 0.367 $$\n",
    "\n",
    "\"Ылғалдылық\" үшін ақпараттық өсім:\n",
    "$$ IG_{humidity} = 0.459 - 0.367 = \\mathbf{0.092} $$\n",
    "\n",
    "---\n",
    "\n",
    "**Сплит туралы шешім қабылдау:**\n",
    "\n",
    "Енді алгоритм барлық мүмкін сплиттерден алынған ақпараттық өсімді салыстырады. Біздің жағдайда:\n",
    "\n",
    "$$ IG_{humidity} (0.092) > IG_{wind} (0.031) $$\n",
    "\n",
    "**Қорытынды:** \"Ылғалдылық\" белгісі бойынша бөлу белгісіздікті көбірек азайтатындықтан, **алгоритм ағаштағы бірінші сплит ретінде дәл \"Ылғалдылықты\" таңдайды.** Бұл процесс әрбір жаңа еншілес түйін үшін рекурсивті түрде қайталанады."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1-бөлім: Сплиттер нақты қалай іріктеледі?\n",
    "\n",
    "#### Сандық белгілер үшін:\n",
    "Алгоритм әрбір мүмкін шекті тексермейді. Ол мұны ақылдырақ жасайды:\n",
    "1.  Ағымдағы түйіндегі белгінің барлық бірегей мәндерін алады.\n",
    "2.  Оларды өсу ретімен сұрыптайды.\n",
    "3.  Тексеруге арналған шектер ретінде **көршілес сұрыпталған нүктелердің арасындағы орташа мәндерді** алады.\n",
    "Мысалы, егер түйінде \"Температура\" белгісі үшін `[10, 20, 30]` мәндері болса, алгоритм `15` және `25` шектерін тексереді.\n",
    "\n",
    "#### Категориялық белгілер үшін:\n",
    "Егер белгі категориялық болса (мысалы, `['Almaty', 'Astana', 'Karaganda']` мәндері бар \"Қала\"), бинарлық ағаш оларды екі топқа бөлуі керек. Алгоритм осындай бөлудің **барлық мүмкін комбинацияларын** қарастырады:\n",
    "*   `{Almaty}` vs `{Astana, Karaganda}`\n",
    "*   `{Astana}` vs `{Almaty, Karaganda}`\n",
    "*   `{Karaganda}` vs `{Almaty, Astana}`\n",
    "...және әрбір комбинация үшін ең жақсы бинарлық бөлуді табу мақсатында Information Gain есептейді."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-бөлім: Ағаштардың басты мәселесі — Артық оқыту (Overfitting)\n",
    "\n",
    "**Артық оқыту** — шешімдер ағаштарының басты осал тұсы. Егер ағаштың өсуін шектемесе, ол әрбір жапырақта бір объектіден қалғанша бөліне береді. Мұндай ағаш оқыту таңдамасын, оның ішіндегі барлық шуды қоса, мінсіз жаттап алады, бірақ жаңа, бұрын кездеспеген деректерде өте нашар жұмыс істейді.\n",
    "\n",
    "![overfitting](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/overfitting_tree_boundary_qaz.png)\n",
    "\n",
    "**Артық оқытумен қалай күресуге болады? (Регуляризация)**\n",
    "Біз ағаштың күрделілігін гиперпараметрлер арқылы шектей аламыз. Бұл процесс **pre-pruning** (алдын ала кесу) деп аталады.\n",
    "- `max_depth`: ағаштың максималды тереңдігі.\n",
    "- `min_samples_split`: түйінді одан әрі бөлу үшін ондағы объектілердің минималды саны.\n",
    "- `min_samples_leaf`: жапырақта болуы тиіс объектілердің минималды саны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1-бөлім: Артық оқытумен күресудің балама әдісі — Кесу (Pruning)\n",
    "\n",
    "Біз **pre-pruning** (немесе *early stopping*) әдісін — ағаштың өсуін гиперпараметрлер (`max_depth` және т.б.) арқылы тоқтатуды қарастырдық. Алайда, одан да тиімдірек болатын басқа тәсіл бар — **post-pruning** (кейінгі кесу немесе \"қырқу\").\n",
    "\n",
    "**Идеясы:**\n",
    "1.  Алдымен біз оқыту таңдамасында қатаң шектеулерсіз **өте терең, әдейі артық оқытылған ағаш** құрамыз.\n",
    "2.  Содан кейін біз оны төменнен жоғары қарай \"қырқа\" бастаймыз. Біз әрбір түйінді (жапырақтарға жақынырақ орналасқандардан бастап) кезекпен қарастырып, мынадай сұрақ қоямыз: \"Егер біз осы бөлуді (осы түйінді) **жойып**, оны жапыраққа айналдырсақ не болады?\".\n",
    "3.  Біз **кейінге қалдырылған (валидациялық) таңдамада** \"қырқуға\" дейінгі және кейінгі модельдің **сапасын салыстырамыз**.\n",
    "4.  Егер түйінді (және оның астындағы бүкіл ішкі ағашты) жою валидациялық таңдамадағы сапаны **нашарлатпаса немесе тіпті жақсартса**, онда біз **\"қырқуды\" орындаймыз**.\n",
    "\n",
    "**Қарапайым сөзбен айтқанда:** Біз алдымен тым күрделі модель құрамыз, содан кейін одан, сірә, оқыту деректеріндегі шуды жаттап алып, жалпылауға нақты пайдасы жоқ бөліктерді \"кесіп тастаймыз\".\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/ShouldWePrune.png\" width=\"500\" height=\"400\">\n",
    "<img src=\"https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/pruneddecisiontree.png\" width=\"800\" height=\"900\">\n",
    "\n",
    "**Неліктен Post-Pruning жақсырақ болуы мүмкін?**\n",
    "Pre-pruning — бұл \"сараң\" тәсіл. Ол ағаштың өсуін тым ерте тоқтатуы мүмкін. Мысалы, қандай да бір бөлу қазір пайдасыздай көрінуі мүмкін, бірақ ол келесі деңгейлерде өте жақсы бөлулерге жол ашуы мүмкін. Pre-pruning ағаштың осы жақсы сплиттерге жетуіне ешқашан мүмкіндік бермейді.\n",
    "\n",
    "Ал post-pruning болса, бүкіл көріністі толық көреді және қай тармақтардың шынымен артық екендігі туралы неғұрлым негізделген шешім қабылдайды.\n",
    "\n",
    "`scikit-learn`-де регуляризацияның негізгі әдісі — гиперпараметрлер арқылы **pre-pruning**. Алайда, `DecisionTreeClassifier`-де **post-pruning**-тің бір түрін іске асыратын `ccp_alpha` (Cost-Complexity Pruning) параметрі бар. Бұл ағаштың күрделілігі мен оның дәлдігі арасындағы оңтайлы тепе-теңдікті табуға мүмкіндік беретін неғұрлым жетілдірілген техника."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-бөлім: Регрессияға арналған Ағаштар\n",
    "\n",
    "Ағаштарды регрессия есептері үшін де қолдануға болады.\n",
    "\n",
    "**Жұмыс істеу принципі:**\n",
    "1.  **Бөлу критерийі:** Белгісіздікті азайтудың (Gini/Entropy) орнына, ағаш **орташа квадраттық қатені (MSE)** максималды түрде азайтатын сплитті іздейді. Ата-аналық түйін $R_m$ және оның екі еншілес түйіні $R_l$ (сол) мен $R_r$ (оң) үшін алгоритм келесі ақпараттық өсімді максимумға жеткізеді:\n",
    "\n",
    "    $$ Q(R_m, j, t) = H(R_m) - \\frac{|R_l|}{|R_m|}H(R_l) - \\frac{|R_r|}{|R_m|}H(R_r) \\rightarrow \\max $$\n",
    "\n",
    "    Мұнда белгісіздік өлшемі $H(R)$ ретінде түйіндегі орташа мәнге қатысты дисперсия немесе MSE қолданылады. Шын мәнінде, ағаш еншілес түйіндердегі мәндерді мүмкіндігінше \"тығыз\" ететін сплитті іздейді.\n",
    "\n",
    "2.  **Жапырақтағы болжам:** Әрбір $v$ жапырағында класс белгісі емес, бір тұрақты $b_v$ мәні сақталады. Бұл мән осы жапыраққа түскен барлық нысаналы айнымалылардың $y_i$ **арифметикалық ортасы** болып табылады:\n",
    "\n",
    "    $$ b_v = \\frac{1}{|R_v|} \\sum_{i \\in R_v} y_i $$\n",
    "\n",
    "    Дәл осы нәрсе регрессия ағашының болжамының **сатылы функция** түрінде көрінуіне әкеледі. Әрбір \"саты\" — бұл жапырақтардың біріндегі орташа мән.\n",
    "\n",
    "\n",
    "Нәтижесінде регрессия ағашының болжамы **сатылы функция** сияқты көрінеді.\n",
    "\n",
    "**Мысал:** `cos(x)` функциясын аппроксимациялау.\n",
    "Косинус қисығында жатқан нүктелер бар деп елестетіңіз. Регрессия ағашы осы тегіс қисықты көлденең кесінділердің көмегімен жақындатуға тырысады.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/cos1d.png\" alt=\"1\" width=\"400\"></td>\n",
    "    <td><img src=\"https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/cos2d.png\" alt=\"2\" width=\"400\"></td>\n",
    "    <td><img src=\"https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/cos3.png\" alt=\"3\" width=\"400\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Ағаштың тереңдігі артқан сайын \"сатылар\" кішірейіп, модель тек функцияның өзіне ғана емес, сонымен қатар деректердегі кездейсоқ шуға да бейімделе бастайды, бұл да артық оқытуға әкеледі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-бөлім: Python-да практикалық іске асыру"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. Классификация мысалы (Iris деректер жиынтығы)\n",
    "\n",
    "Иристің түрін оның параметрлері бойынша анықтаудың классикалық есебі үшін қарапайым ағаш құрайық."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Классификация есебіне арналған шешімдер ағашы\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "def get_grid(data):\n",
    "    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "    return np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "train_data = np.c_[iris.data[:, 0].reshape(-1, 1), iris.data[:, 2].reshape(-1, 1)]\n",
    "train_labels = iris.target\n",
    "\n",
    "clf_tree = DecisionTreeClassifier(criterion='entropy', max_depth=15, random_state=5)\n",
    "clf_tree.fit(train_data, train_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "xx, yy = get_grid(train_data)\n",
    "predicted = clf_tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, predicted, cmap='spring', shading='auto')\n",
    "plt.scatter(train_data[:, 0], train_data[:, 1], c=train_labels, s=50, cmap='spring', edgecolors='black', linewidth=1.5)\n",
    "plt.show()\n",
    "plt.figure(figsize=(14,14))\n",
    "plot_tree(clf_tree,feature_names=iris.feature_names,  \n",
    "          class_names=iris.target_names,\n",
    "          filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Регрессия мысалы (Косинусты аппроксимациялау)\n",
    "\n",
    "Енді біз теорияда талқылаған мысалды іске асырайық: `cos(x)` функциясын аппроксимациялау."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Деректерді генерациялау\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 3 * (0.5 - rng.rand(16)) # Шу қосу\n",
    "\n",
    "# Регрессия ағашын оқыту\n",
    "regr = DecisionTreeRegressor(max_depth=3)\n",
    "regr.fit(X, y)\n",
    "\n",
    "# Болжамдар жасау\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# Нәтижені визуализациялау\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"Деректер\")\n",
    "plt.plot(X_test, y_pred, color=\"cornflowerblue\", label=\"Болжам (max_depth=3)\", linewidth=2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Регрессия ағашымен функцияны аппроксимациялау\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Ағашты визуализациялау\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(regr, \n",
    "         filled=True)\n",
    "plt.title(\"Регрессия есебіне арналған шешімдер ағашы\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-бөлім: Шешімдер Ағаштарының Артықшылықтары мен Кемшіліктері\n",
    "\n",
    "#### Артықшылықтары:\n",
    "*   **Жоғары интерпретациялануы:** Ағаштың логикасын тіпті маман емес адамға да түсіну және түсіндіру оңай. Бұл \"ақ жәшік\".\n",
    "*   **Деректерді масштабтауды қажет етпейді:** Белгілермен олардың бастапқы масштабында жұмыс істейді.\n",
    "*   Сандық та, категориялық та белгілермен жұмыс істей алады.\n",
    "\n",
    "#### Кемшіліктері:\n",
    "*   **Артық оқытуға бейімділік:** Гиперпараметрлерді мұқият баптауды (регуляризацияны) қажет етеді.\n",
    "*   **Тұрақсыздық:** Оқыту деректеріндегі шамалы өзгерістер мүлдем басқа ағаштың құрылуына әкелуі мүмкін."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
