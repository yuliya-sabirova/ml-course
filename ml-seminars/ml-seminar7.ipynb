{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "seminar7-intro",
   "metadata": {},
   "source": [
    "# Семинар 7: Усложнение модели, борьба с переобучением и надежная оценка\n",
    "\n",
    "**Цели семинара:**\n",
    "1.  Реализовать полиномиальную регрессию для моделирования нелинейных зависимостей.\n",
    "2.  Научиться диагностировать переобучение с помощью кривых обучения.\n",
    "3.  Применить кросс-валидацию (`cross_val_score`) для получения робастной оценки качества модели.\n",
    "4.  Изучить и применить модели с регуляризацией: `Ridge`, `Lasso`, `ElasticNet`.\n",
    "5.  Использовать `GridSearchCV` для автоматического подбора оптимального коэффициента регуляризации (`alpha`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seminar7-prep",
   "metadata": {},
   "source": [
    "## 1. Подготовка: загружаем данные и обучаем базовую модель\n",
    "\n",
    "Мы продолжим работать с данными `Advertising.csv` и будем отталкиваться от простой линейной модели, которую мы построили на прошлом семинаре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seminar7-prep-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Загружаем данные\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/data/Advertising.csv')\n",
    "\n",
    "# Готовим X и y\n",
    "X = df.drop('Sales', axis=1)\n",
    "y = df['Sales']\n",
    "\n",
    "# Важно! Для дальнейшей работы нам понадобятся только обучающие и тестовые данные\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seminar7-poly-md",
   "metadata": {},
   "source": [
    "## 2. Полиномиальная регрессия\n",
    "\n",
    "Наша базовая линейная модель была неплохой, но возможно, зависимости в данных более сложные. Попробуем усложнить модель, добавив полиномиальные признаки (например, $TV^2$, $Radio^2$, а также их взаимодействия $TV \\times Radio$).\n",
    "\n",
    "Для этого мы создадим конвейер (`pipeline`), который будет последовательно выполнять два шага:\n",
    "1.  `PolynomialFeatures()`: Генерировать новые признаки.\n",
    "2.  `LinearRegression()`: Обучать линейную регрессию на этих новых, сгенерированных признаках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seminar7-poly-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Создаем конвейер для полиномиальной регрессии 2-й степени\n",
    "poly_model = make_pipeline(PolynomialFeatures(degree=2, include_bias=False),\n",
    "                           LinearRegression())\n",
    "\n",
    "# Обучаем модель\n",
    "poly_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seminar7-poly-eval-md",
   "metadata": {},
   "source": [
    "### 2.1. Оценка полиномиальной модели\n",
    "\n",
    "Сравним качество новой, более сложной модели, с нашей старой простой моделью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seminar7-poly-eval-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Предсказания полиномиальной модели\n",
    "y_poly_pred = poly_model.predict(X_test)\n",
    "\n",
    "# Метрики\n",
    "mae_poly = mean_absolute_error(y_test, y_poly_pred)\n",
    "rmse_poly = np.sqrt(mean_squared_error(y_test, y_poly_pred))\n",
    "\n",
    "print(f\"Полиномиальная модель (2 степень):\")\n",
    "print(f\"MAE: {mae_poly:.2f}\") # Было 1.51 у простой модели\n",
    "print(f\"RMSE: {rmse_poly:.2f}\") # Было 1.93 у простой модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seminar7-learning-curves-md",
   "metadata": {},
   "source": [
    "**Вывод:** Усложнение модели сработало! Ошибки MAE и RMSE значительно уменьшились. Это говорит о том, что в данных действительно были нелинейные зависимости (вероятно, взаимодействия признаков), которые новая модель смогла уловить.\n",
    "\n",
    "### 2.2. Поиск оптимальной сложности (степени полинома)\n",
    "\n",
    "Мы выбрали степень `degree=2` интуитивно. А вдруг `degree=3` или `degree=5` еще лучше? Или, может, мы уже переобучились? Построим **кривые обучения**, чтобы найти оптимальную степень."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seminar7-learning-curves-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse_errors = []\n",
    "test_rmse_errors = []\n",
    "degrees = range(1, 10)\n",
    "\n",
    "for d in degrees:\n",
    "    # Создаем и обучаем модель для каждой степени\n",
    "    poly_converter = PolynomialFeatures(degree=d, include_bias=False)\n",
    "    X_poly_train = poly_converter.fit_transform(X_train)\n",
    "    X_poly_test = poly_converter.transform(X_test) # Важно: используем transform, а не fit_transform\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly_train, y_train)\n",
    "    \n",
    "    # Делаем предсказания и считаем ошибки\n",
    "    y_train_pred = model.predict(X_poly_train)\n",
    "    y_test_pred = model.predict(X_poly_test)\n",
    "    \n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    train_rmse_errors.append(train_rmse)\n",
    "    test_rmse_errors.append(test_rmse)\n",
    "\n",
    "# Визуализируем кривые обучения\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees, train_rmse_errors, label='Train RMSE')\n",
    "plt.plot(degrees, test_rmse_errors, label='Test RMSE')\n",
    "plt.xlabel(\"Степень полинома\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seminar7-curves-conclusion",
   "metadata": {},
   "source": [
    "**Вывод:** График четко показывает, что ошибка на тестовой выборке (синяя линия) минимальна при степени 2-3. Дальнейшее усложнение модели (степень 4 и выше) приводит к росту тестовой ошибки, хотя ошибка на обучении продолжает падать. Это и есть **переобучение**. Значит, мы были правы, выбрав степень 2 или 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seminar7-regularization-md",
   "metadata": {},
   "source": [
    "## 3. Регуляризация: Борьба с переобучением\n",
    "\n",
    "Теперь представим, что у нас очень много признаков, и полиномиальная модель высокой степени все равно показывает лучшие результаты на валидации. Чтобы сделать ее более стабильной и предотвратить переобучение, мы можем применить регуляризацию.\n",
    "\n",
    "**Важно:** Для регуляризации **обязательно** нужно масштабировать данные! Мы добавим `StandardScaler` в наш конвейер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seminar7-regularization-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "# Создадим пайплайн с тремя шагами: масштабирование, генерация признаков, модель\n",
    "# Сразу возьмем степень полинома = 3, как одну из оптимальных\n",
    "\n",
    "# Ridge (L2)\n",
    "ridge_pipe = make_pipeline(StandardScaler(),\n",
    "                           PolynomialFeatures(degree=3, include_bias=False),\n",
    "                           Ridge(alpha=1.0))\n",
    "ridge_pipe.fit(X_train, y_train)\n",
    "y_ridge_pred = ridge_pipe.predict(X_test)\n",
    "print(f\"RMSE для Ridge: {np.sqrt(mean_squared_error(y_test, y_ridge_pred)):.2f}\")\n",
    "\n",
    "# Lasso (L1)\n",
    "lasso_pipe = make_pipeline(StandardScaler(),\n",
    "                           PolynomialFeatures(degree=3, include_bias=False),\n",
    "                           Lasso(alpha=0.01))\n",
    "lasso_pipe.fit(X_train, y_train)\n",
    "y_lasso_pred = lasso_pipe.predict(X_test)\n",
    "print(f\"RMSE для Lasso: {np.sqrt(mean_squared_error(y_test, y_lasso_pred)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seminar7-gridsearch-md",
   "metadata": {},
   "source": [
    "### 3.1. Подбор оптимального `alpha` с помощью кросс-валидации\n",
    "\n",
    "Мы выбрали `alpha` наугад. Правильный способ — найти оптимальное значение с помощью поиска по сетке с кросс-валидацией (`GridSearchCV`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seminar7-gridsearch-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Пайплайн для ElasticNet (комбинация L1 и L2)\n",
    "elastic_pipe = make_pipeline(StandardScaler(),\n",
    "                             PolynomialFeatures(degree=3, include_bias=False),\n",
    "                             ElasticNet(max_iter=10000))\n",
    "\n",
    "# Сетка гиперпараметров, которые мы хотим проверить\n",
    "param_grid = {\n",
    "    'elasticnet__alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'elasticnet__l1_ratio': [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1]\n",
    "}\n",
    "\n",
    "# Создаем объект GridSearchCV. cv=5 означает 5-fold кросс-валидацию.\n",
    "# scoring='neg_mean_squared_error' - метрика для оптимизации.\n",
    "grid_search = GridSearchCV(estimator=elastic_pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=5, \n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           verbose=0)\n",
    "\n",
    "# Запускаем поиск\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seminar7-gridsearch-results-md",
   "metadata": {},
   "source": [
    "### 3.2. Результаты поиска\n",
    "\n",
    "Посмотрим, какие гиперпараметры оказались лучшими."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seminar7-gridsearch-results-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
    "\n",
    "# Оценим лучшую найденную модель на тестовой выборке\n",
    "best_model = grid_search.best_estimator_\n",
    "y_best_pred = best_model.predict(X_test)\n",
    "\n",
    "rmse_best = np.sqrt(mean_squared_error(y_test, y_best_pred))\n",
    "print(f\"\\nФинальный RMSE на тестовой выборке: {rmse_best:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249d60b1-321d-4d8b-b497-55e41abcedf6",
   "metadata": {},
   "source": [
    "### 3.3. Визуализация эффекта регуляризации\n",
    "\n",
    "Мы увидели, как регуляризация влияет на коэффициенты. А как это выглядит на самом графике? Давайте обучим три полиномиальные модели высокой степени (например, 15) на наших синтетических данных: одну без регуляризации, одну с Ridge и одну с Lasso. Это позволит наглядно увидеть \"сглаживающий\" эффект."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8d1b6-f63e-43c2-b401-7c890ecad520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем данные, сгенерированные в начале лекции\n",
    "# Для полноты примера, определим их здесь снова.\n",
    "np.random.seed(0)\n",
    "m = 100\n",
    "X_poly_data = 6 * np.random.rand(m, 1) - 3\n",
    "y_poly_data = 0.5 * X_poly_data**2 + X_poly_data + 2 + np.random.randn(m, 1)\n",
    "\n",
    "# Данные для построения гладких кривых\n",
    "X_plot = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "\n",
    "# Создаем пайплайны для трех моделей. Обратите внимание на StandardScaler!\n",
    "degree = 15\n",
    "pipe_lr = make_pipeline(StandardScaler(), PolynomialFeatures(degree=degree, include_bias=False), LinearRegression())\n",
    "pipe_ridge = make_pipeline(StandardScaler(), PolynomialFeatures(degree=degree, include_bias=False), Ridge(alpha=1))\n",
    "pipe_lasso = make_pipeline(StandardScaler(), PolynomialFeatures(degree=degree, include_bias=False), Lasso(alpha=0.1, max_iter=100000))\n",
    "\n",
    "# Обучаем модели\n",
    "pipe_lr.fit(X_poly_data, y_poly_data)\n",
    "pipe_ridge.fit(X_poly_data, y_poly_data)\n",
    "pipe_lasso.fit(X_poly_data, y_poly_data)\n",
    "\n",
    "# Строим графики\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(X_poly_data, y_poly_data, alpha=0.4, label='Исходные данные')\n",
    "\n",
    "# График для обычной регрессии\n",
    "y_plot_lr = pipe_lr.predict(X_plot)\n",
    "plt.plot(X_plot, y_plot_lr, label='Линейная регрессия (переобучение)', color='red', linestyle='--')\n",
    "\n",
    "# График для Ridge\n",
    "y_plot_ridge = pipe_ridge.predict(X_plot)\n",
    "plt.plot(X_plot, y_plot_ridge, label='Ridge (L2) регрессия', color='green', linewidth=3)\n",
    "\n",
    "# График для Lasso\n",
    "y_plot_lasso = pipe_lasso.predict(X_plot)\n",
    "plt.plot(X_plot, y_plot_lasso, label='Lasso (L1) регрессия', color='purple', linewidth=3)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim(0, 10)\n",
    "plt.title('Сравнение моделей с регуляризацией и без нее')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e718e-6db2-4b2f-9876-282b7a49c51d",
   "metadata": {},
   "source": [
    "**Вывод из графика:**\n",
    "*   **Красная пунктирная линия (Линейная регрессия):** Ведет себя нестабильно, сильно изгибается, пытаясь \"поймать\" каждую точку. Это яркий пример переобучения.\n",
    "*   **Зеленая линия (Ridge):** Гораздо более гладкая и лучше отражает общую квадратичную тенденцию в данных, игнорируя локальные выбросы. Она \"усмирила\" модель.\n",
    "*   **Фиолетовая линия (Lasso):** Также является гладкой и робастной. В данном случае она очень похожа на Ridge, но в других ситуациях, где много неинформативных признаков, она могла бы дать еще более простую модель (ближе к параболе).\n",
    "\n",
    "Этот график наглядно демонстрирует, как регуляризация, штрафуя за большие коэффициенты, заставляет модель находить более простые и более гладкие решения, что улучшает ее обобщающую способность."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seminar7-conclusion",
   "metadata": {},
   "source": [
    "## Заключение\n",
    "\n",
    "На этом семинаре мы сделали большой шаг вперед:\n",
    "*   Научились моделировать нелинейности с помощью **полиномиальной регрессии**.\n",
    "*   Увидели проблему **переобучения** на кривых обучения и поняли, как выбирать оптимальную сложность модели.\n",
    "*   Применили **регуляризацию** (Ridge, Lasso, ElasticNet) для борьбы с переобучением и отбора признаков.\n",
    "*   Использовали **GridSearchCV** — мощный инструмент для автоматического подбора гиперпараметров с помощью кросс-валидации.\n",
    "\n",
    "В результате мы смогли построить сложную, но при этом робастную модель и получили еще более низкую ошибку на тестовых данных, чем раньше."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
