{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 11: Ансамбли деревьев. Случайный Лес\n",
    "\n",
    "**Цель лекции:**\n",
    "1.  Познакомиться с концепцией **ансамблевых методов** как способом повышения качества и стабильности моделей.\n",
    "2.  Разобрать метод **бэггинга (Bagging)** как основу для случайного леса.\n",
    "3.  Изучить алгоритм **Случайный Лес (Random Forest)**, его преимущества и практическое применение.\n",
    "4.  Кратко коснуться альтернативного подхода — **бустинга (Boosting)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1: Почему одного дерева недостаточно? Мудрость толпы\n",
    "\n",
    "На прошлой лекции мы выяснили, что у деревьев решений есть серьезный недостаток: они **нестабильны** и сильно склонны к **переобучению**. Небольшое изменение в данных может кардинально изменить структуру дерева.\n",
    "\n",
    "**Как решить эту проблему?** Давайте обратимся к житейской мудрости. Когда мы хотим принять важное решение, мы редко полагаемся на мнение одного эксперта. Мы собираем мнения нескольких, и итоговое решение принимаем на основе консенсуса. Этот принцип называется **\"мудрость толпы\"**.\n",
    "\n",
    "В машинном обучении этот подход называется **ансамблированием**. Идея состоит в том, чтобы обучить множество разных (слабых или нестабильных) моделей и объединить их предсказания, чтобы получить одно, но более точное и робастное."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ensamble](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/ensamble.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 2: Бэггинг (Bagging = Bootstrap Aggregating)\n",
    "\n",
    "Бэггинг — один из самых популярных ансамблевых методов и основа для случайного леса. Его название состоит из двух частей.\n",
    "\n",
    "#### Шаг 1: Bootstrap (Бутстрэп)\n",
    "\n",
    "Чтобы наши модели (деревья) получились разными, нам нужно обучить их на **разных данных**. Но у нас есть только одна обучающая выборка. Где взять другие?\n",
    "\n",
    "Бутстрэп — это статистический метод, который позволяет из одной выборки размером `N` сгенерировать множество **подвыборок** того же размера `N`. Это делается путем **случайного выбора объектов из исходной выборки с возвращением**.\n",
    "\n",
    "![bootstrap](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/bootstrap1.png)\n",
    "Еще пример\n",
    "![bootstrap1](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/bootstrapping.png)\n",
    "\n",
    "В результате каждая подвыборка будет уникальной: некоторые исходные объекты в ней будут повторяться, а некоторые (в среднем около 37%) не попадут вовсе.\n",
    "\n",
    "#### Шаг 2: Aggregation (Агрегирование)\n",
    "\n",
    "На каждой из `T` полученных бутстрэп-подвыборок мы обучаем **свое независимое дерево решений** $b_t(x)$. Обычно эти деревья делают очень глубокими, намеренно позволяя им переобучиться на своей части данных.\n",
    "\n",
    "Когда нам нужно сделать предсказание для нового объекта, мы собираем все `T` предсказаний и агрегируем их.\n",
    "\n",
    "**Для классификации — мажоритарное голосование:**\n",
    "Итоговым предсказанием $a(x)$ становится тот класс, за который проголосовало большинство деревьев.\n",
    "$$ a(x) = \\text{sign} \\left( \\frac{1}{T} \\sum_{t=1}^{T} b_t(x) \\right) $$\n",
    "\n",
    "**Для регрессии — усреднение:**\n",
    "Итоговым предсказанием $a(x)$ становится среднее арифметическое предсказаний всех деревьев.\n",
    "$$ a(x) = \\frac{1}{T} \\sum_{t=1}^{T} b_t(x) $$\n",
    "\n",
    "В результате ошибки и \"странности\" отдельных переобученных деревьев взаимно компенсируют друг друга, и итоговое предсказание ансамбля становится гораздо более точным и стабильным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Математическое обоснование: как усреднение уменьшает ошибку\n",
    "\n",
    "Почему \"мудрость толпы\" работает? Этот эффект можно легко объяснить с помощью базовой теории вероятностей.\n",
    "\n",
    "Предположим, мы решаем задачу регрессии. Пусть:\n",
    "*   **$\\theta$** — это истинное, правильное значение, которое мы хотим предсказать.\n",
    "*   **$x_j$** — это ответ $j$-го \"эксперта\" (или $j$-го дерева в нашем ансамбле).\n",
    "\n",
    "Каждый ответ можно представить в виде аддитивной модели:\n",
    "$$ x_j = \\theta + \\eta_j $$\n",
    "где $\\eta_j$ — это случайная ошибка $j$-го эксперта.\n",
    "\n",
    "Для простоты сделаем два разумных допущения:\n",
    "1.  **Ошибки несмещенные:** В среднем, эксперты не ошибаются систематически в одну и ту же сторону. Математически: среднее значение ошибки равно нулю, $E[\\eta_j] = 0$.\n",
    "2.  **Ошибки независимы:** Ошибка одного эксперта не связана с ошибкой другого. Математически: $E[\\eta_i \\eta_j] = 0$ для $i \\neq j$. Это ключевое условие, для выполнения которого мы и используем бутстрэп и `max_features`!\n",
    "\n",
    "Пусть **дисперсия** ошибки каждого отдельного эксперта равна $\\sigma^2$. Дисперсия — это мера \"разброса\" или нестабильности предсказаний.\n",
    "$$ \\sigma^2 = E[\\eta_j^2] $$\n",
    "\n",
    "Теперь найдем предсказание нашего ансамбля, усреднив ответы $T$ экспертов:\n",
    "$$ y = \\frac{1}{T} \\sum_{j=1}^{T} x_j $$\n",
    "\n",
    "Какова будет дисперсия ошибки этого усредненного ответа?\n",
    "$$ E[(\\theta - y)^2] = E\\left[\\left(\\theta - \\frac{1}{T} \\sum_{j=1}^{T} x_j\\right)^2\\right] = E\\left[\\left(\\frac{1}{T} \\sum_{j=1}^{T} (\\theta - x_j)\\right)^2\\right] = E\\left[\\left(\\frac{1}{T} \\sum_{j=1}^{T} \\eta_j\\right)^2\\right] $$\n",
    "\n",
    "Раскрывая квадрат суммы и используя свойство независимости ошибок, мы получаем:\n",
    "$$ \\frac{1}{T^2} \\sum_{j=1}^{T} E[\\eta_j^2] = \\frac{1}{T^2} \\cdot T \\cdot \\sigma^2 = \\frac{\\sigma^2}{T} $$\n",
    "\n",
    "**Итог:**\n",
    "> Дисперсия усредненного ответа (ансамбля) в **$T$ раз меньше**, чем дисперсия ответа одного отдельного дерева.\n",
    "\n",
    "**Простыми словами:** Усредняя предсказания $T$ независимых, но шумных моделей, мы уменьшаем итоговый \"шум\" (ошибку) в $T$ раз. Именно поэтому бэггинг и случайный лес так эффективно борются с нестабильностью и переобучением одиночных деревьев. Чем больше деревьев мы добавляем, тем сильнее сглаживаются их индивидуальные ошибки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 3: Случайный Лес (Random Forest) — Бэггинг с \"изюминкой\"\n",
    "\n",
    "**Случайный лес** — это усовершенствованная версия бэггинга деревьев. Он добавляет еще один элемент случайности, чтобы сделать деревья еще более независимыми друг от друга.\n",
    "\n",
    "**Отличие от бэггинга:**\n",
    "При построении **каждого узла** в каждом дереве, алгоритм выбирает лучший сплит не из **всех** доступных признаков, а только из **случайного подмножества** этих признаков.\n",
    "\n",
    "Этот параметр в `scikit-learn` называется `max_features`.\n",
    "\n",
    "**Зачем это нужно?**\n",
    "Представьте, что в ваших данных есть один очень сильный, доминирующий признак. В обычном бэггинге большинство деревьев выберет именно его для первого разбиения в корне. В результате все деревья будут очень похожи друг на друга (\"скоррелированы\"), и эффект от усреднения будет слабее.\n",
    "\n",
    "Случайный лес заставляет некоторые деревья строиться **без доступа** к этому сильному признаку, вынуждая их находить другие, альтернативные закономерности в данных. Это увеличивает \"разнообразие мнений\" в ансамбле и, как правило, повышает итоговую точность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![randomforest1](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/random_forest1.png)\n",
    "![randomforest1](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/randomforest1.png)\n",
    "![randomforest1](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/randomforest2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 4: Историческая справка и реализация в Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ключевые фигуры и идеи\n",
    "*   **CART (Classification and Regression Trees):** Алгоритм, лежащий в основе построения отдельных деревьев в `scikit-learn`, был предложен **Лео Брейманом** и др. в 1984 году.\n",
    "*   **Random Subspace Method:** Идея обучать классификаторы на случайных подмножествах признаков была предложена **Тин Кам Хо** в 1998 году. Это и есть ядро `max_features`.\n",
    "*   **Бэггинг и Случайный Лес:** **Лео Брейман** в 1994 году предложил бэггинг, а в 2001 году объединил его с методом случайных подпространств, формализовав и популяризовав алгоритм **Random Forest**.\n",
    "\n",
    "#### Реализация в Scikit-Learn\n",
    "В `scikit-learn` есть два основных класса:\n",
    "- `sklearn.ensemble.RandomForestClassifier` — для задач классификации.\n",
    "- `sklearn.ensemble.RandomForestRegressor` — для задач регрессии.\n",
    "\n",
    "**Ключевые гиперпараметры:**\n",
    "- `n_estimators`: Количество деревьев в лесу. Самый важный параметр. Чем больше, тем лучше (до определенного предела), но тем дольше обучение.\n",
    "- `max_features`: Количество признаков, которые рассматриваются при поиске лучшего сплита. Типичные значения: `'sqrt'` (квадратный корень из общего числа признаков), `'log2'`, или конкретное число.\n",
    "- `criterion`: Критерий для оценки качества сплита (`'gini'` или `'entropy'` для классификации, `'squared_error'` для регрессии).\n",
    "- `max_depth`, `min_samples_split`, `min_samples_leaf`: Параметры для регуляризации каждого отдельного дерева (аналогично `DecisionTreeClassifier`).\n",
    "- `bootstrap`: `True` (по умолчанию) или `False`. Включает или выключает механизм бутстрэпа.\n",
    "- `oob_score`: `True` или `False`. Включает или выключает оценку Out-of-Bag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Что такое Out-of-Bag (OOB) Score?\n",
    "\n",
    "Это одна из самых элегантных особенностей Случайного Леса. \n",
    "\n",
    "**Идея:** При создании бутстрэп-подвыборки для обучения дерева №1, около 37% исходных данных в нее не попали. Эти \"отложенные\" данные можно использовать как **валидационный набор** специально для этого дерева №1. \n",
    "\n",
    "**Процесс:**\n",
    "1.  Для **каждого** объекта $x_i$ из исходной обучающей выборки мы находим все те деревья, при обучении которых этот объект **не использовался** (был \"out-of-bag\").\n",
    "2.  Мы просим эти деревья сделать предсказание для объекта $x_i$.\n",
    "3.  Усредняем их предсказания, чтобы получить итоговое \"OOB-предсказание\" для объекта $x_i$.\n",
    "4.  Сравниваем все OOB-предсказания с реальными метками $y_i$ и получаем **Out-of-Bag Score** — честную оценку качества модели, полученную без необходимости создавать отдельный валидационный набор!\n",
    "\n",
    "Чтобы использовать эту возможность, нужно установить `bootstrap=True` (что является значением по умолчанию) и `oob_score=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ensamble](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/randomforest4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Глубокое погружение: Зачем нужен `max_features` и откуда взялись значения?\n",
    "\n",
    "Параметр `max_features` — это та самая \"изюминка\", которая отличает Случайный Лес от простого Бэггинга деревьев. Давайте разберемся, зачем он нужен и почему для него используются такие странные значения, как $\\sqrt{n_{features}}$.\n",
    "\n",
    "**Главная цель: Декорреляция деревьев**\n",
    "\n",
    "Как мы помним, \"мудрость толпы\" работает лучше всего, когда мнения в этой толпе **независимы** и **разнообразны**. Если все \"эксперты\" (деревья) будут думать одинаково, то и ансамбль не будет умнее, чем один-единственный эксперт.\n",
    "\n",
    "**Проблема:** Представьте, что в ваших данных есть один очень сильный, доминирующий признак (например, \"доход\" при предсказании покупки дорогого товара). Если мы будем строить деревья методом простого бэггинга, то почти каждое дерево, скорее всего, выберет именно этот признак для самого первого, корневого разбиения. В результате все деревья в лесу будут очень похожи друг на друга — они станут **скоррелированными**. Эффективность ансамбля от этого сильно упадет.\n",
    "\n",
    "**Решение:** Параметр `max_features` решает эту проблему, искусственно ограничивая \"выбор\" для каждого узла. При построении очередного узла в очередном дереве алгоритм делает следующее:\n",
    "1.  Берет **все** признаки, доступные в датасете.\n",
    "2.  Выбирает из них **случайное подмножество** размером `max_features`.\n",
    "3.  Ищет лучший сплит **только среди признаков из этого подмножества**.\n",
    "\n",
    "Это заставляет некоторые деревья строиться, даже не \"видя\" доминирующего признака, и вынуждает их находить другие, менее очевидные закономерности в данных. Это и есть **декорреляция**.\n",
    "\n",
    "**Основной компромисс (Trade-off)**\n",
    "\n",
    "Выбор `max_features` — это баланс между силой отдельных деревьев и их разнообразием:\n",
    "\n",
    "*   **Маленькое значение `max_features`:**\n",
    "    *   **Плюс:** Деревья получаются очень разными (низкая корреляция). Это хорошо для \"мудрости толпы\".\n",
    "    *   **Минус:** Отдельные деревья могут получиться очень \"слабыми\", так как у них может не быть доступа к важным признакам. Их индивидуальная предсказательная сила падает.\n",
    "\n",
    "*   **Большое значение `max_features`:**\n",
    "    *   **Плюс:** Каждое дерево получается сильным, так как имеет доступ почти ко всем признакам.\n",
    "    *   **Минус:** Деревья становятся очень похожими друг на друга (высокая корреляция), и ансамбль теряет свою эффективность. Если `max_features` равно общему числу признаков, то мы получаем обычный Бэггинг.\n",
    "\n",
    "**Откуда взялись значения $\\sqrt{n}$ и $\\log_2(n)$?**\n",
    "\n",
    "Эти значения — не результат строгой математической теоремы, а **эвристики** (практические правила), которые были предложены **Лео Брейманом** в его оригинальной работе по Случайному Лесу. Они были найдены эмпирически, в ходе множества экспериментов на разных наборах данных, и зарекомендовали себя как отличная отправная точка.\n",
    "\n",
    "*   `max_features='sqrt'` (или $\\sqrt{n_{features}}$):\n",
    "    *   **Почему это хорошо?** Это хороший компромисс. Количество выбираемых признаков растет с общим числом признаков, но растет медленно. Это позволяет сохранить разнообразие деревьев, но при этом дает им достаточно хороший шанс \"увидеть\" важные предикторы.\n",
    "    *   **Когда используется?** Это значение по умолчанию в `scikit-learn` для **задач классификации**, и в большинстве случаев его не нужно менять.\n",
    "\n",
    "*   `max_features='log2'` (или $\\log_2(n_{features})$):\n",
    "    *   **Почему это хорошо?** Это еще более жесткое ограничение. Количество выбираемых признаков растет очень медленно.\n",
    "    *   **Когда используется?** Этот вариант может быть полезен, когда у вас **очень много признаков** (тысячи), и вы подозреваете, что многие из них либо неинформативны (шум), либо сильно скоррелированы между собой.\n",
    "\n",
    "*   `max_features=1.0` (или `n_features`):\n",
    "    *   **Что это?** Это эквивалент обычного Бэггинга. Алгоритм на каждом шаге видит все признаки.\n",
    "    *   **Когда используется?** Используется по умолчанию в `scikit-learn` для **задач регрессии**. Считается, что для регрессии важнее иметь более сильные индивидуальные деревья, и проблема корреляции стоит не так остро."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 6: Кратко о Бустинге (Boosting)\n",
    "\n",
    "Бустинг — это другой мощный подход к ансамблированию, который кардинально отличается от бэггинга.\n",
    "\n",
    "| Бэггинг (Random Forest) | Бустинг (Gradient Boosting, AdaBoost) |\n",
    "| :--- | :--- |\n",
    "| Деревья строятся **независимо** и **параллельно**. | Деревья строятся **последовательно**. |\n",
    "| Каждое дерево \\\"не знает\\\" о других. | Каждое следующее дерево **учится на ошибках** предыдущего. |\n",
    "| Цель — уменьшить **дисперсию (variance)** за счет усреднения. | Цель — уменьшить **смещение (bias)** за счет исправления ошибок. |\n",
    "| Используются глубокие, переобученные деревья. | Используются очень простые, \\\"слабые\\\" деревья (часто \\\"пни\\\" с глубиной 1-3). |\n",
    "\n",
    "#### Интуиция бустинга: Работа над ошибками\n",
    "\n",
    "**Интуиция:** Представьте, что вы собираете команду экспертов для решения сложной задачи. Вместо того чтобы просить их работать параллельно, вы делаете это последовательно:\n",
    "1.  Первый эксперт делает свою попытку.\n",
    "2.  Вы смотрите на его ошибки и просите второго эксперта сосредоточиться именно на тех случаях, где первый потерпел неудачу.\n",
    "3.  Третий эксперт концентрируется на ошибках, которые остались после работы первых двух.\n",
    "4.  И так далее. Итоговое решение — это взвешенное \"мнение\" всех экспертов.\n",
    "\n",
    "#### Пример: Алгоритм AdaBoost (Adaptive Boosting)\n",
    "\n",
    "AdaBoost — один из первых и самых наглядных алгоритмов бустинга.\n",
    "\n",
    "1.  **Инициализация:** В самом начале все объекты обучающей выборки имеют одинаковый **вес** $w_i = 1/N$.\n",
    "\n",
    "2.  **Итеративное построение:** Для каждого шага $t=1, \\dots, T$:\n",
    "    a. Обучается \"слабая\" модель (например, дерево-пень) $b_t(x)$ на данных с учетом текущих весов $w_i$. Модель будет стараться правильно классифицировать объекты с большими весами.\n",
    "    б. Вычисляется ошибка $\\epsilon_t$ этой модели.\n",
    "    в. Вычисляется **\"вес голоса\"** этой модели $\\alpha_t$. Чем меньше ошибка модели, тем \"громче\" будет ее голос в финальном ансамбле.\n",
    "       $$ \\alpha_t = \\frac{1}{2} \\ln{\\frac{1 - \\epsilon_t}{\\epsilon_t}} $$\n",
    "    г. **Обновляются веса объектов:**\n",
    "       *   Веса **неправильно** классифицированных объектов **увеличиваются**.\n",
    "       *   Веса **правильно** классифицированных объектов **уменьшаются**.\n",
    "       Это заставляет следующую модель $b_{t+1}(x)$ сконцентрироваться на самых \"сложных\" объектах.\n",
    "\n",
    "3.  **Финальное предсказание:** Итоговая модель — это взвешенное голосование всех слабых моделей.\n",
    "\n",
    "    $$ a(x) = \\text{sign} \\left( \\sum_{t=1}^{T} \\alpha_{t} b_{t}(x) \\right) $$\n",
    "\n",
    "**Gradient Boosting** развивает эту идею: каждая следующая модель учится предсказывать не просто ошибки, а **градиент функции потерь** предыдущей модели. Это более общий и, как правило, более мощный подход.\n",
    "\n",
    "Это очень мощные, но более сложные для настройки и понимания методы. Популярные реализации — Gradient Boosting, XGBoost, LightGBM. В рамках нашего курса мы их подробно рассматривать не будем, но это важное направление для дальнейшего изучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![baggingboosting](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/baggingboosting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 4.1: \"Бесплатный обед\" Случайного Леса: почему он не переобучается с ростом числа деревьев?\n",
    "\n",
    "Вспомним главную проблему одного дерева решений: чем глубже (сложнее) дерево, тем выше риск переобучения. Интуитивно кажется, что добавляя в ансамбль все больше и больше сложных, переобученных деревьев, мы должны сделать итоговую модель еще более переобученной.\n",
    "\n",
    "Но со Случайным Лесом происходит удивительная вещь: **с увеличением количества деревьев (`n_estimators`) ошибка на тестовой выборке (обобщающая способность) перестает расти и выходит на плато.**\n",
    "\n",
    "**Почему так происходит?**\n",
    "\n",
    "Этот эффект является следствием **Закона больших чисел**. Каждое дерево в лесу — это, по сути, случайная величина, которая делает свое предсказание. Оно имеет высокое смещение (variance), то есть сильно подогнано под свою бутстрэп-выборку.\n",
    "\n",
    "Когда мы усредняем предсказания большого количества **независимых (или слабо скоррелированных)** случайных величин, их индивидуальные ошибки (дисперсия) начинают взаимно уничтожать друг друга.\n",
    "\n",
    "*   **Первые деревья (от 1 до ~20):** Каждое новое дерево приносит много новой, уникальной \"информации\". Ошибка на тестовой выборке резко падает.\n",
    "*   **Следующие деревья (~20 до ~100-200):** Новые деревья все еще вносят свой вклад, улучшая результат, но эффект уже не такой сильный. Ошибка продолжает плавно снижаться.\n",
    "*   **Дальнейшее увеличение числа деревьев:** Ансамбль становится достаточно \"мудрым\". Каждое новое дерево уже не добавляет принципиально новой информации, а лишь незначительно \"уточняет\" общую картину. Ошибка на тестовой выборке стабилизируется и выходит на **асимптотическое плато**. Она больше не ухудшается!\n",
    "\n",
    "**Вывод:** В отличие от многих других моделей, где нужно искать компромисс со сложностью, для `n_estimators` в Случайном Лесе правило простое: **\"чем больше, тем лучше\"** (или, по крайней мере, не хуже). Единственным ограничением становится время обучения.\n",
    "\n",
    "Давайте посмотрим на это на практике."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Генерируем более сложный датасет\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=5,\n",
    "    n_redundant=0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Диапазон количества деревьев для проверки\n",
    "n_estimators_range = range(1, 201)\n",
    "\n",
    "# Списки для хранения ошибок\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for n_estimators in n_estimators_range:\n",
    "    # Обучаем модель с текущим количеством деревьев\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Делаем предсказания\n",
    "    train_preds = rf.predict(X_train)\n",
    "    test_preds = rf.predict(X_test)\n",
    "    \n",
    "    # Считаем ошибку (1 - accuracy) и добавляем в списки\n",
    "    train_errors.append(1 - accuracy_score(y_train, train_preds))\n",
    "    test_errors.append(1 - accuracy_score(y_test, test_preds))\n",
    "\n",
    "# Рисуем график\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(n_estimators_range, train_errors, label=\"Ошибка на обучающей выборке (Train Error)\")\n",
    "plt.plot(n_estimators_range, test_errors, label=\"Ошибка на тестовой выборке (Test Error)\", linewidth=2)\n",
    "\n",
    "plt.ylabel(\"Уровень ошибки (1 - Accuracy)\")\n",
    "plt.xlabel(\"Количество деревьев (n_estimators)\")\n",
    "plt.title(\"Зависимость ошибки от количества деревьев в Случайном Лесу\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 5: Случайный Лес в действии (практический пример)\n",
    "\n",
    "Давайте сравним разделяющую поверхность одного дерева и случайного леса на одних и тех же данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Генерируем данные с высоким уровнем шума, чтобы задача была сложной\n",
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "\n",
    "# Разделяем данные на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# --- ИСПРАВЛЕНИЕ: Обучаем модели ТОЛЬКО на обучающей выборке ---\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# --- Визуализация разделяющих поверхностей ---\n",
    "def plot_boundary(ax, model, X_data, y_data, title):\n",
    "    x_min, x_max = X_data[:, 0].min() - 0.5, X_data[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_data[:, 1].min() - 0.5, X_data[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='winter')\n",
    "    # Отображаем только тестовые точки, чтобы видеть, как модели их классифицируют\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=30, edgecolor='k', cmap='winter')\n",
    "    ax.set_title(title)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "plot_boundary(axes[0], tree_clf, X, y, \"Одно Дерево Решений\")\n",
    "plot_boundary(axes[1], rf_clf, X, y, \"Случайный Лес (100 деревьев)\")\n",
    "plt.suptitle(\"Сравнение разделяющих поверхностей (на тестовых данных)\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Оценка моделей на тестовой выборке ---\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Classification Report: Одно Дерево Решений\")\n",
    "print(classification_report(y_test, y_pred_tree))\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Classification Report: Случайный Лес\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "# --- Матрицы ошибок ---\n",
    "cm_tree = confusion_matrix(y_test, y_pred_tree)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "disp_tree = ConfusionMatrixDisplay(confusion_matrix=cm_tree)\n",
    "disp_tree.plot(ax=axes[0], cmap='Blues', values_format='d')\n",
    "axes[0].set_title(\"Матрица ошибок - Дерево\")\n",
    "\n",
    "disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf)\n",
    "disp_rf.plot(ax=axes[1], cmap='Blues', values_format='d')\n",
    "axes[1].set_title(\"Матрица ошибок - Случайный лес\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Генерируем данные с высоким уровнем шума\n",
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "\n",
    "# Разделяем данные на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 1. Обучаем \"отдельно стоящее\" Дерево Решений\n",
    "# Мы не ограничиваем глубину, чтобы оно переобучилось\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# 2. Обучаем Случайный Лес\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# --- ВИЗУАЛИЗАЦИЯ ДЕРЕВЬЕВ ---\n",
    "\n",
    "# 3. Рисуем \"отдельно стоящее\" Дерево Решений\n",
    "# Ограничим глубину отображения (max_depth=3), чтобы график был читаемым,\n",
    "# но помним, что на самом деле дерево гораздо глубже.\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(tree_clf, \n",
    "          max_depth=3, # Ограничение только для визуализации\n",
    "          feature_names=['feature_0', 'feature_1'], \n",
    "          class_names=['Класс 0', 'Класс 1'],\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(\"Верхние уровни 'отдельно стоящего' Дерева Решений\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 4. \"Достаем\" одно дерево из леса и рисуем его\n",
    "# rf_clf.estimators_ - это список всех деревьев в лесу. Возьмем первое.\n",
    "first_tree_from_forest = rf_clf.estimators_[0]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(first_tree_from_forest, \n",
    "          max_depth=3, # Ограничение только для визуализации\n",
    "          feature_names=['feature_0', 'feature_1'],\n",
    "          class_names=['Класс 0', 'Класс 1'],\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(\"Верхние уровни одного дерева ИЗ Случайного Леса\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** Граница одного дерева — \"рваная\" и сложная, она пытается идеально подстроиться под каждую точку. Граница Случайного Леса — гораздо более гладкая и обобщенная. Она отражает общую тенденцию в данных, а не шум. Структура одного дерева из случайного леса заметно отличается от структуры дерева, обученного на всех данных, из-за применения бутстрэпа и случайного выбора признаков на каждом узле. Именно это принудительное \"разнообразие мнений\" между деревьями и позволяет ансамблю достигать высокой точности и устойчивости к переобучению."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 7: Преимущества и Недостатки Случайного Леса\n",
    "\n",
    "#### Преимущества:\n",
    "*   **Высокая точность прогнозов:** На большинстве задач работает лучше линейных алгоритмов; точность сравнима с точностью бустинга.\n",
    "*   **Устойчивость к выбросам и шуму:** Практически не чувствителен к выбросам в данных из-за случайного сэмплирования выборок методом бутстрэпа.\n",
    "*   **Нечувствительность к масштабированию** и другим монотонным преобразованиям значений признаков.\n",
    "*   **Работает «из коробки»:** Не требует тщательной настройки параметров для получения хорошего результата.\n",
    "*   **Эффективно работает с большим числом признаков и классов.**\n",
    "*   **Редко переобучается:** На практике добавление деревьев почти всегда только улучшает композицию.\n",
    "*   **Хорошо работает с пропущенными данными.**\n",
    "*   **Легко распараллеливать** и масштабировать.\n",
    "\n",
    "#### Недостатки:\n",
    "*   **Сложность интерпретации:** В отличие от одного дерева, результаты случайного леса сложнее интерпретировать.\n",
    "*   **Проблемы с разреженными данными:** Работает хуже многих линейных методов, когда в выборке очень много разреженных признаков (например, в задачах анализа текстов).\n",
    "*   **Не умеет экстраполировать:** Как и одиночные деревья, лес не может предсказывать значения за пределами диапазона, который он видел в обучающей выборке.\n",
    "*   **Предвзятость к категориальным признакам:** Склонен отдавать предпочтение категориальным признакам с большим количеством уровней (категорий).\n",
    "*   **Большой размер модели:** Требует O(N·K) памяти для хранения, где N - число деревьев, а K - число узлов в них."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
