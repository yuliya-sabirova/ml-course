{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "seminar7-intro",
      "metadata": {},
      "source": [
        "# 7-семинар: Модельді күрделендіру, артық оқытумен күрес және сенімді бағалау\n",
        "\n",
        "**Семинар мақсаттары:**\n",
        "1.  Сызықтық емес тәуелділіктерді модельдеу үшін полиномдық регрессияны жүзеге асыру.\n",
        "2.  Оқыту қисықтарының көмегімен артық оқытуды диагностикалауды үйрену.\n",
        "3.  Модель сапасының робасты бағасын алу үшін кросс-валидацияны (`cross_val_score`) қолдану.\n",
        "4.  Реттеуі бар модельдерді зерттеу және қолдану: `Ridge`, `Lasso`, `ElasticNet`.\n",
        "5.  Реттеудің оңтайлы коэффициентін (`alpha`) автоматты түрде таңдау үшін `GridSearchCV` пайдалану."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "seminar7-prep",
      "metadata": {},
      "source": [
        "## 1. Дайындық: деректерді жүктеп, негізгі модельді оқытамыз\n",
        "\n",
        "Біз `Advertising.csv` деректерімен жұмысты жалғастырамыз және өткен семинарда құрған қарапайым сызықтық модельге сүйенеміз."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "seminar7-prep-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Деректерді жүктейміз\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/data/Advertising.csv')\n",
        "\n",
        "# X және y дайындаймыз\n",
        "X = df.drop('Sales', axis=1)\n",
        "y = df['Sales']\n",
        "\n",
        "# Маңызды! Әрі қарай жұмыс істеу үшін бізге тек оқыту және тест деректері қажет болады\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "seminar7-poly-md",
      "metadata": {},
      "source": [
        "## 2. Полиномдық регрессия\n",
        "\n",
        "Біздің негізгі сызықтық моделіміз жаман болған жоқ, бірақ деректердегі тәуелділіктер күрделірек болуы мүмкін. Полиномдық белгілерді (мысалы, $TV^2$, $Radio^2$, сондай-ақ олардың өзара әрекеттесуі $TV \\times Radio$) қосу арқылы модельді күрделендіріп көрейік.\n",
        "\n",
        "Ол үшін біз екі қадамды бірізді орындайтын конвейер (`pipeline`) құрамыз:\n",
        "1.  `PolynomialFeatures()`: Жаңа белгілерді генерациялау.\n",
        "2.  `LinearRegression()`: Осы жаңа, генерацияланған белгілерде сызықтық регрессияны оқыту."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "seminar7-poly-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# 2-дәрежелі полиномдық регрессия үшін конвейер құрамыз\n",
        "poly_model = make_pipeline(PolynomialFeatures(degree=2, include_bias=False),\n",
        "                           LinearRegression())\n",
        "\n",
        "# Модельді оқытамыз\n",
        "poly_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "seminar7-poly-eval-md",
      "metadata": {},
      "source": [
        "### 2.1. Полиномдық модельді бағалау\n",
        "\n",
        "Жаңа, күрделірек модельдің сапасын ескі қарапайым моделімізбен салыстырайық."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "seminar7-poly-eval-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Полиномдық модельдің болжамдары\n",
        "y_poly_pred = poly_model.predict(X_test)\n",
        "\n",
        "# Метрикалар\n",
        "mae_poly = mean_absolute_error(y_test, y_poly_pred)\n",
        "rmse_poly = np.sqrt(mean_squared_error(y_test, y_poly_pred))\n",
        "\n",
        "print(f\"Полиномдық модель (2-дәреже):\")\n",
        "print(f\"MAE: {mae_poly:.2f}\") # Қарапайым модельде 1.51 болған\n",
        "print(f\"RMSE: {rmse_poly:.2f}\") # Қарапайым модельде 1.93 болған"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "seminar7-learning-curves-md",
      "metadata": {},
      "source": [
        "**Қорытынды:** Модельді күрделендіру нәтиже берді! MAE және RMSE қателері едәуір азайды. Бұл деректерде шынымен де сызықтық емес тәуелділіктер (мүмкін, белгілердің өзара әрекеттесуі) болғанын және жаңа модель оларды ұстай алғанын көрсетеді.\n",
        "\n",
        "### 2.2. Оңтайлы күрделілікті (полином дәрежесін) іздеу\n",
        "\n",
        "Біз `degree=2` дәрежесін интуитивті түрде таңдадық. Ал егер `degree=3` немесе `degree=5` одан да жақсы болса ше? Немесе, бәлкім, біз қазірдің өзінде артық оқытып жібердік пе? Оңтайлы дәрежені табу үшін **оқыту қисықтарын** құрайық."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "seminar7-learning-curves-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_rmse_errors = []\n",
        "test_rmse_errors = []\n",
        "degrees = range(1, 10)\n",
        "\n",
        "for d in degrees:\n",
        "    # Әр дәреже үшін модель құрып, оқытамыз\n",
        "    poly_converter = PolynomialFeatures(degree=d, include_bias=False)\n",
        "    X_poly_train = poly_converter.fit_transform(X_train)\n",
        "    X_poly_test = poly_converter.transform(X_test) # Маңызды: fit_transform емес, transform қолданамыз\n",
        "    \n",
        "    model = LinearRegression()\n",
        "    model.fit(X_poly_train, y_train)\n",
        "    \n",
        "    # Болжамдар жасап, қателерді есептейміз\n",
        "    y_train_pred = model.predict(X_poly_train)\n",
        "    y_test_pred = model.predict(X_poly_test)\n",
        "    \n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "    \n",
        "    train_rmse_errors.append(train_rmse)\n",
        "    test_rmse_errors.append(test_rmse)\n",
        "\n",
        "# Оқыту қисықтарын визуализациялаймыз\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(degrees, train_rmse_errors, label='Train RMSE')\n",
        "plt.plot(degrees, test_rmse_errors, label='Test RMSE')\n",
        "plt.xlabel(\"Полином дәрежесі\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "seminar7-curves-conclusion",
      "metadata": {},
      "source": [
        "**Қорытынды:** График тест іріктемесіндегі қатенің (көк сызық) 2-3 дәрежеде минималды екенін анық көрсетеді. Модельді одан әрі күрделендіру (4-дәреже және одан жоғары) тест қатесінің өсуіне әкеледі, бірақ оқытудағы қате төмендей береді. Бұл — **артық оқыту**. Демек, біз 2 немесе 3 дәрежесін таңдап дұрыс істегенбіз."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "seminar7-regularization-md",
      "metadata": {},
      "source": [
        "## 3. Реттеу: Артық оқытумен күрес\n",
        "\n",
        "Енді бізде өте көп белгілер бар деп елестетейік, және жоғары дәрежелі полиномдық модель бәрібір валидацияда жақсы нәтиже көрсетіп тұр. Оны тұрақтырақ ету және артық оқытуды болдырмау үшін біз реттеуді қолдана аламыз.\n",
        "\n",
        "**Маңызды:** Реттеу үшін деректерді **міндетті түрде** масштабтау керек! Біз `StandardScaler`-ді конвейерімізге қосамыз."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "seminar7-regularization-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "\n",
        "# Үш қадамнан тұратын пайплайн құрайық: масштабтау, белгілерді генерациялау, модель\n",
        "# Бірден оңтайлылардың бірі ретінде полиномның 3-дәрежесін алайық\n",
        "\n",
        "# Ridge (L2)\n",
        "ridge_pipe = make_pipeline(StandardScaler(),\n",
        "                           PolynomialFeatures(degree=3, include_bias=False),\n",
        "                           Ridge(alpha=1.0))\n",
        "ridge_pipe.fit(X_train, y_train)\n",
        "y_ridge_pred = ridge_pipe.predict(X_test)\n",
        "print(f\"Ridge үшін RMSE: {np.sqrt(mean_squared_error(y_test, y_ridge_pred)):.2f}\")\n",
        "\n",
        "# Lasso (L1)\n",
        "lasso_pipe = make_pipeline(StandardScaler(),\n",
        "                           PolynomialFeatures(degree=3, include_bias=False),\n",
        "                           Lasso(alpha=0.01))\n",
        "lasso_pipe.fit(X_train, y_train)\n",
        "y_lasso_pred = lasso_pipe.predict(X_test)\n",
        "print(f\"Lasso үшін RMSE: {np.sqrt(mean_squared_error(y_test, y_lasso_pred)):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "seminar7-gridsearch-md",
      "metadata": {},
      "source": [
        "### 3.1. Оңтайлы `alpha`-ны кросс-валидация көмегімен таңдау\n",
        "\n",
        "Біз `alpha`-ны кездейсоқ таңдадық. Дұрыс тәсіл — оңтайлы мәнді кросс-валидациямен тор бойынша іздеу (`GridSearchCV`) арқылы табу."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "seminar7-gridsearch-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# ElasticNet үшін пайплайн (L1 және L2 комбинациясы)\n",
        "elastic_pipe = make_pipeline(StandardScaler(),\n",
        "                             PolynomialFeatures(degree=3, include_bias=False),\n",
        "                             ElasticNet(max_iter=10000))\n",
        "\n",
        "# Тексергіміз келетін гиперпараметрлер торы\n",
        "param_grid = {\n",
        "    'elasticnet__alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'elasticnet__l1_ratio': [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1]\n",
        "}\n",
        "\n",
        "# GridSearchCV нысанын құрамыз. cv=5 5-fold кросс-валидацияны білдіреді.\n",
        "# scoring='neg_mean_squared_error' - оңтайландыру үшін метрика.\n",
        "grid_search = GridSearchCV(estimator=elastic_pipe, \n",
        "                           param_grid=param_grid, \n",
        "                           cv=5, \n",
        "                           scoring='neg_mean_squared_error',\n",
        "                           verbose=0)\n",
        "\n",
        "# Іздеуді іске қосамыз\n",
        "grid_search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "seminar7-gridsearch-results-md",
      "metadata": {},
      "source": [
        "### 3.2. Іздеу нәтижелері\n",
        "\n",
        "Қандай гиперпараметрлердің үздік болғанын көрейік."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "seminar7-gridsearch-results-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Үздік параметрлер:\", grid_search.best_params_)\n",
        "\n",
        "# Табылған үздік модельді тест іріктемесінде бағалайық\n",
        "best_model = grid_search.best_estimator_\n",
        "y_best_pred = best_model.predict(X_test)\n",
        "\n",
        "rmse_best = np.sqrt(mean_squared_error(y_test, y_best_pred))\n",
        "print(f\"\\nТест іріктемесіндегі соңғы RMSE: {rmse_best:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "249d60b1-321d-4d8b-b497-55e41abcedf6",
      "metadata": {},
      "source": [
        "### 3.3. Реттеу эффектісін визуализациялау\n",
        "\n",
        "Біз реттеудің коэффициенттерге қалай әсер ететінін көрдік. Ал бұл графикте қалай көрінеді? Біздің синтетикалық деректерде жоғары дәрежелі (мысалы, 15) үш полиномдық модельді оқытайық: біреуі реттеусіз, біреуі Ridge-мен және біреуі Lasso-мен. Бұл \"тегістеу\" эффектісін анық көруге мүмкіндік береді."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0a8d1b6-f63e-43c2-b401-7c890ecad520",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Дәрістің басында генерацияланған деректерді қолданамыз\n",
        "# Мысалдың толықтығы үшін оларды осы жерде қайта анықтайық.\n",
        "np.random.seed(0)\n",
        "m = 100\n",
        "X_poly_data = 6 * np.random.rand(m, 1) - 3\n",
        "y_poly_data = 0.5 * X_poly_data**2 + X_poly_data + 2 + np.random.randn(m, 1)\n",
        "\n",
        "# Тегіс қисықтарды салу үшін деректер\n",
        "X_plot = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
        "\n",
        "# Үш модель үшін пайплайндар құрамыз. StandardScaler-ге назар аударыңыз!\n",
        "degree = 15\n",
        "pipe_lr = make_pipeline(StandardScaler(), PolynomialFeatures(degree=degree, include_bias=False), LinearRegression())\n",
        "pipe_ridge = make_pipeline(StandardScaler(), PolynomialFeatures(degree=degree, include_bias=False), Ridge(alpha=1))\n",
        "pipe_lasso = make_pipeline(StandardScaler(), PolynomialFeatures(degree=degree, include_bias=False), Lasso(alpha=0.1, max_iter=100000))\n",
        "\n",
        "# Модельдерді оқытамыз\n",
        "pipe_lr.fit(X_poly_data, y_poly_data)\n",
        "pipe_ridge.fit(X_poly_data, y_poly_data)\n",
        "pipe_lasso.fit(X_poly_data, y_poly_data)\n",
        "\n",
        "# Графиктерді саламыз\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(X_poly_data, y_poly_data, alpha=0.4, label='Бастапқы деректер')\n",
        "\n",
        "# Кәдімгі регрессия үшін график\n",
        "y_plot_lr = pipe_lr.predict(X_plot)\n",
        "plt.plot(X_plot, y_plot_lr, label='Сызықтық регрессия (артық оқыту)', color='red', linestyle='--')\n",
        "\n",
        "# Ridge үшін график\n",
        "y_plot_ridge = pipe_ridge.predict(X_plot)\n",
        "plt.plot(X_plot, y_plot_ridge, label='Ridge (L2) регрессия', color='green', linewidth=3)\n",
        "\n",
        "# Lasso үшін график\n",
        "y_plot_lasso = pipe_lasso.predict(X_plot)\n",
        "plt.plot(X_plot, y_plot_lasso, label='Lasso (L1) регрессия', color='purple', linewidth=3)\n",
        "\n",
        "plt.legend()\n",
        "plt.ylim(0, 10)\n",
        "plt.title('Реттеуі бар және онсыз модельдерді салыстыру')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c8e718e-6db2-4b2f-9876-282b7a49c51d",
      "metadata": {},
      "source": [
        "**Графиктен қорытынды:**\n",
        "*   **Қызыл үзік сызық (Сызықтық регрессия):** Тұрақсыз, әрбір нүктені \"ұстауға\" тырысып, қатты иіледі. Бұл артық оқытудың айқын мысалы.\n",
        "*   **Жасыл сызық (Ridge):** Әлдеқайда тегіс және деректердегі жалпы квадраттық тенденцияны жақсырақ көрсетеді, жергілікті шығарындыларды елемейді. Ол модельді \"бағындырды\".\n",
        "*   **Күлгін сызық (Lasso):** Сондай-ақ тегіс және робасты. Бұл жағдайда ол Ridge-ге өте ұқсас, бірақ ақпаратсыз белгілер көп басқа жағдайларда, ол одан да қарапайым модель бере алар еді (параболаға жақынырақ).\n",
        "\n",
        "Бұл график реттеудің, үлкен коэффициенттер үшін айыппұл сала отырып, модельді қарапайым және тегіс шешімдерді табуға қалай мәжбүрлейтінін, бұл оның жалпылау қабілетін жақсартатынын анық көрсетеді."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "seminar7-conclusion",
      "metadata": {},
      "source": [
        "## Қорытынды\n",
        "\n",
        "Бұл семинарда біз алға үлкен қадам жасадық:\n",
        "*   **Полиномдық регрессия** көмегімен сызықтық еместіктерді модельдеуді үйрендік.\n",
        "*   Оқыту қисықтарында **артық оқыту** мәселесін көрдік және модельдің оңтайлы күрделілігін қалай таңдау керектігін түсіндік.\n",
        "*   Артық оқытумен күресу және белгілерді іріктеу үшін **реттеуді** (Ridge, Lasso, ElasticNet) қолдандық.\n",
        "*   Кросс-валидация көмегімен гиперпараметрлерді автоматты түрде таңдауға арналған қуатты құрал — **GridSearchCV**-ді пайдаландық.\n",
        "\n",
        "Нәтижесінде біз күрделі, бірақ сонымен бірге робасты модель құра алдық және тест деректерінде бұрынғыдан да төмен қателікке қол жеткіздік."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}