{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9b7a29f",
   "metadata": {},
   "source": [
    "# Семинар 8: Практика инжиниринга признаков и логистической регрессии\n",
    "\n",
    "**Цели семинара:**\n",
    "1.  Провести EDA для задачи классификации, выявляя категориальные признаки, скрытые в числовых колонках.\n",
    "2.  Применить One-Hot Encoding для категориальных признаков.\n",
    "3.  Применить `StandardScaler` для масштабирования числовых признаков.\n",
    "4.  Обучить модель `LogisticRegression` с автоматическим подбором гиперпараметров через `GridSearchCV`.\n",
    "5.  Научиться строить и подробно интерпретировать матрицу ошибок, отчет о классификации и ROC-кривую."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19db8588",
   "metadata": {},
   "source": [
    "## 1. Загрузка и исследовательский анализ данных (EDA)\n",
    "\n",
    "Мы будем работать с набором данных `heart.csv`, который содержит информацию о пациентах и наличии у них заболевания сердца."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9e32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/data/heart.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da00668",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='target', data=df)\n",
    "plt.title('Распределение целевой переменной')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085f17a4",
   "metadata": {},
   "source": [
    "**Ключевой момент EDA:** Некоторые признаки, такие как `cp` (тип боли в груди), `thal` (состояние сосудов) и `slope` (наклон сегмента ST), представлены числами, но по своей сути являются **категориями**. Линейные модели будут неверно интерпретировать их как упорядоченные числовые значения. Поэтому их необходимо преобразовать."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1817e6fa",
   "metadata": {},
   "source": [
    "## 2. Шаг 1: Инжиниринг признаков и подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0408505",
   "metadata": {},
   "source": [
    "### 2.1. One-Hot Encoding\n",
    "\n",
    "Мы применим One-Hot Encoding к категориальным признакам. Это создаст новые бинарные столбцы для каждой категории."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc79e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Указываем столбцы, которые являются категориальными\n",
    "categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "\n",
    "# Применяем pd.get_dummies\n",
    "X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(\"Размерность X после OHE:\", X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a98be2",
   "metadata": {},
   "source": [
    "### 2.2. Разделение на выборки и масштабирование\n",
    "\n",
    "Теперь, когда все данные числовые, мы можем разделить их на обучающую и тестовую выборки, а затем применить масштабирование."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e92685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Обучаем scaler ТОЛЬКО на обучающих данных\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "# Применяем обученный scaler к обеим выборкам\n",
    "scaled_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ab61df",
   "metadata": {},
   "source": [
    "## 3. Шаг 2: Обучение модели\n",
    "\n",
    "Мы используем `GridSearchCV` для автоматического поиска лучших гиперпараметров `C` (сила регуляризации) и `penalty` (тип регуляризации)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a4a16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Создаем модель\n",
    "log_model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Создаем сетку гиперпараметров\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': np.logspace(-3, 3, 7)\n",
    "}\n",
    "\n",
    "# Создаем объект GridSearchCV\n",
    "grid_search = GridSearchCV(log_model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Обучаем\n",
    "grid_search.fit(scaled_X_train, y_train)\n",
    "\n",
    "print(\"Лучшие параметры:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50233273",
   "metadata": {},
   "source": [
    "## 4. Шаг 3: Оценка модели\n",
    "\n",
    "Теперь оценим лучшую модель, найденную GridSearchCV, на тестовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9aea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    RocCurveDisplay,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Предсказания\n",
    "y_pred = grid_search.predict(scaled_X_test)\n",
    "\n",
    "# Матрица ошибок (замена plot_confusion_matrix)\n",
    "print(\"Матрица ошибок:\")\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    grid_search,            # fitted estimator (например, GridSearchCV)\n",
    "    scaled_X_test,\n",
    "    y_test\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Отчет о классификации\n",
    "print(\"\\nОтчет о классификации:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC-кривая (замена plot_roc_curve)\n",
    "# Работает, если у модели есть predict_proba или decision_function\n",
    "print(\"\\nROC-кривая:\")\n",
    "RocCurveDisplay.from_estimator(grid_search, scaled_X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c601be7-6bd8-4127-b8ae-2338e909eac7",
   "metadata": {},
   "source": [
    "### 3.4. Как читать отчет по классификации: практическое руководство\n",
    "\n",
    "После того как модель обучена, `scikit-learn` предоставляет мощные инструменты для ее анализа: `confusion_matrix` и `classification_report`. Давайте разберемся, как их правильно интерпретировать.\n",
    "\n",
    "#### **1. Матрица ошибок (Confusion Matrix)**\n",
    "\n",
    "Это \"рентгеновский снимок\" вашей модели. Он показывает, где именно модель права, а где ошибается.\n",
    "\n",
    "|                | **Предсказано: 0** (Negative) | **Предсказано: 1** (Positive) |\n",
    "|:---:|:---:|:---:|\n",
    "| **Факт: 0** (Negative) | TN (True Negative) | FP (False Positive) |\n",
    "| **Факт: 1** (Positive) | FN (False Negative) | TP (True Positive) |\n",
    "\n",
    "-   **TP (True Positive) — Истинно-положительный результат:** Модель правильно предсказала класс **1**. *(Нашла больного)*.\n",
    "-   **TN (True Negative) — Истинно-отрицательный результат:** Модель правильно предсказала класс **0**. *(Нашла здорового)*.\n",
    "-   **FP (False Positive) — Ложно-положительный результат (Ошибка I рода):** Модель предсказала **1**, а на самом деле был **0**. *(Ложная тревога: назвала здорового больным)*.\n",
    "-   **FN (False Negative) — Ложно-отрицательный результат (Ошибка II рода):** Модель предсказала **0**, а на самом деле был **1**. *(Пропуск: назвала больного здоровым)*.\n",
    "\n",
    "**Ключевой вывод:**\n",
    "*   Если для вас критически важно **избежать ложных тревог** (например, в спам-фильтре), вы должны стремиться минимизировать **FP**.\n",
    "*   Если для вас критически важно **ничего не пропустить** (например, в медицинской диагностике), вы должны стремиться минимизировать **FN**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Отчет о классификации (classification_report)**\n",
    "\n",
    "Этот отчет агрегирует информацию из матрицы ошибок в удобные для анализа метрики.\n",
    "\n",
    "**Метрики для каждого класса:**\n",
    "\n",
    "*   **Precision (Точность)**\n",
    "    <br>\n",
    "    $$ Precision = \\frac{TP}{TP + FP} $$\n",
    "    <br>\n",
    "    Показывает, какая доля объектов, названных моделью положительными, **действительно** является таковыми. Высокая точность означает, что если модель сказала \"Да\", то ей можно доверять.\n",
    "\n",
    "*   **Recall (Полнота)**\n",
    "    <br>\n",
    "    $$ Recall = \\frac{TP}{TP + FN} $$\n",
    "    <br>\n",
    "    Показывает, какую долю **всех реальных** положительных объектов модель смогла найти. Высокая полнота означает, что модель хорошо обнаруживает нужный класс.\n",
    "\n",
    "*   **F1-score**\n",
    "    <br>\n",
    "    $$ F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall} $$\n",
    "    <br>\n",
    "    Гармоническое среднее между `precision` и `recall`. Это полезная сводная метрика, особенно при несбалансированных классах, так как она будет низкой, если одна из двух компонент (precision или recall) очень низкая.\n",
    "\n",
    "*   **Support**\n",
    "    Количество реальных объектов каждого класса в тестовой выборке. Всегда смотрите на это число: если `support` для какого-то класса очень мал (например, 5-10 объектов), то метрики для этого класса будут статистически ненадежными.\n",
    "\n",
    "**Сводные строки:**\n",
    "\n",
    "*   **Accuracy:** Общая доля правильных предсказаний по всем классам. Полезна только на сбалансированных выборках.\n",
    "*   **Macro avg:** Среднее арифметическое метрик по всем классам. Дает каждому классу равный вес, независимо от его размера. Показывает, насколько хорошо модель работает \"в среднем по всем классам\".\n",
    "*   **Weighted avg:** Средневзвешенное значение метрик с учетом `support` каждого класса. Показывает, насколько хорошо модель работает \"в среднем по всем объектам\".\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. ROC-кривая и AUC**\n",
    "\n",
    "ROC-кривая визуализирует компромисс между двумя типами ошибок при изменении порога принятия решения.\n",
    "\n",
    "*   **TPR (True Positive Rate / Recall):** Доля верно найденных положительных объектов. $$ TPR = \\frac{TP}{TP + FN} $$\n",
    "*   **FPR (False Positive Rate):** Доля ложных срабатываний. $$ FPR = \\frac{FP}{FP + TN} $$\n",
    "\n",
    "**AUC — площадь под ROC-кривой (Area Under the Curve):**\n",
    "Это единое число, которое оценивает общую способность модели разделять классы.\n",
    "\n",
    "*   **0.9 - 1.0:** Отличная модель.\n",
    "*   **0.8 - 0.9:** Очень хорошая модель.\n",
    "*   **0.7 - 0.8:** Хорошая модель.\n",
    "*   **0.6 - 0.7:** Удовлетворительная модель.\n",
    "*   **0.5 - 0.6:** Плохая модель.\n",
    "*   **0.5:** Случайное угадывание (бесполезная модель).\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Выбор порога (Threshold)**\n",
    "\n",
    "По умолчанию, модель относит объект к классу 1, если его вероятность > `0.5`. Этот порог можно изменять, если цена ошибок FP и FN для вас разная.\n",
    "\n",
    "*   **Низкий порог (например, 0.3):** Увеличивает **recall** (находим больше положительных), но снижает **precision** (больше ложных тревог).\n",
    "*   **Высокий порог (например, 0.7):** Увеличивает **precision** (предсказания очень надежны), но снижает **recall** (пропускаем больше положительных).\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Чек-лист анализа модели классификации**\n",
    "\n",
    "1.  Посмотри `support`: Сбалансированы ли классы в тестовой выборке?\n",
    "2.  Посмотри **матрицу ошибок**: Где модель ошибается чаще всего — **FP** или **FN**? Какая ошибка для вашей бизнес-задачи \"больнее\"?\n",
    "3.  Сравни **`precision`** и **`recall`** для интересующего класса: Есть ли между ними сильный перекос?\n",
    "4.  Сравни **`macro avg`** и **`weighted avg`**: Если они сильно отличаются, значит, модель плохо справляется с редким классом.\n",
    "5.  Посмотри **`AUC`**: Насколько хорошо модель в принципе способна разделять классы?\n",
    "6.  При необходимости (если цена ошибок разная) — подбери оптимальный **порог**, чтобы сбалансировать `precision` и `recall` под вашу задачу."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a728ef35-0571-45b1-af0c-2b8a7f22acfc",
   "metadata": {},
   "source": [
    "*   **AUC ≈ 0.93**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Анализ по чек-листу:**\n",
    "\n",
    "1.  **`support` (баланс классов):**\n",
    "    *   Класс 0: 29 объектов.\n",
    "    *   Класс 1: 32 объекта.\n",
    "    *   **Вывод:** Классы практически идеально сбалансированы. Это значит, что метрика `accuracy` является надежной и не вводит нас в заблуждение.\n",
    "\n",
    "2.  **`precision` и `recall` по классам:**\n",
    "    *   **Класс 0:** `precision` = 0.86, `recall` = 0.86. Модель одинаково хорошо избегает ложных срабатываний и пропусков для этого класса.\n",
    "    *   **Класс 1:** `precision` = 0.88, `recall` = 0.88. Модель также демонстрирует сбалансированную точность и полноту для второго класса.\n",
    "    *   **Вывод:** Сильного перекоса между `precision` и `recall` нет ни для одного из классов, что говорит о стабильной работе модели.\n",
    "\n",
    "3.  **Сравнение `macro avg` и `weighted avg`:**\n",
    "    *   `macro avg f1-score` = 0.87.\n",
    "    *   `weighted avg f1-score` = 0.87.\n",
    "    *   **Вывод:** Значения практически идентичны, что еще раз подтверждает вывод о хорошем балансе классов. Модель не \"жертвует\" качеством на одном классе ради другого.\n",
    "\n",
    "4.  **`AUC` (качество разделения):**\n",
    "    *   **AUC ≈ 0.93**.\n",
    "    *   **Вывод:** Это очень высокое значение. Оно означает, что модель **отлично разделяет** объекты двух классов. С высокой вероятностью она присвоит случайно выбранному объекту класса 1 более высокую оценку (вероятность), чем случайно выбранному объекту класса 0.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Краткое резюме по модели**\n",
    "\n",
    "*   **Стабильность:** Модель работает одинаково хорошо на обоих классах, без явных перекосов.\n",
    "*   **Разделяющая способность:** Модель уверенно отличает один класс от другого (высокий `AUC`).\n",
    "*   **Общее качество:** Высокое (Accuracy ≈ 87%, F1 ≈ 87%).\n",
    "\n",
    "**Следующий шаг:** По умолчанию, модель работает с порогом `0.5`, что дает хороший баланс `precision` и `recall`. Если для бизнес-задачи цена ошибки **False Negative** (пропустить больного) гораздо выше, чем цена **False Positive** (ложная тревога), то следующим шагом мог бы стать **подбор более низкого порога** (например, `0.4`) для увеличения `recall`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
