{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc0d620f",
   "metadata": {},
   "source": [
    "# Лекция 8: Инжиниринг признаков и Логистическая регрессия\n",
    "\n",
    "**Цели лекции:**\n",
    "1.  Сформировать понимание важности и основных техник инжиниринга признаков.\n",
    "2.  Научиться работать с отсутствующими данными, выбросами и категориальными переменными.\n",
    "3.  Понять, почему линейная регрессия не подходит для задач классификации.\n",
    "4.  Изучить математические основы логистической регрессии, включая логистическую функцию, шансы и log-odds.\n",
    "5.  Освоить ключевые метрики оценки качества моделей классификации (Confusion Matrix, Accuracy, Precision, Recall, F1, ROC/AUC).\n",
    "6.  Рассмотреть расширение логистической регрессии на случай нескольких классов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d46fa95",
   "metadata": {},
   "source": [
    "## Часть 1: Инжиниринг признаков (Feature Engineering)\n",
    "\n",
    "**Инжиниринг признаков (Feature Engineering)** — это процесс использования знаний о предметной области для создания новых признаков из уже существующих. Цель — представить данные в таком виде, который наилучшим образом описывает базовую проблему для алгоритмов машинного обучения. Качество модели на 80% зависит от качества признаков, а не от сложности самого алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7e50bd",
   "metadata": {},
   "source": [
    "#### Три основных подхода в инжиниринге признаков:\n",
    "\n",
    "1.  **Извлечение (Extraction):** Создание новых, более простых признаков из сложных. Например, извлечение дня недели, месяца или времени суток из полной временной метки `timestamp`.\n",
    "2.  **Комбинирование (Combination):** Объединение нескольких признаков в один. Например, создание полиномиальных признаков (`TV_budget * Radio_budget`) для учета их совместного влияния.\n",
    "3.  **Преобразование (Transformation):** Изменение исходных признаков для улучшения их свойств. К этому относится масштабирование (`StandardScaler`), логарифмирование или кодирование категориальных признаков, которое мы рассмотрим ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af3c4fe",
   "metadata": {},
   "source": [
    "### 1.1. Работа с отсутствующими данными (Missing Data)\n",
    "\n",
    "Реальные данные почти никогда не бывают полными. Пропуски могут возникать из-за ошибок сбора данных, сбоев систем или потому, что какой-то признак просто неприменим к объекту (например, 'год постройки гаража' для дома без гаража)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d9bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Создадим простой DataFrame с пропусками\n",
    "data = {'temperature': [25, 26, np.nan, 28, 29, 24],\n",
    "        'humidity': [80, np.nan, 82, 83, 81, 79],\n",
    "        'wind_speed': [10, 12, 11, np.nan, 13, 9]}\n",
    "sample_df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Исходные данные с пропусками:\")\n",
    "print(sample_df)\n",
    "\n",
    "print(\"\\nПоиск пропусков с помощью .isnull().sum():\")\n",
    "print(sample_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc1ce8",
   "metadata": {},
   "source": [
    "#### 1.1.1. Удаление данных\n",
    "\n",
    "Самый простой подход. Можно удалять либо строки (`.dropna(axis=0)`), либо целые столбцы (`.dropna(axis=1)`). Это оправдано, если пропусков очень мало, иначе мы рискуем потерять ценную информацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5bbf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление строк с любым пропущенным значением\n",
    "print(\"DataFrame после удаления строк:\")\n",
    "print(sample_df.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aa3643",
   "metadata": {},
   "source": [
    "#### 1.1.2. Заполнение (импутация) данных\n",
    "\n",
    "Более предпочтительный метод. Пропуски можно заполнять:\n",
    "*   **Простыми значениями:** нулем, средним (`.mean()`), медианой (`.median()`) или модой (самым частым значением).\n",
    "*   **Продвинутыми методами:** например, предсказывая пропущенное значение на основе других признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e384bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заполнение пропусков средним значением по столбцу\n",
    "print(\"DataFrame после заполнения средним:\")\n",
    "print(sample_df.fillna(sample_df.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2362aa17-d00b-4e8a-a01b-f16779c9f873",
   "metadata": {},
   "source": [
    "### 1.2. Работа с дублями (Duplicates)\n",
    "\n",
    "**Дубликаты** — это строки в наборе данных, которые являются полностью идентичными. Наличие дублей может привести к нескольким проблемам:\n",
    "\n",
    "1.  **Искажение результатов:** Дубликаты придают неоправданно больший вес определенным наблюдениям, что может сместить статистические показатели (среднее, медиану) и повлиять на обучение модели.\n",
    "2.  **Утечка данных (Data Leakage):** Самая серьезная проблема. Если одна и та же строка попадет и в обучающую, и в тестовую выборку, модель получит \"бесплатный\" правильный ответ на тестовых данных. Это приведет к искусственно завышенным метрикам качества и ложному представлению о том, как модель будет работать на действительно новых данных.\n",
    "\n",
    "Поэтому **первым шагом** в любом проекте по анализу данных должна быть проверка и удаление полных дубликатов.\n",
    "\n",
    "**Как с ними работать в Pandas:**\n",
    "*   **Обнаружение:** Метод `.duplicated().sum()` позволяет быстро посчитать количество полных дубликатов.\n",
    "*   **Просмотр:** Чтобы увидеть все дублирующиеся строки (включая их \"оригиналы\"), используется `df[df.duplicated(keep=False)]`.\n",
    "*   **Удаление:** Метод `.drop_duplicates()` удаляет дубликаты, по умолчанию оставляя первое вхождение каждой строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a2858f-71a7-49f9-b861-f82de81c4527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим DataFrame с явными дубликатами\n",
    "data = {'Имя': ['Арман', 'Айгерим', 'Бауыржан', 'Арман', 'Алия', 'Айгерим'],\n",
    "        'Возраст': [25, 30, 35, 25, 28, 30],\n",
    "        'Город': ['Астана', 'Алматы', 'Шымкент', 'Астана', 'Караганда', 'Алматы']}\n",
    "df_duplicates = pd.DataFrame(data)\n",
    "\n",
    "print(\"Исходный DataFrame:\")\n",
    "print(df_duplicates)\n",
    "\n",
    "# --- Обнаружение дублей ---\n",
    "print(\"\\n--- Обнаружение ---\")\n",
    "num_duplicates = df_duplicates.duplicated().sum()\n",
    "print(f\"Количество полных дубликатов в данных: {num_duplicates}\")\n",
    "\n",
    "# Посмотрим на сами дублирующиеся строки\n",
    "print(\"\\nОтображение всех дублирующихся строк (включая оригиналы):\")\n",
    "print(df_duplicates[df_duplicates.duplicated(keep=False)])\n",
    "\n",
    "\n",
    "# --- Удаление дублей ---\n",
    "print(\"\\n--- Удаление ---\")\n",
    "df_cleaned = df_duplicates.drop_duplicates()\n",
    "print(\"DataFrame после удаления дублей:\")\n",
    "print(df_cleaned)\n",
    "\n",
    "# --- Проверка ---\n",
    "print(\"\\n--- Проверка ---\")\n",
    "print(f\"Количество дубликатов после очистки: {df_cleaned.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33c8c42",
   "metadata": {},
   "source": [
    "### 1.3. Работа с выбросами (Outliers)\n",
    "\n",
    "**Выброс** — это точка данных, которая сильно отличается от остальных. Выбросы могут искажать результаты обучения, \"перетягивая\" на себя линию регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d12166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Сгенерируем нормальное распределение и добавим выбросы\n",
    "np.random.seed(42)\n",
    "data_normal = np.random.normal(loc=100, scale=10, size=100)\n",
    "data_with_outliers = np.concatenate([data_normal, [180, 190, -50]]) # Добавляем 3 выброса\n",
    "df_outliers = pd.DataFrame(data_with_outliers, columns=['value'])\n",
    "\n",
    "# Визуализируем с помощью boxplot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(data=df_outliers, x='value')\n",
    "plt.title('Обнаружение выбросов с помощью Box Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b7e372",
   "metadata": {},
   "source": [
    "#### Практический пример: Ограничение выбросов по методу межквартильного размаха (IQR)\n",
    "\n",
    "Этот метод является статистическим аналогом \"усов\" на boxplot. Выбросами считаются все точки, которые лежат за пределами:\n",
    "\n",
    "<br>\n",
    "$$ Нижняя\\_граница = Q1 - 1.5 \\cdot IQR $$\n",
    "<br>\n",
    "$$ Верхняя\\_граница = Q3 + 1.5 \\cdot IQR $$\n",
    "<br>\n",
    "\n",
    "где $Q1$ — 25-й перцентиль, $Q3$ — 75-й перцентиль, $IQR = Q3 - Q1$. Вместо удаления, мы можем \"ограничить\" значения, заменив все, что выходит за рамки, на значения границ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7360018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Рассчитаем границы\n",
    "Q1 = df_outliers['value'].quantile(0.25)\n",
    "Q3 = df_outliers['value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"Нижняя граница: {lower_bound:.2f}\")\n",
    "print(f\"Верхняя граница: {upper_bound:.2f}\")\n",
    "\n",
    "# Ограничим значения (clipping)\n",
    "df_clipped = df_outliers.copy()\n",
    "df_clipped['value'] = df_clipped['value'].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "# Визуализируем результат\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(data=df_clipped, x='value')\n",
    "plt.title('Данные после ограничения выбросов')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c48926",
   "metadata": {},
   "source": [
    "### 1.4. Работа с категориальными данными\n",
    "\n",
    "**Категориальные данные** — это переменные, которые содержат метки, а не числовые значения. Они описывают принадлежность объекта к какой-либо группе.\n",
    "\n",
    "Существует два основных типа:\n",
    "1.  **Номинальные (Nominal):** Категории не имеют внутреннего порядка. *Примеры: 'Город' (Москва, Казань), 'Пол' (Мужской, Женский).* \n",
    "2.  **Порядковые (Ordinal):** Категории имеют естественный порядок или ранг. *Примеры: 'Размер' (S, M, L), 'Оценка' (Плохо, Хорошо, Отлично).* \n",
    "\n",
    "Большинство моделей машинного обучения работают с числами, поэтому текстовые категории нужно преобразовать."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99df55cc-fd2f-4ecd-9ec2-4642ffee4b78",
   "metadata": {},
   "source": [
    "#### 1.4.0. Кодирование чисел (Integer/Label Encoding)\n",
    "\n",
    "Это самый простой способ преобразования: каждой уникальной категории присваивается целое число (0, 1, 2 и так далее).\n",
    "\n",
    "**Однако, этот метод нужно использовать с большой осторожностью!**\n",
    "\n",
    "**Проблема: Ложная упорядоченность**\n",
    "\n",
    "Когда мы присваиваем числа `1`, `2`, `3` категориям, большинство моделей машинного обучения воспримут их как упорядоченные величины. Они \"подумают\", что `3 > 2 > 1`.\n",
    "\n",
    "*   **Когда это плохо (для номинальных данных):** Если мы кодируем страны: `{'США': 1, 'Мексика': 2, 'Канада': 3}`, модель может ошибочно сделать вывод, что \"Канада\" в каком-то смысле \"больше\" или \"важнее\", чем \"Мексика\". Это вносит в данные ложную информацию, которая может навредить качеству модели.\n",
    "\n",
    "*   **Когда это хорошо (для порядковых данных):** Если наши категории имеют естественный, логический порядок (например, уровень остроты блюда), то кодирование числами становится правильным выбором. В этом случае порядок `{'Mild': 1, 'Hot': 2, 'Fire': 3}` несет полезную информацию для модели.\n",
    "\n",
    "**Вывод:** Используйте кодирование числами только для **порядковых (ordinal)** признаков. Для **номинальных (nominal)** признаков этот метод, как правило, вреден."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861fd8a9-89bd-4d53-90d4-ddd941bdee6d",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec8-14.png\" alt=\"Кодирование стран\" width=\"400\"></td>\n",
    "    <td><img src=\"https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec8-16.png\" alt=\"Кодирование остроты\" width=\"400\"></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8af117",
   "metadata": {},
   "source": [
    "#### 1.4.1. Преобразование непрерывных данных в категории (Binning)\n",
    "\n",
    "Иногда полезно превратить непрерывный признак (например, возраст) в категориальный. Этот процесс называется **дискретизацией** или **биннингом** (от англ. *bin* — корзина). Это может помочь модели уловить нелинейные зависимости, которые она не смогла бы обнаружить в исходных данных. Например, влияние возраста на какой-либо показатель может быть нелинейным: сначала оно растет, а после определенной точки начинает снижаться. Создание возрастных групп ('Молодой', 'Взрослый', 'Пожилой') позволяет модели присвоить каждой группе свой собственный, независимый вес."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим серию с разными возрастами\n",
    "ages = pd.Series([15, 22, 35, 48, 65, 70, 28, 55])\n",
    "\n",
    "# Определим границы для возрастных групп (бины)\n",
    "# [0, 18) -> 0-17 лет\n",
    "# [18, 35) -> 18-34 лет\n",
    "# [35, 60) -> 35-59 лет\n",
    "# [60, 100) -> 60-99 лет\n",
    "bins = [0, 18, 35, 60, 100]\n",
    "\n",
    "# Определим названия для этих групп\n",
    "labels = ['Ребенок', 'Молодой', 'Взрослый', 'Пожилой']\n",
    "\n",
    "# Используем функцию pd.cut для разбиения данных на категории\n",
    "age_groups = pd.cut(ages, bins=bins, labels=labels, right=False)\n",
    "\n",
    "print(\"Исходный возраст:\")\n",
    "print(ages)\n",
    "print(\"\\nПосле разбиения на категории (Binning):\")\n",
    "print(age_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08e0e16",
   "metadata": {},
   "source": [
    "#### 1.4.2. Преобразование числовых кодов в категории (Type Conversion)\n",
    "\n",
    "Очень часто в реальных данных категориальные признаки уже закодированы числами. Например, в датасете Ames Housing признак `MSSubClass` (тип жилья) представлен числами: 20, 30, 60 и т.д. \n",
    "\n",
    "**Проблема:** Если оставить эти данные как есть, большинство алгоритмов машинного обучения (особенно линейные модели) будут ошибочно интерпретировать их как непрерывные числовые переменные. Модель может предположить, что между классами есть математическая зависимость (например, что класс 60 в 3 раза 'больше' или 'важнее' класса 20), хотя на самом деле это просто уникальные коды.\n",
    "\n",
    "**Решение:** Чтобы избежать этого, необходимо явно указать Pandas, что этот столбец следует рассматривать как категориальный. Самый надежный способ — преобразовать тип данных этого столбца в `object` или `str` **перед** применением One-Hot Encoding. После такого преобразования функция `pd.get_dummies` корректно распознает каждое число как отдельную категорию и создаст для него свой бинарный столбец."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9faa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим DataFrame, имитирующий проблему\n",
    "subclass_df = pd.DataFrame({'MSSubClass': [20, 30, 60, 20, 70]})\n",
    "\n",
    "print(\"Исходный DataFrame и тип данных:\")\n",
    "print(subclass_df)\n",
    "print(f\"Тип данных столбца: {subclass_df['MSSubClass'].dtype}\")\n",
    "\n",
    "# --- ПРАВИЛЬНЫЙ ПОДХОД ---\n",
    "\n",
    "# Шаг 1: Преобразуем тип данных в 'object' (или 'str')\n",
    "subclass_df['MSSubClass'] = subclass_df['MSSubClass'].astype(str)\n",
    "\n",
    "print(\"\\nТип данных после преобразования:\")\n",
    "print(f\"Новый тип данных столбца: {subclass_df['MSSubClass'].dtype}\")\n",
    "\n",
    "# Шаг 2: Теперь применяем One-Hot Encoding\n",
    "dummies = pd.get_dummies(subclass_df, drop_first=True)\n",
    "\n",
    "print(\"\\nРезультат One-Hot Encoding после правильного преобразования типов:\")\n",
    "print(dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baadce5",
   "metadata": {},
   "source": [
    "#### 1.4.3. One-Hot Encoding для номинальных признаков\n",
    "\n",
    "Для **номинальных** признаков (где нет порядка) используется **One-Hot Encoding**. Он создает новый бинарный столбец (0 или 1) для каждой категории. Это стандартный и наиболее безопасный способ кодирования таких данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd897f7c-d429-4344-ae29-c562eacbbcbb",
   "metadata": {},
   "source": [
    "![OHE](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec8-19.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5957ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример с городами\n",
    "cities = pd.Series(['Almaty', 'Astana', 'Almaty', 'Astana', 'Shymkent'])\n",
    "\n",
    "print(\"Исходные данные:\")\n",
    "print(cities)\n",
    "\n",
    "print(\"\\nПосле One-Hot Encoding (с удалением первого столбца):\")\n",
    "# drop_first=True, чтобы избежать ловушки фиктивных переменных\n",
    "print(pd.get_dummies(cities, drop_first=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff9fdc5",
   "metadata": {},
   "source": [
    "## Часть 2: Логистическая регрессия\n",
    "\n",
    "Прежде чем изучать конкретный алгоритм, давайте четко определим, какую задачу мы собираемся решать.\n",
    "\n",
    "### Задача классификации\n",
    "\n",
    "**Классификация (Classification)** — это одна из основных задач машинного обучения с учителем. Ее цель — предсказать, к какому из нескольких предопределенных **классов** (категорий) принадлежит объект. В отличие от задачи регрессии, где мы предсказывали непрерывное число (например, цену дома), здесь мы предсказываем дискретную метку.\n",
    "\n",
    "*   **Регрессия отвечает на вопрос \"Сколько?\":** *Сколько стоит квартира? Какая завтра будет температура?*\n",
    "*   **Классификация отвечает на вопрос \"Какой?\":** *Это письмо — спам или нет? Какое животное на картинке?*\n",
    "\n",
    "### Типы задач классификации\n",
    "\n",
    "Задачи классификации делятся на два основных типа:\n",
    "\n",
    "1.  **Бинарная классификация (Binary Classification):**\n",
    "    *   **Что это:** Задача, в которой существует ровно два взаимоисключающих класса. Это самый распространенный тип классификации.\n",
    "    *   **Примеры:**\n",
    "        *   Медицинский диагноз: есть заболевание (`1`) или нет (`0`).\n",
    "        *   Спам-фильтр: письмо является спамом (`1`) или нет (`0`).\n",
    "        *   Банковский скоринг: клиент вернет кредит (`1`) или не вернет (`0`).\n",
    "        *   Маркетинг: пользователь кликнет на рекламу (`1`) или нет (`0`).\n",
    "\n",
    "2.  **Многоклассовая классификация (Multiclass Classification):**\n",
    "    *   **Что это:** Задача, в которой существует более двух взаимоисключающих классов. Объект может принадлежать только к одному из них.\n",
    "    *   **Примеры:**\n",
    "        *   Распознавание рукописных цифр: `0`, `1`, `2`, ..., `9`.\n",
    "        *   Классификация новостей по темам: 'Политика', 'Спорт', 'Технологии', 'Культура'.\n",
    "        *   Анализ тональности текста: 'Позитивный', 'Нейтральный', 'Негативный'.\n",
    "        *   Классификация цветков ириса (как мы увидим далее): 'Setosa', 'Versicolor', 'Virginica'.\n",
    "\n",
    "### Логистическая регрессия: Основной инструмент для бинарной классификации\n",
    "\n",
    "Фундаментальным и наиболее часто используемым алгоритмом для решения задач **бинарной классификации** является **логистическая регрессия**. Несмотря на слово \"регрессия\" в названии, это именно метод классификации, который предсказывает вероятность принадлежности объекта к одному из двух классов.\n",
    "\n",
    "Хотя ее основное предназначение — это бинарная классификация, существуют методы (например, One-vs-Rest), которые позволяют расширить ее и для решения многоклассовых задач, что мы рассмотрим на примере датасета Ирисов Фишера."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faca6ca8",
   "metadata": {},
   "source": [
    "### 2.1. От регрессии к классификации\n",
    "\n",
    "Представим, что у нас есть данные о студентах (часы подготовки) и результат экзамена (сдал/не сдал). Линейная регрессия здесь не подойдет, так как ее предсказания могут выходить за пределы [0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c333cbc1-1d0c-4dbd-92ea-677b1ec3c59d",
   "metadata": {},
   "source": [
    "![linear-to-logitic](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec8-37.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675442e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Синтетические данные\n",
    "hours = np.array([0.5, 0.75, 1, 1.25, 1.5, 1.75, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 4, 4.25, 4.5, 4.75, 5, 5.5]).reshape(-1, 1)\n",
    "passed = np.array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(hours, passed)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(hours, passed, color='blue', label='Данные')\n",
    "plt.plot(hours, lin_reg.predict(hours), color='red', label='Линейная регрессия')\n",
    "plt.axhline(y=0, color='grey', linestyle='--')\n",
    "plt.axhline(y=1, color='grey', linestyle='--')\n",
    "plt.title('Почему линейная регрессия не подходит для классификации')\n",
    "plt.xlabel('Часы подготовки')\n",
    "plt.ylabel('Результат (0 - не сдал, 1 - сдал)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8b2cb1",
   "metadata": {},
   "source": [
    "### 2.2. Логистическая функция (Сигмоида)\n",
    "\n",
    "Решение — пропустить выход линейной модели через S-образную **сигмоиду**, которая преобразует любое число в вероятность от 0 до 1.\n",
    "\n",
    "<br>\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068e4494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "z = np.linspace(-10, 10, 100)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(z, sigmoid(z))\n",
    "plt.title('График сигмоидной функции')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('$\\sigma(z)$')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847f7621-6634-4ad1-9cee-47e606392439",
   "metadata": {},
   "source": [
    "Пример: income = 1. Вероятность возврата кредита - 90%\n",
    "\n",
    "![ExmplPossibility](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec8-41.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f8b51",
   "metadata": {},
   "source": [
    "### 2.3. Уравнение логистической регрессии\n",
    "\n",
    "Мы подставляем линейную часть $z = w_0 + w_1x_1 + ...$ в сигмоиду. Модель предсказывает вероятность принадлежности к классу '1'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3322ff33-c94f-4afb-9b60-3bbe5382ffd2",
   "metadata": {},
   "source": [
    "![Формула](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec8-48.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d4e588-65e0-4d74-8f74-4e72889d9566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(hours, passed)\n",
    "\n",
    "# Создаем плавную кривую для графика\n",
    "x_test = np.linspace(0, 6, 100).reshape(-1, 1)\n",
    "y_prob = log_reg.predict_proba(x_test)[:, 1] # Вероятность класса 1\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(hours, passed, color='blue', label='Данные')\n",
    "plt.plot(x_test, y_prob, color='green', label='Логистическая регрессия')\n",
    "plt.title('Логистическая регрессия для классификации')\n",
    "plt.xlabel('Часы подготовки')\n",
    "plt.ylabel('Вероятность сдачи экзамена')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f4f74-2849-4412-ad6d-bba42ccd8ac3",
   "metadata": {},
   "source": [
    "2.3.1. Интерпретация: от Вероятностей к Log-Odds и обратно\n",
    "\n",
    "В отличие от линейной регрессии, коэффициенты логистической регрессии $w_i$ нельзя интерпретировать напрямую как \"изменение $y$ при изменении $x_i$ на единицу\". Они влияют на результат нелинейно через сигмоиду. Чтобы понять их смысл, нужно ввести понятия **Шансы (Odds)** и **Логарифм шансов (Log-Odds)**.\n",
    "\n",
    "**Шаг 1: От Вероятности к Шансам (Odds)**\n",
    "\n",
    "Шансы — это отношение вероятности того, что событие произойдет, к вероятности того, что оно не произойдет. Если вероятность сдать экзамен ($p$) равна 0.8, то вероятность не сдать ($1-p$) равна 0.2. Шансы сдать равны 0.8 / 0.2 = 4, или \"4 к 1\".\n",
    "\n",
    "<br>\n",
    "$$ Odds = \\frac{p}{1 - p} $$\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33add5c1-8b54-4212-ad42-118518f42bc4",
   "metadata": {},
   "source": [
    "**Шаг 2: От Шансов к Логарифму Шансов (Log-Odds)**\n",
    "\n",
    "Взяв натуральный логарифм от шансов, мы получаем величину, которая изменяется от $-\\infty$ до $+\\infty$. Это и есть **Log-Odds**.\n",
    "\n",
    "<br>\n",
    "$$ Log\\_Odds = \\ln\\left(\\frac{p}{1 - p}\\right) $$\n",
    "<br>\n",
    "\n",
    "**Ключевая идея логистической регрессии:** Линейная комбинация признаков $z = w_0 + w_1x_1 + ... + w_mx_m$ на самом деле является моделью не для самой вероятности, а для **логарифма шансов**.\n",
    "\n",
    "<br>\n",
    "$$ \\ln\\left(\\frac{p}{1 - p}\\right) = w_0 + w_1x_1 + ... + w_mx_m $$\n",
    "<br>\n",
    "\n",
    "Это означает, что при увеличении признака $x_i$ на одну единицу, **логарифм шансов** увеличивается на $w_i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7967ae-80c5-464b-b010-70b65e6e044e",
   "metadata": {},
   "source": [
    "![logodds](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec8-71.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb33b7-2900-4a28-97b9-5586949c5ac8",
   "metadata": {},
   "source": [
    "**Шаг 3: Обратный путь — от Log-Odds к Вероятности**\n",
    "\n",
    "Как, зная предсказанный моделью Log-Odds ($z$), получить вероятность ($p$)? Давайте выразим $p$ из уравнения выше:\n",
    "\n",
    "1. Возьмем экспоненту от обеих частей: $$ \\frac{p}{1 - p} = e^z $$\n",
    "2. Домножим на $(1-p)$: $$ p = e^z \\cdot (1-p) $$\n",
    "3. Раскроем скобки: $$ p = e^z - p \\cdot e^z $$\n",
    "4. Перенесем все слагаемые с $p$ налево: $$ p + p \\cdot e^z = e^z $$\n",
    "5. Вынесем $p$ за скобки: $$ p(1 + e^z) = e^z $$\n",
    "6. Получим финальную формулу: $$ p = \\frac{e^z}{1 + e^z} $$\n",
    "\n",
    "Если разделить числитель и знаменатель на $e^z$, мы получим уже знакомую нам формулу сигмоиды:\n",
    "\n",
    "<br>\n",
    "$$ p = \\frac{e^z/e^z}{(1+e^z)/e^z} = \\frac{1}{1/e^z + 1} = \\frac{1}{1 + e^{-z}} = \\sigma(z) $$\n",
    "<br>\n",
    "\n",
    "Таким образом, **сигмоида — это просто математический способ перейти от предсказанных моделью Log-Odds обратно к вероятности.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2947f284-58d1-4759-bd93-085ad513f9c5",
   "metadata": {},
   "source": [
    "### 2.4. Обучение модели: Метод максимального правдоподобия и Log Loss\n",
    "\n",
    "Как модель находит оптимальные веса `w`? В отличие от линейной регрессии, где мы минимизировали сумму квадратов ошибок (MSE), в логистической регрессии используется **Метод максимального правдоподобия (Maximum Likelihood Estimation, MLE)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89db168-50a5-4819-9d6f-7a79233d6db3",
   "metadata": {},
   "source": [
    "![whatisthebest](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec8-77.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794cc93c-8a1b-4c81-bc0b-49157fcb179b",
   "metadata": {},
   "source": [
    "\n",
    "**Интуиция на простом примере:**\n",
    "Представьте, что вы подбросили монетку 10 раз и получили 7 орлов и 3 решки. Какова, по-вашему, вероятность выпадения орла ($p$)? Большинство скажет, что 0.7. Вы интуитивно использовали MLE.\n",
    "\n",
    "**Логика MLE:** Давайте найдем такое значение параметра ($p$), при котором вероятность получить **именно те данные, которые мы наблюдали**, будет максимальной.\n",
    "*   Если бы $p=0.5$ (честная монета), вероятность нашей последовательности была бы $(0.5)^7 \\cdot (1-0.5)^3 \\approx 0.00097$.\n",
    "*   Если бы $p=0.7$, вероятность нашей последовательности была бы $(0.7)^7 \\cdot (1-0.7)^3 \\approx 0.00222$.\n",
    "\n",
    "Вероятность выше при $p=0.7$. MLE — это процесс нахождения такого $p$, который максимизирует эту вероятность (правдоподобие).\n",
    "\n",
    "**Применительно к логистической регрессии:** Алгоритм подбирает такие веса `w`, которые максимизируют правдоподобие того, что для каждого объекта из обучающей выборки модель выдаст вероятность, близкую к его истинной метке (близкую к 1 для класса '1' и близкую к 0 для класса '0').\n",
    "\n",
    "Математически, максимизация правдоподобия эквивалентна минимизации отрицательного логарифма правдоподобия. Эта функция потерь для логистической регрессии называется **Log Loss** (или бинарной кросс-энтропией).\n",
    "\n",
    "Вот как выглядит эта функция для **одного** объекта:\n",
    "\n",
    "<br>\n",
    "$$ Loss(y, \\hat{p}) = -[y \\cdot \\log(\\hat{p}) + (1 - y) \\cdot \\log(1 - \\hat{p})] $$\n",
    "<br>\n",
    "\n",
    "где:\n",
    "*   $y$ — истинная метка класса (0 или 1).\n",
    "*   $\\hat{p}$ — предсказанная моделью вероятность того, что объект принадлежит к классу 1.\n",
    "\n",
    "**Давайте разберемся, как она работает:**\n",
    "\n",
    "*   **Случай 1: Истинная метка y = 1.**\n",
    "    Формула упрощается до $Loss(1, \\hat{p}) = -[1 \\cdot \\log(\\hat{p}) + (0) \\cdot \\log(1 - \\hat{p})] = -\\log(\\hat{p})$.\n",
    "    Чтобы минимизировать эту потерю, значение $-\\log(\\hat{p})$ должно быть как можно меньше. Это происходит, когда $\\log(\\hat{p})$ максимален, то есть когда $\\hat{p}$ стремится к **1**. Это именно то, что нам нужно!\n",
    "\n",
    "*   **Случай 2: Истинная метка y = 0.**\n",
    "    Формула упрощается до $Loss(0, \\hat{p}) = -[0 \\cdot \\log(\\hat{p}) + (1) \\cdot \\log(1 - \\hat{p})] = -\\log(1 - \\hat{p})$.\n",
    "    Чтобы минимизировать эту потерю, значение $-\\log(1 - \\hat{p})$ должно быть как можно меньше. Это происходит, когда $\\log(1 - \\hat{p})$ максимален, то есть когда $1 - \\hat{p}$ стремится к 1, а само $\\hat{p}$ — к **0**. Снова, именно то, что нам нужно!\n",
    "\n",
    "Чтобы получить общую **функцию стоимости (Cost Function)** для всей обучающей выборки из N объектов, мы просто усредняем эту потерю:\n",
    "\n",
    "<br>\n",
    "$$ J(w) = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\cdot \\log(\\hat{p}_i) + (1 - y_i) \\cdot \\log(1 - \\hat{p}_i)] $$\n",
    "<br>\n",
    "\n",
    "Поскольку предсказанная вероятность $\\hat{p}_i$ является результатом применения сигмоидной функции к линейной комбинации признаков ($\\hat{p}_i = \\sigma(w^T x_i)$), то полная формула функции стоимости, которую необходимо минимизировать, выглядит следующим образом:\n",
    "\n",
    "<br>\n",
    "$$ J(w) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\cdot \\log\\left( \\frac{1}{1 + e^{-w^T x_i}} \\right) + (1 - y_i) \\cdot \\log\\left(1 - \\frac{1}{1 + e^{-w^T x_i}}\\right) \\right] $$\n",
    "<br>\n",
    "\n",
    "Хотя эта формула выглядит сложной, она является выпуклой, что гарантирует наличие единственного глобального минимума. Как и в случае с линейной регрессией, для нахождения этого минимума (то есть оптимальных весов `w`) используется **метод градиентного спуска**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21689892-d0b9-40df-a549-1bc7090fe7ae",
   "metadata": {},
   "source": [
    "![logodds-p](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec8-100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91450db2",
   "metadata": {},
   "source": [
    "### 2.5. Мультиклассовая Логистическая Регрессия\n",
    "\n",
    "Что делать, если у нас больше двух классов? Стандартный подход называется **One-vs-Rest (OvR)**. Для K классов строится K бинарных классификаторов: первый отличает класс 1 от всех остальных, второй — класс 2 от всех остальных, и так далее. Для нового объекта запускаются все K классификаторов, и выбирается тот, который выдал наибольшую уверенность (вероятность).\n",
    "\n",
    "Рассмотрим это на классическом датасете **Ирисы Фишера**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdef0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "df_iris = pd.DataFrame(X_iris, columns=iris.feature_names)\n",
    "df_iris['species'] = y_iris\n",
    "\n",
    "# Заменим числовые метки на названия для наглядности\n",
    "target_names = iris.target_names\n",
    "df_iris['species'] = df_iris['species'].apply(lambda x: target_names[x])\n",
    "\n",
    "print(\"Первые 5 строк датасета Ирисов Фишера:\")\n",
    "print(df_iris.head())\n",
    "\n",
    "sns.pairplot(df_iris, hue='species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fbe17f",
   "metadata": {},
   "source": [
    "На `pairplot` видно, что класс `setosa` легко отделим, а `versicolor` и `virginica` частично пересекаются. Обучим модель, чтобы разделить все три класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b36633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Подготовка данных\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler_iris = StandardScaler()\n",
    "X_train_iris_scaled = scaler_iris.fit_transform(X_train_iris)\n",
    "X_test_iris_scaled = scaler_iris.transform(X_test_iris)\n",
    "\n",
    "# Обучение модели. Scikit-learn автоматически использует OvR для мультиклассовой задачи.\n",
    "log_reg_multi = LogisticRegression()\n",
    "log_reg_multi.fit(X_train_iris_scaled, y_train_iris)\n",
    "\n",
    "# Оценка\n",
    "y_pred_iris = log_reg_multi.predict(X_test_iris_scaled)\n",
    "\n",
    "print(\"Матрица ошибок для Ирисов Фишера:\")\n",
    "print(confusion_matrix(y_test_iris, y_pred_iris))\n",
    "print(\"\\nОтчет о классификации:\")\n",
    "print(classification_report(y_test_iris, y_pred_iris, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea90c0c9",
   "metadata": {},
   "source": [
    "## Часть 3: Метрики оценки моделей классификации\n",
    "\n",
    "Метрики для регрессии (MAE, RMSE) здесь не подходят. Для классификации используется свой набор метрик, основанный на **Матрице ошибок (Confusion Matrix)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f923d3aa",
   "metadata": {},
   "source": [
    "### 3.1. Матрица ошибок (Confusion Matrix)\n",
    "\n",
    "Это таблица, которая показывает, сколько предсказаний модель сделала правильно, а сколько — неправильно, для каждого класса. Она является основой для всех остальных метрик.\n",
    "\n",
    "*   **True Positive (TP):** Истинно-положительные. Модель предсказала '1', и это был '1'.\n",
    "*   **True Negative (TN):** Истинно-отрицательные. Модель предсказала '0', и это был '0'.\n",
    "*   **False Positive (FP):** Ложно-положительные (Ошибка I рода). Модель предсказала '1', а на самом деле это был '0'.\n",
    "*   **False Negative (FN):** Ложно-отрицательные (Ошибка II рода). Модель предсказала '0', а на самом деле это был '1'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d27f1c-593b-46e8-b500-cdf5fb6c2135",
   "metadata": {},
   "source": [
    "![confusionmatrix](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec8-126.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35da37fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Примерные истинные значения и предсказания\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
    "y_pred = [1, 1, 1, 0, 0, 1, 0, 0, 1, 0]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Класс 0', 'Класс 1'], yticklabels=['Класс 0', 'Класс 1'])\n",
    "plt.xlabel('Предсказанные значения')\n",
    "plt.ylabel('Истинные значения')\n",
    "plt.title('Матрица ошибок')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee331f4a-99b4-49e8-a0a0-b61aaa51d99b",
   "metadata": {},
   "source": [
    "### 3.2. Основные метрики\n",
    "\n",
    "#### **Accuracy (Доля правильных ответов)**\n",
    "*   **Вопрос:** Какова доля всех правильных предсказаний (и положительных, и отрицательных) в общем количестве наблюдений?\n",
    "*   **Формула:** $$ Accuracy = \\frac{TP+TN}{TP+TN+FP+FN} $$\n",
    "*   **Диапазон:** от 0 до 1 (или от 0% до 100%).\n",
    "    *   **Ближе к 1:** Модель делает много правильных предсказаний в целом.\n",
    "    *   **Ближе к 0:** Модель часто ошибается.\n",
    "*   **Интерпретация:** Простая и понятная метрика. Однако, она может вводить в заблуждение на **несбалансированных данных** — ситуациях, когда одного класса гораздо больше, чем другого. Это явление называется **Парадокс точности**.\n",
    "\n",
    "**Пример Парадокса точности:**\n",
    "Представим задачу диагностики редкого заболевания. В выборке 1000 человек: 990 здоровы (класс 0) и 10 больны (класс 1). Мы создали очень простую (и бесполезную) модель, которая всегда предсказывает \"здоров\" (класс 0).\n",
    "*   **TN:** 990 (правильно угадала здоровых).\n",
    "*   **TP:** 0 (не нашла ни одного больного).\n",
    "*   **FP:** 0 (никого не назвала больным).\n",
    "*   **FN:** 10 (пропустила всех больных).\n",
    "\n",
    "Ее Accuracy будет: $$ Accuracy = \\frac{0 + 990}{0 + 990 + 0 + 10} = \\frac{990}{1000} = 99\\\\% $$. \n",
    "Точность 99% выглядит фантастически, но модель абсолютно бесполезна, так как не решает главную задачу — находить больных. Поэтому для несбалансированных данных `Accuracy` использовать нельзя. Нужны другие метрики."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c31942-7304-4871-bf90-22982c6739ef",
   "metadata": {},
   "source": [
    "![Precision1](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec8-134.png)\n",
    "\n",
    "![Precision2](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec8-137.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a7118-203b-41a7-b0a8-800b463845c7",
   "metadata": {},
   "source": [
    "#### **Recall (Полнота, или Чувствительность)**\n",
    "*   **Вопрос:** Из всех реальных 'положительных' объектов, какую долю мы смогли найти?\n",
    "*   **Формула:** $$ Recall = \\frac{TP}{TP + FN} $$\n",
    "*   **Диапазон:** от 0 до 1.\n",
    "    *   **Ближе к 1:** Модель находит почти все объекты положительного класса.\n",
    "    *   **Ближе к 0:** Модель пропускает большинство объектов положительного класса.\n",
    "*   **Интерпретация:** Recall важна, когда цена ошибки **False Negative** высока. *Пример: медицинская диагностика. Пропустить больного человека (FN) — катастрофа. Мы хотим, чтобы модель находила как можно больше реально больных, даже если при этом она иногда будет ошибочно отправлять здоровых на доп. обследование (FP). Здесь важен высокий Recall.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90379b31-99b4-4794-b147-ca42b729901e",
   "metadata": {},
   "source": [
    "![Recall1](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec8-146.png)\n",
    "\n",
    "![Recall2](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec8-148.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5564fb8c-d652-48e9-aa64-e1b5f92b6448",
   "metadata": {},
   "source": [
    "#### **Precision (Точность)**\n",
    "*   **Вопрос:** Из всех объектов, которые модель назвала 'положительными', какая доля действительно была 'положительной'?\n",
    "*   **Формула:** $$ Precision = \\frac{TP}{TP + FP} $$\n",
    "*   **Диапазон:** от 0 до 1.\n",
    "    *   **Ближе к 1:** Модель очень точна в своих положительных предсказаниях. Если она говорит \"Да\", то ей можно верить.\n",
    "    *   **Ближе к 0:** Большинство положительных предсказаний модели — ложные тревоги.\n",
    "*   **Интерпретация:** Precision важна, когда цена ошибки **False Positive** высока. *Пример: спам-фильтр. Если модель пометит важное письмо как спам (FP), это большая проблема. Мы хотим, чтобы модель была очень уверена в своих \"спам\"-вердиктах, то есть имела высокий Precision.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb16f3c9-05cd-402b-8ed6-1fc6c8080814",
   "metadata": {},
   "source": [
    "![Precision1](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec8-150.png)\n",
    "\n",
    "![Precision2](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec8-152.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b339f393-a910-42e4-a559-aeb2524ac180",
   "metadata": {},
   "source": [
    "\n",
    "#### **F1-Score**\n",
    "*   **Вопрос:** Как найти баланс между Precision и Recall?\n",
    "*   **Формула:** $$ F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall} $$\n",
    "*   **Диапазон:** от 0 до 1.\n",
    "    *   **Ближе к 1:** У модели хороший баланс между точностью и полнотой.\n",
    "    *   **Ближе к 0:** У модели низкая либо точность, либо полнота, либо и то, и другое.\n",
    "*   **Интерпретация:** Гармоническое среднее между Precision и Recall. Эта метрика полезна, когда важны обе ошибки (FP и FN) и нужно найти компромисс между ними. Она наказывает модели, у которых одна из метрик (Precision или Recall) очень низкая."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cb2439",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776690dd",
   "metadata": {},
   "source": [
    "### 3.3. ROC-кривая и AUC\n",
    "\n",
    "Логистическая регрессия выдает вероятности. Чтобы получить классы (0 или 1), мы используем порог (обычно 0.5). **ROC-кривая** показывает качество модели при *всех возможных* пороговых значениях, строя график зависимости True Positive Rate (Recall) от False Positive Rate.\n",
    "\n",
    "**AUC (Площадь под кривой)** — это интегральная метрика качества, не зависящая от порога. \n",
    "*   AUC = 1.0 — идеальный классификатор.\n",
    "*   AUC = 0.5 — случайное угадывание."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae750a29-e9cb-4723-85d1-7cb683578b9b",
   "metadata": {},
   "source": [
    "Идеальная ROC & Реалистичная ROC\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec8-177.png\" alt=\"Идеальная\" width=\"400\"></td>\n",
    "    <td><img src=\"https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec8-162.png\" alt=\"Реалистичная\" width=\"400\"></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d988660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "# Используем модель, обученную на данных о подготовке к экзамену\n",
    "plot_roc_curve(log_reg, hours, passed)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Случайное угадывание (AUC = 0.5)')\n",
    "plt.title('ROC-кривая')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
