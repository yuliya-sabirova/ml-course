{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5223ceb8-revised",
      "metadata": {},
      "source": [
        "# 7-дәріс: Модельдің күрделілігі, Артық оқыту және онымен күресу әдістері\n",
        "\n",
        "**Дәріс мақсаттары:**\n",
        "1.  Сызықтық емес тәуелділіктерді модельдеу тәсілі ретінде полиномдық регрессияны зерттеу.\n",
        "2.  ML-дің орталық мәселесін тереңінен түсіну: ығысу мен дисперсия арасындағы ымыра (bias-variance tradeoff), және оның көріністері — кем оқыту мен артық оқыту.\n",
        "3.  Модельдерді диагностикалау құралдарын меңгеру: оқыту қисықтарын және қалдықтарды визуалды талдау.\n",
        "4.  Модельдің жалпылау қабілетін сенімді бағалау әдісі ретінде кросс-валидацияны меңгеру.\n",
        "5.  Модельдің күрделілігін бақылау арқылы артық оқытумен күресу әдісі ретінде реттеу (L1 және L2) идеясын түсіну."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b01804e-48b8-4494-840a-3a79fd08bbe9-moved",
      "metadata": {},
      "source": [
        "## 1. Полиномдық регрессия: Сызықтық жеткіліксіз болғанда\n",
        "\n",
        "Өткен дәрісте біз сызықтық регрессияны зерттедік. Бірақ егер деректердегі тәуелділік сызықтық болмаса ше? Қарапайым түзу сызық күрделі деректерді жақсы сипаттай алмайды."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ac858ab-cce4-40a5-b3ac-28b7bd5a5d20-moved",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Сызықтық емес (квадраттық) деректерді генерациялайық\n",
        "np.random.seed(0)\n",
        "m = 100\n",
        "X_poly_data = 6 * np.random.rand(m, 1) - 3\n",
        "y_poly_data = 0.5 * X_poly_data**2 + X_poly_data + 2 + np.random.randn(m, 1)\n",
        "\n",
        "# Оларды қарапайым сызықтық регрессиямен сипаттауға тырысайық\n",
        "plain_lin_reg = LinearRegression()\n",
        "plain_lin_reg.fit(X_poly_data, y_poly_data)\n",
        "\n",
        "plt.scatter(X_poly_data, y_poly_data, alpha=0.7, label='Бастапқы деректер')\n",
        "plt.plot(X_poly_data, plain_lin_reg.predict(X_poly_data), color='red', linewidth=2, label='Қарапайым сызықтық регрессия')\n",
        "plt.title('Қарапайым сызықтық регрессияның шектеулері')\n",
        "plt.xlabel('X белгісі')\n",
        "plt.ylabel('y нысаналы айнымалысы')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bec1fb4-6355-4928-84a3-d937d6c9ff07-moved",
      "metadata": {},
      "source": [
        "### 1.1. Идея: Модельді белгілер арқылы күрделендіру\n",
        "\n",
        "Егер біз қолданыстағы белгілерден **жасанды түрде жаңа белгілер жасасақ**, сызықтық модельді қисықты сипаттауға мәжбүрлей аламыз. Бұл тәсіл **Полиномдық регрессия** деп аталады.\n",
        "\n",
        "Біз модельді \"алдап\", оған тек бастапқы $x_1$ белгісін ғана емес, сонымен қатар оның дәрежелерін ($x_1^2, x_1^3, ...$) де береміз. $x_1$ және $x_2 = x_1^2$ белгілері үшін сызықтық регрессия теңдеуі келесідей болады:\n",
        "\n",
        "$$ \\hat{y} = w_0 + w_1x_1 + w_2x_2 $$\n",
        "\n",
        "Егер $x_2 = x_1^2$ -ты кері қойсақ, біз мынаны аламыз:\n",
        "\n",
        "$$ \\hat{y} = w_0 + w_1x_1 + w_2x_1^2 $$\n",
        "\n",
        "Бұл парабола теңдеуі! Сызықтық регрессия моделі үшін ештеңе өзгерген жоқ: ол бұрынғысынша белгілердің *сызықтық* комбинациясын іздейді. Бірақ біз белгілерді түрлендіргеніміздің арқасында қорытынды функция сызықтық емес болады."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a46b9a7d-d57c-4d42-90a6-24e2511f2ebe-moved",
      "metadata": {},
      "source": [
        "### 1.2. Белгілердің өзара әрекеттесуі (Interaction Terms)\n",
        "\n",
        "Бізде бірнеше бастапқы белгілер болғанда (мысалы, $x_1$ және $x_2$), полиномдық белгілер генераторы олардың дәрежелерін ($x_1^2, x_2^2, ...$) ғана емес, сонымен қатар олардың **көбейтінділерін ($x_1x_2, ...$)** де жасайды. Олардың мағынасы — бір белгінің әсері екіншісінің мәніне байланысты болған кездегі **синергетикалық эффектіні** ұстау.\n",
        "\n",
        "**Өмірден мысал 1: Жарнамалық кампаниялар**\n",
        "\n",
        "Instagram-дағы ($x_1$) және көше баннерлеріндегі ($x_2$) жарнама бюджетіне негізделген сатылымды (`y`) болжап жатырмыз деп елестетіңіз.\n",
        "Қарапайым $y = w_0 + w_1x_1 + w_2x_2$ моделі әрбір арнаның тәуелсіз үлес қосатынын болжайды. Бірақ егер адам Instagram-да жарнаманы көріп, содан кейін көшедегі баннер оған тауарды \"есіне түсіріп\", эффектіні **күшейтсе** ше? Бұл бірлескен эффект $w_3(x_1x_2)$ қосылғышымен модельденеді.\n",
        "\n",
        "**Өмірден мысал 2: Ауыл шаруашылығы**\n",
        "\n",
        "Күн сәулелі күндер санына ($x_1$) және жауын-шашын мөлшеріне ($x_2$) негізделген егін өнімділігін (`y`) болжаймыз. Жаңбырсыз көп күн (қуаңшылық) немесе күнсіз үнемі жаңбыр (шіру) — жаман. Ең жақсы өнім осы факторлардың **тепе-теңдігі** кезінде болады, оны дәл $x_1 \\cdot x_2$ өзара әрекеттесу белгісі ұстайды."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ef1d09a-2437-450a-a945-484da5a19f7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "\n",
        "\n",
        "# Конвейер (pipeline) құрамыз: алдымен полиномдық белгілерді қосамыз, содан кейін сызықтық регрессияны оқытамыз\n",
        "poly_reg_model = make_pipeline(PolynomialFeatures(degree=2, include_bias=False), \n",
        "                               LinearRegression())\n",
        "\n",
        "# Модельді оқытамыз\n",
        "poly_reg_model.fit(X_poly_data, y_poly_data)\n",
        "\n",
        "# Тегіс қисықты салу үшін деректерді дайындаймыз\n",
        "X_plot = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
        "y_plot = poly_reg_model.predict(X_plot)\n",
        "\n",
        "# Нәтижені визуализациялаймыз\n",
        "plt.scatter(X_poly_data, y_poly_data, alpha=0.7, label='Бастапқы деректер')\n",
        "plt.plot(X_plot, y_plot, color='red', linewidth=2, label='Полиномдық регрессия (2 дәреже)')\n",
        "plt.title('Полиномдық регрессия жұмысының нәтижесі')\n",
        "plt.xlabel('X белгісі')\n",
        "plt.ylabel('y нысаналы айнымалысы')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5d0d5d0-c534-456f-9228-09132c3e8264-moved",
      "metadata": {},
      "source": [
        "## 2. Артық оқыту және кем оқыту мәселесі\n",
        "\n",
        "Полиномдық белгілер — бұл қуатты құрал. Алайда, ол жаңа маңызды сұрақ тудырады: **қай жерде тоқтау керек?** Полиномның қандай дәрежесін таңдау керек? Тым үлкен күрделілік **артық оқытуға** әкеледі."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee034f5d-9344-4aed-8e4b-f95a9f56b91c-moved",
      "metadata": {},
      "source": [
        "### 2.1. Ығысу (Bias) және Дисперсия (Variance)\n",
        "\n",
        "Бұл машиналық оқытудағы орталық мәселе.\n",
        "\n",
        "*   **Кем оқыту (Underfitting, High Bias):** Модель тым қарапайым және деректердегі негізгі заңдылықтарды ұстай алмайды. Ол **оқыту және жаңа деректерде де нашар сапа** көрсетеді.\n",
        "*   **Артық оқыту (Overfitting, High Variance):** Модель тым күрделі. Ол тек заңдылықтарды үйреніп қана қоймай, оқыту іріктемесінің кездейсоқ шуын \"жаттап алды\". Ол **оқыту деректерінде тамаша сапа, бірақ жаңа деректерде өте нашар сапа** көрсетеді.\n",
        "\n",
        "**Алтын орта:** Бізге заңдылықтарды жақсы жалпылайтын және ескі де, жаңа да деректерде бірдей жақсы жұмыс істейтін модель қажет. Бұл **жақсы жалпылау қабілеті** деп аталады."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vis_overfitting_1",
      "metadata": {},
      "source": [
        "Мұны визуалды түрде былай көрсетуге болады:\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <td><img src=\"https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec7-11.png\" alt=\"Жақсы модель\" width=\"400\"></td>\n",
        "    <td><img src=\"https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec7-1.png\" alt=\"Артық оқытылған модель\" width=\"400\"></td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36f3dedc-f4d5-48ca-8f44-3bcda17d08fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "degrees = [1, 2, 20]\n",
        "for i, degree in enumerate(degrees):\n",
        "    ax = plt.subplot(1, len(degrees), i + 1)\n",
        "    \n",
        "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "    model.fit(X_poly_data, y_poly_data)\n",
        "    \n",
        "    X_plot = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
        "    y_plot = model.predict(X_plot)\n",
        "    \n",
        "    plt.scatter(X_poly_data, y_poly_data, alpha=0.5)\n",
        "    plt.plot(X_plot, y_plot, color='red', linewidth=2)\n",
        "    plt.title(f'Полином дәрежесі = {degree}')\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('y')\n",
        "    plt.ylim(0, 10)\n",
        "    \n",
        "    if degree == 1:\n",
        "        plt.text(0.5, 8, \"Кем оқыту (High Bias)\", fontsize=12)\n",
        "    if degree == 2:\n",
        "        plt.text(0, 8, \"Оңтайлы модель\", fontsize=12)\n",
        "    if degree == 20:\n",
        "        plt.text(-2.5, 1, \"Артық оқыту (High Variance)\", fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "970f6d9b-a4b5-4f40-a3ea-bb99613095c9-moved",
      "metadata": {},
      "source": [
        "## 3. Модельді диагностикалау және бағалау\n",
        "\n",
        "**Басты мәселе:** Модельдің артық оқытылғанын қалай түсінуге және оңтайлы күрделілікті қалай табуға болады?\n",
        "\n",
        "**Негізгі қағида:** Модельдің соңғы сапасын ешқашан ол оқытылған деректерде бағаламау керек. Ол үшін деректерді бөліктерге бөледі."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1216520-bf8f-4a7b-8079-9aa7d5283f8e-moved",
      "metadata": {},
      "source": [
        "### 3.1. Деректерді бөлу: Train / Validation / Test\n",
        "\n",
        "*   **Оқыту іріктемесі (train set, ~60-80%):** Модельді оқыту үшін қолданылады (`w` салмақтарын таңдау).\n",
        "*   **Валидациялық іріктеме (validation set, ~10-20%):** **Модельді және оның гиперпараметрлерін таңдау** үшін қолданылады (мысалы, оңтайлы полином дәрежесін). Біз *валидациялық* деректерде ең аз қате көрсететін модельді таңдаймыз.\n",
        "*   **Тест іріктемесі (test set, ~10-20%):** \"Тиісуге болмайтын қор\". Таңдалған модельдің сапасын соңғы, әділ бағалау үшін **тек бір рет** қолданылады."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vis_learning_curves_1",
      "metadata": {},
      "source": [
        "Модельдің күрделілігіне байланысты оқыту және тест іріктемелеріндегі қателердің мінез-құлқы — бұл классикалық диагностика әдісі.\n",
        "\n",
        "![Оқыту қисықтары](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec7-2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "790188b4-3de1-4bb8-8e37-336bc80eee6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_poly_data, y_poly_data, test_size=0.3, random_state=10)\n",
        "\n",
        "train_errors, val_errors = [], []\n",
        "degrees = range(1, 15)\n",
        "\n",
        "for degree in degrees:\n",
        "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    y_train_predict = model.predict(X_train)\n",
        "    y_val_predict = model.predict(X_val)\n",
        "    \n",
        "    train_errors.append(np.sqrt(mean_squared_error(y_train, y_train_predict)))\n",
        "    val_errors.append(np.sqrt(mean_squared_error(y_val, y_val_predict)))\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(degrees, train_errors, 'r-+', linewidth=2, label='Оқытудағы қате (Train)')\n",
        "plt.plot(degrees, val_errors, 'b-', linewidth=3, label='Валидациядағы қате (Validation)')\n",
        "plt.legend(loc='upper right', fontsize=14)\n",
        "plt.xlabel('Модель күрделілігі (полином дәрежесі)', fontsize=14)\n",
        "plt.ylabel('RMSE', fontsize=14)\n",
        "plt.title('Оқыту қисықтары', fontsize=16)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f5f47f1-3962-4fba-bc2d-9fd08378b773-moved",
      "metadata": {},
      "source": [
        "**Графиктен қорытынды (Оқыту қисықтары):**\n",
        "*   **Оқытудағы** қате күрделілік артқан сайын үнемі төмендейді — модель көрген деректерге барған сайын жақсы бейімделеді.\n",
        "*   **Валидациядағы** қате алдымен төмендеп, минимумға жетеді (біздің жағдайда 2-3 дәрежеде), содан кейін өсе бастайды. Бұл модельдің артық оқытыла бастайтын нүктесі.\n",
        "\n",
        "**Біздің мақсатымыз — валидациялық қисықтағы минимумға сәйкес келетін күрделіліктегі модельді табу.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c347348-38ed-48aa-9070-9da91ee58ed0-moved",
      "metadata": {},
      "source": [
        "### 3.2. Қалдықтарды визуалды талдау\n",
        "\n",
        "Қалдықтар ($y_{True} - y_{pred}$) — бұл біздің модельдің қателері. Идеалды жағдайда, олар заңдылықсыз кездейсоқ шу болуы керек.\n",
        "\n",
        "**Қалдықтар графигінен не іздейміз:**\n",
        "1.  **Кездейсоқ таралу:** Нүктелер нөлдік сызықтың айналасында ретсіз шашыраңқы орналасқан.\n",
        "2.  **Үлгілердің болмауы:** Егер құрылым көрінсе (мысалы, парабола), бұл модель қандай да бір тәуелділікті ұстай алмағанын білдіреді.\n",
        "3.  **Гомоскедастикалық:** Нүктелердің шашырауы X осінің бойында бірдей болуы керек. Егер шашырау артса (шұңғыл пішіні), бұл гетероскедастикалық деп аталады."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d76164ff-65ad-4493-85bd-beac1244a505",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Оңтайлы дәрежесі = 2 болатын модельді алайық\n",
        "import seaborn as sns\n",
        "optimal_model = make_pipeline(PolynomialFeatures(2), LinearRegression())\n",
        "optimal_model.fit(X_train, y_train)\n",
        "y_val_pred = optimal_model.predict(X_val)\n",
        "residuals = y_val.flatten() - y_val_pred.flatten()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=y_val_pred.flatten(), y=residuals)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.title('Қалдықтар графигі (Residual Plot)')\n",
        "plt.xlabel('Болжанған мәндер')\n",
        "plt.ylabel('Қалдықтар (қателер)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vis_anscombe",
      "metadata": {},
      "source": [
        "Қалдықтарды талдау тек метрикалар арқылы көрінбейтін мәселелерді анықтауға көмектеседі. Классикалық мысал — **Энскомб квартеті**: бірдей статистикасы және бірдей регрессия сызығы бар, бірақ құрылымы мүлдем әртүрлі төрт деректер жиынтығы.\n",
        "\n",
        "![Энскомб квартеті](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec7-3.png)\n",
        "\n",
        "Тек бірінші деректер жиынтығы үшін (жоғары сол жақта) сызықтық регрессия адекватты модель болып табылады. Қалған жағдайларда қалдықтарды талдау айқын мәселелерді көрсетер еді."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "137ca34c-3fe2-4632-9b88-0b2f0398726b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import seaborn as sns\n",
        "\n",
        "# Оңтайлы дәрежесі = 2 болатын модельді алайық\n",
        "optimal_model = make_pipeline(PolynomialFeatures(2), LinearRegression())\n",
        "optimal_model.fit(X_train, y_train)\n",
        "y_val_pred = optimal_model.predict(X_val)\n",
        "residuals = y_val.flatten() - y_val_pred.flatten()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=y_val_pred.flatten(), y=residuals)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.title('Қалдықтар графигі (Residual Plot)')\n",
        "plt.xlabel('Болжанған мәндер')\n",
        "plt.ylabel('Қалдықтар (қателер)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96688695-0d03-415d-9ddb-9ba8b405538d-moved",
      "metadata": {},
      "source": [
        "### 3.3. Кросс-валидация әдісі (Cross-Validation)\n",
        "\n",
        "Кәдімгі Train/Validation Split-тің мәселесі — бағалаудың кездейсоқ бөлуге қатты тәуелді болуы. **K-Fold Cross-Validation** — бұл сенімдірек бағалау әдісі:\n",
        "\n",
        "1.  Оқыту іріктемесі K қиылыспайтын бөлікке (\"фолдтарға\") бөлінеді, мысалы, K=5.\n",
        "2.  Цикл K рет іске қосылады. Әрбір `i` итерациясында:\n",
        "    *   `i` блогы **валидациялық** ретінде қолданылады.\n",
        "    *   Қалған K-1 блок **оқыту** үшін қолданылады.\n",
        "    *   `i` валидациялық блогында сапа бағаланады.\n",
        "3.  Алынған K сапа бағалары орташаланады.\n",
        "\n",
        "#### Жалпы баптау және бағалау процесі\n",
        "\n",
        "```mermaid\n",
        "graph LR\n",
        "    A[\"Деректер<br/>X және y\"] --> B[\"Оқыту (+ Валидациялық)<br/>деректер жиыны\"]\n",
        "    A --> C[\"Тест<br/>(кейінге қалдырылған) жиыны\"]\n",
        "    \n",
        "    subgraph Итеративті баптау циклі\n",
        "        B --> D[\"Модельді Train-де<br/>оқыту\"]\n",
        "        D --> F[\"Validation-да<br/>бағалау\"]\n",
        "        F -->|Бағалау нәтижесі| E[\"Гиперпараметрлерді<br/>баптау\"]\n",
        "        E --> D\n",
        "    end\n",
        "    \n",
        "    C -->|Қорытынды тексеру| G[\"Ең жақсы модельді<br/>Test-те бағалау\"]\n",
        "    E -- ең жақсы модель --> G\n",
        "    G --> H[\"Модельді<br/>енгізу\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06b2dc34-c010-424c-b8f2-d33726c3805a",
      "metadata": {},
      "source": [
        "![K-Fold CV](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n",
        "\n",
        "**Артықшылықтары:**\n",
        "*   **Сенімділік:** Бағалау бөлудің кездейсоқтығына аз тәуелді болады.\n",
        "*   **Деректерді тиімді пайдалану:** Әрбір нысан тесттік рөлінде дәл бір рет болады."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99c1872a-5785-4d61-ba27-afec4b7e2a93",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# X_poly_data және y_poly_data деректері алдыңғы ұяшықтардағыдай\n",
        "# ноутбукта бұрын анықталуы керек. Мысалдың толықтығы үшін оларды осы жерде қайта анықтайық.\n",
        "np.random.seed(0)\n",
        "m = 100\n",
        "X_poly_data = 6 * np.random.rand(m, 1) - 3\n",
        "y_poly_data = 0.5 * X_poly_data**2 + X_poly_data + 2 + np.random.randn(m, 1)\n",
        "\n",
        "\n",
        "# Біздің оңтайлы 2-дәрежелі моделімізді алайық\n",
        "model_deg_2 = make_pipeline(PolynomialFeatures(2), LinearRegression())\n",
        "\n",
        "# 10-fold кросс-валидацияны іске қосамыз.\n",
        "# cv=10 деректердің 10 бөлікке бөлінетінін білдіреді.\n",
        "# scoring='neg_mean_squared_error' теріс MSE-ді есептейді.\n",
        "scores = cross_val_score(model_deg_2, X_poly_data, y_poly_data, cv=10, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Теріс MSE-ді оң RMSE-ге айналдырамыз\n",
        "rmse_scores = np.sqrt(-scores)\n",
        "\n",
        "print(\"10 фолдтың әрқайсысындағы қателер (RMSE):\")\n",
        "print(rmse_scores.round(2))\n",
        "print(\"\\nКросс-валидациядағы орташа RMSE: {:.2f}\".format(rmse_scores.mean()))\n",
        "print(\"RMSE стандартты ауытқуы: {:.2f}\".format(rmse_scores.std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fed30556-3594-480b-a314-f20414e905ec-moved",
      "metadata": {},
      "source": [
        "## 4. Реттеу: Артық оқытумен күрес\n",
        "\n",
        "Артық оқытылған модельдің $w$ салмақтары жиі модуль бойынша өте үлкен болады. Ол тым \"жүйкелі\", кіріс деректеріндегі кішкене өзгерістерге сезімтал болады.\n",
        "\n",
        "**Реттеу идеясы:** Шығын функциясына **үлкен салмақтар үшін айыппұл** қосу. Енді модель ымыраға келуге мәжбүр болады: деректерді жақсы сипаттау және сонымен бірге салмақтарды кішкентай ұстау.\n",
        "\n",
        "$$ L_{new}(w) = L_{old}(w) + \\alpha \\cdot P(w) = \\text{MSE} + \\text{айыппұл} $$ \n",
        "\n",
        "*   $P(w)$ — **реттеу мүшесі**.\n",
        "*   $\\alpha \\ge 0$ — **реттеу коэффициенті**, айыппұлдың күшін бақылайтын гиперпараметр."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "453e4b71-a34e-4172-993b-d6285bcce18d",
      "metadata": {},
      "source": [
        "### 4.1. L2-реттеу (Ridge Regression, Гребеньдік регрессия)\n",
        "\n",
        "Айыппұл ретінде салмақтар векторының L2-нормасы (олардың квадраттарының қосындысы) қолданылады.\n",
        "$$ L_{Ridge}(w) = \\sum_{i=1}^{n}(y_i - w^T x_i)^2 + \\alpha \\sum_{j=1}^{m} w_j^2 $$ \n",
        "(бос мүше $w_0$ әдетте реттелмейді)\n",
        "\n",
        "*   **Әсері:** Барлық салмақтарды нөлге қарай \"қысады\", бірақ, әдетте, оларды **толығымен нөлге айналдырмайды**.\n",
        "*   **Қашан пайдалы:** Белгілердің көпшілігі нәтижеге өз үлесін қосқанда жақсы жұмыс істейді."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fa13dd6-e238-49d4-92b2-c077927600df",
      "metadata": {},
      "source": [
        "### 4.2. L1-реттеу (Lasso Regression, Лассо)\n",
        "\n",
        "Айыппұл ретінде салмақтар векторының L1-нормасы (олардың модульдерінің қосындысы) қолданылады.\n",
        "$$ L_{Lasso}(w) = \\sum_{i=1}^{n}(y_i - w^T x_i)^2 + \\alpha \\sum_{j=1}^{m} |w_j| $$ \n",
        "\n",
        "*   **Әсері:** Кейбір маңыздылығы төмен белгілердің салмақтарын **толығымен нөлге айналдыра алады**.\n",
        "*   **Негізгі қасиеті:** **Белгілерді автоматты түрде іріктеуді (feature selection)** жүргізеді, модельді қарапайым және интерпретацияланатын етеді.\n",
        "*   **Қашан пайдалы:** Көптеген белгілер \"қоқыс\" деп болжаған кезде."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vis_regularization_1",
      "metadata": {},
      "source": [
        "Геометриялық интерпретация L1-реттеудің неліктен салмақтарды нөлге айналдыратынын, ал L2 — жоқ екенін түсінуге көмектеседі. L1 үшін салмақтардың рұқсат етілген мәндерінің аймағы — ромб, ал L2 үшін — шеңбер. Шығын функциясының деңгей сызықтары (эллипстер) ромбқа оның шыңдарының бірінде (коэффициенттердің бірі нөлге тең жерде) тию ықтималдығы жоғары.\n",
        "\n",
        "![L1 және L2 реттеуінің геометриялық интерпретациясы](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec7-5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6779327c-09bb-4acb-974b-3eef78cb3bb1",
      "metadata": {},
      "source": [
        "Әсерін визуалды түрде көрейік. 10-дәрежелі полиномдық деректерде (артық оқытуға бейім) 3 модельді оқытайық: қарапайым, Ridge және Lasso.\n",
        "\n",
        "**Маңызды:** Реттеуді қолданбас бұрын **белгілерді масштабтау қажет** (мысалы, `StandardScaler` көмегімен), әйтпесе айыппұл дұрыс қолданылмайды."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cd78315-db83-43a2-8aab-4430d3265771",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Конвейер құрамыз: масштабтау, полиномдық белгілер, модель\n",
        "pipe_lr = make_pipeline(StandardScaler(), PolynomialFeatures(degree=10, include_bias=False), LinearRegression())\n",
        "pipe_ridge = make_pipeline(StandardScaler(), PolynomialFeatures(degree=10, include_bias=False), Ridge(alpha=1))\n",
        "pipe_lasso = make_pipeline(StandardScaler(), PolynomialFeatures(degree=10, include_bias=False), Lasso(alpha=0.1))\n",
        "\n",
        "# Оқытамыз\n",
        "pipe_lr.fit(X_train, y_train)\n",
        "pipe_ridge.fit(X_train, y_train)\n",
        "pipe_lasso.fit(X_train, y_train)\n",
        "\n",
        "# Коэффициенттерді шығарамыз\n",
        "coeffs_df = pd.DataFrame({\n",
        "    'Linear Regression': pipe_lr.named_steps['linearregression'].coef_.flatten(),\n",
        "    'Ridge (alpha=1)': pipe_ridge.named_steps['ridge'].coef_.flatten(),\n",
        "    'Lasso (alpha=0.1)': pipe_lasso.named_steps['lasso'].coef_.flatten()\n",
        "})\n",
        "\n",
        "coeffs_df.plot(kind='bar', figsize=(15, 7))\n",
        "plt.title('Реттеудің модель коэффициенттеріне әсері (дәреже=10)')\n",
        "plt.ylabel('Коэффициент мәні')\n",
        "plt.ylim(-5, 5) # Көрнекілік үшін Y осін шектейік\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c27f47d-8cbd-4a06-bde7-190959470ed6-revised",
      "metadata": {},
      "source": [
        "**Қорытынды:**\n",
        "*   **Linear Regression:** Коэффициенттердің өте үлкен мәндері бар (мұнда біз оларды көрнекілік үшін шектедік), бұл артық оқытудың айқын белгісі.\n",
        "*   **Ridge:** Барлық коэффициенттер едәуір кішірейді. Модель тұрақтырақ болды.\n",
        "*   **Lasso:** Коэффициенттердің көпшілігі нөлге тең болды. Модель ең маңызды белгілерді іріктеді.\n",
        "\n",
        "### 4.3. ElasticNet және `alpha` гиперпараметрін таңдау\n",
        "\n",
        "*   **ElasticNet** — бұл L1 және L2 реттеулерінің комбинациясы. Қатты корреляцияланған белгілер топтары болған кезде жақсы жұмыс істейді.\n",
        "*   **`alpha`-ны таңдау:** Реттеу коэффициенті $\\alpha$-ның оңтайлы мәні **кросс-валидация** көмегімен таңдалады. Біз жай ғана бірнеше мәнді (мысалы, `[0.01, 0.1, 1, 10]`) іріктеп, кросс-валидацияда ең жақсы орташа сапаны беретін мәнді таңдаймыз."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51c12da6-3cbe-426d-b68d-17885af1576a-moved",
      "metadata": {},
      "source": [
        "## 5. Жетілдірілген тақырыптар: Шығын функциялары және Оңтайландыру әдістері\n",
        "\n",
        "Бірінші дәрісте біз MSE-мен стандартты шығын функциясы ретінде таныстық. Енді контекстті түсіне отырып, баламаларды қарастырайық.\n",
        "\n",
        "#### 5.1. Баламалы шығын функциялары\n",
        "\n",
        "Шығын функциясын таңдау деректердің қасиеттеріне және модельдің мақсаттарына байланысты.\n",
        "\n",
        "*   **MSE (L2 Loss):** $(y_i - \\hat{y}_i)^2$\n",
        "    *   **Қасиеті:** Үлкен қателер үшін қатты жазалайды. **Шығарындыларға сезімтал**.\n",
        "\n",
        "*   **MAE (L1 Loss):** $|y_i - \\hat{y}_i|$\n",
        "    *   **Қасиеті:** Қателер үшін сызықты жазалайды. Шығарындыларға **төзімдірек (робасты)**.\n",
        "\n",
        "*   **Хьюбер шығыны (Huber Loss):** L1 және L2 гибриді.\n",
        "    *   **Қасиеті:** Кішкентай қателер үшін MSE сияқты, ал үлкен қателер үшін MAE сияқты әрекет етеді. MSE-нің тұрақтылығын және MAE-нің шығарындыларға төзімділігін біріктіреді.\n",
        "\n",
        "#### 5.2. Оңтайландыру әдістері: Минимумды қалай табуға болады?\n",
        "\n",
        "*   **Аналитикалық шешім (Қалыпты теңдеу):**\n",
        "    $$ w = (X^T X)^{-1} X^T y $$\n",
        "    *   **Артықшылықтары:** Итерациясыз нақты шешім.\n",
        "    *   **Кемшіліктері:** Белгілер саны көп болғанда есептеу жағынан қымбат ($O(m^3)$). Көптеген модельдерге (мысалы, Lasso) қолданылмайды.\n",
        "\n",
        "*   **Сандық шешім (Градиенттік түсу):**\n",
        "    *   **Интуиция:** Шығын функциясының ең тік түсу бағытында (антиградиент) итеративті қадамдар жасау: $w := w - \\alpha \\cdot ∇L(w)$.\n",
        "\n",
        "    ![Градиенттік түсу аналогиясы](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec7-6.png)\n",
        "    ![Градиенттік түсу аналогиясы 1](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec7-8.png)\n",
        "    ![Градиенттік түсу аналогиясы 1](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/lec7-7.png)\n",
        "\n",
        "    *   **Артықшылықтары:** Әмбебап және масштабталатын тәсіл. Қазіргі заманғы ML негізі.\n",
        "    *   **Кемшіліктері:** Жуырланған шешім табады, оқыту жылдамдығын $\\alpha$ таңдауды қажет етеді."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b49cedd5-b6dc-4e04-b110-2cf746033075-revised",
      "metadata": {},
      "source": [
        "## 7-дәріс бойынша қорытынды\n",
        "\n",
        "Бүгін біз модельдердің жалпылау қабілетімен байланысты негізгі концепцияларды қарастырдық:\n",
        "1.  **Полиномдық регрессия** сызықтық емес тәуелділіктерді сипаттауға мүмкіндік береді, бірақ **артық оқыту** қаупін тудырады.\n",
        "2.  Біз **оқыту қисықтары** мен **қалдықтарды талдау** арқылы артық оқыту мен кем оқытуды диагностикалауды үйрендік.\n",
        "3.  Модельді сенімді бағалау және гиперпараметрлерді таңдау үшін біз **кросс-валидацияны** қолданамыз.\n",
        "4.  **Реттеу (Ridge, Lasso)** — модельді шамадан тыс күрделілік (үлкен салмақтар) үшін жазалайтын артық оқытумен күресудің қуатты әдісі.\n",
        "5.  Біз сондай-ақ шығын функциялары мен оңтайландыру әдістеріне оралып, олардың таңдауы модельдің шығарындыларға робастылығы сияқты қасиеттеріне қалай әсер ететінін көрдік."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}