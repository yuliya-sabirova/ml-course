{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 10: Логические методы и Деревья Решений\n",
    "\n",
    "**Цель лекции:**\n",
    "1. Познакомиться с интуицией **логических методов** классификации.\n",
    "2. Глубоко разобрать устройство, принцип обучения и применение **деревьев решений**.\n",
    "3. Показать сильные стороны деревьев (интерпретируемость) и выявить их главную слабость (склонность к переобучению), подготавливая основу для изучения ансамблей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1: Введение в логические методы\n",
    "\n",
    "В основе многих наших рассуждений лежит логика. Мы часто принимаем решения, последовательно отвечая на простые вопросы. Машинное обучение использует этот же принцип в **логических методах классификации**.\n",
    "\n",
    "Представьте простую схему, которая помогает определить профиль человека:\n",
    "\n",
    "```mermaid\n",
    "graph TD;\n",
    "    A{Использует ChatGPT?} -->|Да| B{Использует осознанно?};\n",
    "    A -->|Нет| C{Делает вообще задания?};\n",
    "    B -->|Да| D[<font color=green><b>Сдаст экзамен</b></font>];\n",
    "    B -->|Нет| E[<font color=red><b>Не сдаст экзамен</b></font>];\n",
    "    C -->|Да| F[<font color=green><b>Сдаст экзамен</b></font>];\n",
    "    C -->|Нет| G[<font color=red><b>Не сдаст экзамен</b></font>];\n",
    "\n",
    "    style D fill:#d4edda,stroke:#155724\n",
    "    style F fill:#d4edda,stroke:#155724\n",
    "    style E fill:#f8d7da,stroke:#721c24\n",
    "    style G fill:#f8d7da,stroke:#721c24\n",
    "```\n",
    "\n",
    "Эта схема — это набор простых правил. Математически эти правила часто являются **конъюнкцией** (логическим \"И\") нескольких условий. Например, один из путей к провалу на экзамене можно записать так:\n",
    "\n",
    "**ЕСЛИ** (\"Использует ChatGPT?\" = ДА) **И** (\"Использует осознанно?\" = НЕТ) **ТОГДА** (профиль = \"Не сдаст экзамен\")\n",
    "\n",
    "В виде формулы это выглядит так:\n",
    "$$ R(x) = [f_1(x)=1] \\land [f_2(x)=0] \\rightarrow \\text{Не сдаст экзамен} $$\n",
    "\n",
    "где $f_1(x)$ — признак \"Использует ChatGPT?\", а $f_2(x)$ — \"Использует осознанно?\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### От бинарных правил к вещественным признакам\n",
    "\n",
    "В реальных данных признаки редко бывают бинарными (`да/нет`). Чаще они вещественные (температура, рост, цена). В этом случае логическое правило строится путем сравнения признака с некоторым **пороговым значением**.\n",
    "\n",
    "Например, правило для классификации ирисов может выглядеть так: **ЕСЛИ** (\"Длина лепестка\" <= 2.45 см) **И** (\"Ширина лепестка\" > 1.8 см) **ТОГДА** (вид = \"Virginica\").\n",
    "\n",
    "Математически это записывается с использованием нотации Айверсона:\n",
    "$$ [f_j(x) \\le a_j] $$\n",
    "Это выражение равно `1` (истина), если значение $j$-го признака $f_j(x)$ для объекта $x$ меньше или равно порогу $a_j$, и `0` (ложь) в противном случае. Комбинируя такие простые правила, мы и строим **решающие деревья**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Альтернативные логические подходы\n",
    "\n",
    "Важно понимать, что решающие деревья — не единственный логический метод. Другой популярный подход, подсмотренный во врачебной практике, — это **синдромный подход**.\n",
    "\n",
    "**Интуиция:** Врач ставит диагноз не по одному симптому, а по их совокупности. Если у пациента наблюдается достаточное количество (`d`) симптомов, характерных для определенной болезни, ставится соответствующий диагноз.\n",
    "\n",
    "**Пример:** **ЕСЛИ** ((Температура > 38) + (Есть кашель) + (Болит горло) >= 2) **ТОГДА** (диагноз = \"ОРВИ\").\n",
    "\n",
    "Математически это записывается так: мы подсчитываем, сколько признаков объекта $x$ выходят за рамки \"нормы\" (попадают в определенный интервал), и если это число превышает порог $d$, то правило срабатывает.\n",
    "\n",
    "$$ R(x) = \\left[ \\sum_{j \\in J} [a_j \\le f_j(x) \\le b_j] \\ge d \\right] $$\n",
    "\n",
    "Хотя такие методы существуют, именно **решающие деревья** получили наибольшее распространение благодаря своей интерпретируемости и эффективности, поэтому далее мы сосредоточимся на них."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 2: Анатомия Дерева Решений\n",
    "\n",
    "**Дерево решений** — это интуитивно понятная модель, представляющая собой ациклический граф, похожий на перевернутое дерево.\n",
    "\n",
    "![tree](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/tree.png)\n",
    "\n",
    "**Основные компоненты:**\n",
    "*   **Корень (Root Node):** Самый верхний узел, с которого начинается разделение. В него попадают все объекты обучающей выборки.\n",
    "*   **Внутренний узел (Internal Node):** Узел, который задает \"вопрос\" (проверяет условие) и разделяет данные на две или более ветви.\n",
    "*   **Ветвь (Branch):** Линия, соединяющая узлы. Каждая ветвь соответствует одному из исходов проверки условия (например, \"да\" или \"нет\").\n",
    "*   **Лист (Leaf / Terminal Node):** Конечный узел, в котором содержится итоговое решение (метка класса для классификации или среднее значение для регрессии).\n",
    "\n",
    "**Процесс классификации:** Новый объект \"проходит\" по дереву от корня к листу, на каждом узле выбирая ветвь в зависимости от своих признаков. Класс, указанный в листе, в который он попал, и является предсказанием модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 3: Как дерево учится? Критерии качества разбиения\n",
    "\n",
    "Главный вопрос при построении дерева: **Как в каждом узле найти лучший признак и лучший порог для разделения?**\n",
    "\n",
    "Ответ: нужно выбрать такое правило, которое сделает дочерние узлы как можно более **\"чистыми\"** (однородными по составу классов).\n",
    "\n",
    "#### Понятие \"неопределенности\" (Impurity)\n",
    "\n",
    "\"Неопределенность\" — это мера того, насколько перемешаны классы в одном узле.\n",
    "\n",
    "![impurity](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/impurity_examples.png)\n",
    "\n",
    "Для численного измерения неопределенности используются специальные критерии. Два самых популярных — **критерий Джини** и **энтропия**.\n",
    "\n",
    "**1. Критерий Джини (Gini Impurity):**\n",
    "Показывает вероятность того, что случайно выбранный объект из узла будет неправильно классифицирован, если присвоить ему случайную метку из тех, что есть в этом узле.\n",
    "$$ G = 1 - \\sum_{k=1}^{K} p_k^2 $$\n",
    "где $p_k$ — доля объектов класса $k$ в узле.\n",
    "- $G=0$ для абсолютно \"чистого\" узла.\n",
    "- $G$ стремится к максимуму для узла со смешанными классами.\n",
    "\n",
    "**2. Энтропия (Entropy):**\n",
    "Понятие из теории информации, которое измеряет хаос или неопределенность в системе.\n",
    "$$ S = -\\sum_{k=1}^{K} p_k \\log_2 p_k $$\n",
    "- $S=0$ для абсолютно \"чистого\" узла.\n",
    "- $S$ максимальна, когда все классы представлены поровну.\n",
    "\n",
    "![impurity](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/metrics.png)\n",
    "\n",
    "#### Информационный выигрыш (Information Gain)\n",
    "Дерево принимает решение о лучшем разбиении, максимизируя **информационный выигрыш**:\n",
    "$$ IG = \\text{Impurity}(\\text{родитель}) - \\sum_{j=1}^{m} \\frac{N_j}{N} \\text{Impurity}(\\text{потомок}_j) $$\n",
    "Алгоритм перебирает все возможные признаки и пороги, вычисляет для каждого `IG` и выбирает тот, который дает максимальное значение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример: Пойдем ли мы играть в теннис?\n",
    "\n",
    "Представим, что у нас есть 14 дней наблюдений, и мы хотим предсказать, пойдем ли мы играть в теннис. В корневом узле (до всяких разделений) у нас есть все 14 дней:\n",
    "*   **9 дней**, когда мы пошли играть (`Play=Yes`)\n",
    "*   **5 дней**, когда мы не пошли (`Play=No`)\n",
    "\n",
    "**1. Расчет Критерия Джини (Gini Impurity) для корня:**\n",
    "Показывает вероятность того, что мы ошибемся, если присвоим случайно выбранному дню случайную метку из имеющихся.\n",
    "$$ G = 1 - \\sum_{k=1}^{K} p_k^2 $$\n",
    "Для нашего примера:\n",
    "$$ G_{root} = 1 - ( (9/14)^2 + (5/14)^2 ) \\approx 1 - (0.41 + 0.13) = 0.46 $$\n",
    "Это наша исходная неопределенность.\n",
    "\n",
    "**2. Расчет Энтропии (Entropy) для корня:**\n",
    "Мера хаоса или неопределенности в системе.\n",
    "$$ S = -\\sum_{k=1}^{K} p_k \\log_2 p_k $$\n",
    "$$ S_{root} = - ( (9/14) \\log_2(9/14) + (5/14) \\log_2(5/14) ) \\approx 0.94 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Как алгоритм находит лучший сплит? Информационный выигрыш (Жадный подход)\n",
    "\n",
    "Алгоритм максимизирует **Информационный выигрыш (Information Gain)**. Это показатель того, насколько уменьшилась неопределенность после разбиения. **Алгоритм делает это для КАЖДОГО возможного сплита (по всем признакам и всем порогам) и выбирает тот, у которого `IG` максимален!**. Рассмотрим подробнее. \n",
    "\n",
    "Алгоритм построения дерева на каждом узле действует по **\"жадному\" (greedy)** принципу. Чтобы найти лучшее правило для разделения, он выполняет исчерпывающий, но эффективный поиск:\n",
    "\n",
    "1.  **Цикл по всем признакам:** Алгоритм последовательно перебирает каждый доступный признак.\n",
    "2.  **Поиск лучшего порога для признака:** Для текущего признака он проверяет все возможные пороговые значения (для числовых признаков) или все возможные комбинации разделения категорий (для категориальных). Для каждого такого потенциального сплита он вычисляет **Информационный выигрыш (Information Gain)**.\n",
    "3.  **Выбор \"чемпиона\" признака:** Запоминается тот сплит (правило), который дал максимальный `IG` для данного признака.\n",
    "4.  **Выбор абсолютного победителя:** После того как \"чемпионы\" найдены для **всех** признаков, алгоритм сравнивает их `IG` между собой и выбирает **единственное лучшее правило** — то, у которого `IG` оказался максимальным.\n",
    "\n",
    "Именно это правило и используется для разделения текущего узла. Затем весь процесс рекурсивно повторяется для каждого из получившихся дочерних узлов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример: Пойдем ли мы играть в теннис?**\n",
    "\n",
    "Представим, что мы находимся в **корневом узле** нашего будущего дерева. В нем все 14 дней наблюдений: **9 дней**, когда мы пошли играть (`Play=Yes`), и **5 дней**, когда не пошли (`Play=No`).\n",
    "\n",
    "Неопределенность этого родительского узла по Gini:\n",
    "$$ G_{root} = 1 - ( (9/14)^2 + (5/14)^2 ) \\approx 1 - (0.413 + 0.128) = 0.459 $$\n",
    "\n",
    "Теперь алгоритм должен выбрать, по какому признаку разделить эти 14 дней. Давайте сравним два варианта: \"Ветер\" и \"Влажность\".\n",
    "\n",
    "**Вариант 1: Разделение по признаку \"Ветер\"**\n",
    "\n",
    "```mermaid\n",
    "graph TD;\n",
    "    A[<b>Корень</b> <br> 14 дней <br> 9 Yes, 5 No <br> Gini=0.459] -->|Ветер=Слабый| B[<b>Потомок 1</b> <br> 8 дней <br> 6 Yes, 2 No <br> Gini=0.375];\n",
    "    A -->|Ветер=Сильный| C[<b>Потомок 2</b> <br> 6 дней <br> 3 Yes, 3 No <br> Gini=0.5];\n",
    "```\n",
    "\n",
    "*   $G_{weak} = 1 - ((6/8)^2 + (2/8)^2) = 0.375$\n",
    "*   $G_{strong} = 1 - ((3/6)^2 + (3/6)^2) = 0.5$\n",
    "\n",
    "Средневзвешенная неопределенность потомков:\n",
    "$$ \\text{Weighted\\_Gini}_{wind} = (\\frac{8}{14}) \\times 0.375 + (\\frac{6}{14}) \\times 0.5 \\approx 0.214 + 0.214 = 0.428 $$\n",
    "\n",
    "Информационный выигрыш для \"Ветра\":\n",
    "$$ IG_{wind} = 0.459 - 0.428 = \\mathbf{0.031} $$\n",
    "\n",
    "**Вывод по Варианту 1:** Интуитивно это разбиение кажется не очень удачным. Один из потомков (`Wind=Strong`) такой же \"грязный\", как 50/50, а второй лишь ненамного \"чище\" родителя. Маленькое значение `IG=0.031` это и подтверждает.\n",
    "\n",
    "---\n",
    "\n",
    "**Вариант 2: Разделение по признаку \"Влажность\"**\n",
    "\n",
    "Допустим, признак \"Влажность\" делит те же 14 дней на другие две группы:\n",
    "\n",
    "```mermaid\n",
    "graph TD;\n",
    "    A[<b>Корень</b> <br> 14 дней <br> 9 Yes, 5 No <br> Gini=0.459] -->|Влажность=Высокая| B[<b>Потомок 1</b> <br> 7 дней <br> 3 Yes, 4 No <br> Gini=0.49];\n",
    "    A -->|Влажность=Нормальная| C[<b>Потомок 2</b> <br> 7 дней <br> 6 Yes, 1 No <br> Gini=0.245];\n",
    "```\n",
    "\n",
    "*   $G_{high} = 1 - ((3/7)^2 + (4/7)^2) \\approx 0.49$\n",
    "*   $G_{normal} = 1 - ((6/7)^2 + (1/7)^2) \\approx 0.245$\n",
    "\n",
    "Средневзвешенная неопределенность потомков:\n",
    "$$ \\text{Weighted\\_Gini}_{humidity} = (\\frac{7}{14}) \\times 0.49 + (\\frac{7}{14}) \\times 0.245 \\approx 0.245 + 0.122 = 0.367 $$\n",
    "\n",
    "Информационный выигрыш для \"Влажности\":\n",
    "$$ IG_{humidity} = 0.459 - 0.367 = \\mathbf{0.092} $$\n",
    "\n",
    "---\n",
    "\n",
    "**Принятие решения о сплите:**\n",
    "\n",
    "Теперь алгоритм сравнивает информационный выигрыш от всех возможных сплитов. В нашем случае:\n",
    "\n",
    "$$ IG_{humidity} (0.092) > IG_{wind} (0.031) $$\n",
    "\n",
    "**Вывод:** Поскольку разделение по признаку \"Влажность\" дает большее уменьшение неопределенности, **алгоритм выберет именно \"Влажность\" в качестве первого сплита в дереве.** Этот процесс будет рекурсивно повторяться для каждого нового дочернего узла."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 3.1: Как именно перебираются сплиты?\n",
    "\n",
    "#### Для числовых признаков:\n",
    "Алгоритм не проверяет каждый возможный порог. Он делает это умнее:\n",
    "1.  Берет все уникальные значения признака в текущем узле.\n",
    "2.  Сортирует их по возрастанию.\n",
    "3.  В качестве порогов для проверки берет **средние значения между соседними отсортированными точками**.\n",
    "Например, если для признака \"Температура\" в узле есть значения `[10, 20, 30]`, алгоритм проверит пороги `15` и `25`.\n",
    "\n",
    "#### Для категориальных признаков:\n",
    "Если признак категориальный (например, \"Город\" со значениями `['Almaty', 'Astana', 'Karaganda']`), бинарное дерево должно разделить их на две группы. Алгоритм перебирает **все возможные комбинации** такого разделения:\n",
    "*   `{Almaty}` vs `{Astana, Karaganda}`\n",
    "*   `{Astana}` vs `{Almaty, Karaganda}`\n",
    "*   `{Karaganda}` vs `{Almaty, Astana}`\n",
    "...и для каждой комбинации считает Information Gain, чтобы найти лучшее бинарное разбиение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 4: Главная проблема деревьев — Переобучение (Overfitting)\n",
    "\n",
    "**Переобучение** — главная ахиллесова пята деревьев решений. Если не ограничивать рост дерева, оно будет продолжать делиться до тех пор, пока в каждом листе не останется по одному объекту. Такое дерево идеально запомнит обучающую выборку, включая весь шум, но будет очень плохо работать на новых, невиданных данных.\n",
    "\n",
    "![overfitting](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/overfitting_tree_boundary.png)\n",
    "\n",
    "**Как бороться с переобучением? (Регуляризация)**\n",
    "Мы можем ограничить сложность дерева с помощью гиперпараметров. Этот процесс называется **pre-pruning** (предварительная обрезка).\n",
    "- `max_depth`: максимальная глубина дерева.\n",
    "- `min_samples_split`: минимальное количество объектов в узле для его дальнейшего разделения.\n",
    "- `min_samples_leaf`: минимальное количество объектов, которое должно быть в листе.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 4.1: Альтернативный метод борьбы с переобучением — Усечение (Pruning)\n",
    "\n",
    "Мы уже рассмотрели **pre-pruning** (или *early stopping*) — остановку роста дерева с помощью гиперпараметров (`max_depth` и т.д.). Однако есть и другой, часто более эффективный подход — **post-pruning** (последующее усечение или \"стрижка\").\n",
    "\n",
    "**Идея:**\n",
    "1.  Сначала мы строим **очень глубокое, заведомо переобученное дерево** без жестких ограничений на обучающей выборке.\n",
    "2.  Затем мы начинаем \"подстригать\" его снизу вверх. Мы поочередно рассматриваем каждый узел (начиная с тех, что ближе к листьям) и задаем вопрос: \"А что, если мы **удалим** это разделение (этот узел) и превратим его в лист?\".\n",
    "3.  Мы **сравниваем качество** модели до и после \"стрижки\" на **отложенной (валидационной) выборке**.\n",
    "4.  Если удаление узла (и всего поддерева под ним) **не ухудшает или даже улучшает** качество на валидационной выборке, то мы **выполняем \"стрижку\"**.\n",
    "\n",
    "**Простыми словами:** Мы сначала строим слишком сложную модель, а затем \"отрезаем\" от нее те части, которые, скорее всего, просто выучили шум в обучающих данных и не несут реальной пользы для обобщения.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/ShouldWePrune.png\" width=\"500\" height=\"400\">\n",
    "<img src=\"https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/pruneddecisiontree.png\" width=\"800\" height=\"900\">\n",
    "\n",
    "**Почему Post-Pruning может быть лучше?**\n",
    "Pre-pruning — это \"жадный\" подход. Он может остановить рост дерева слишком рано. Например, какое-то разделение может сейчас казаться бесполезным, но оно откроет путь к очень хорошим разделениям на следующих уровнях. Pre-pruning никогда не даст дереву дойти до этих хороших сплитов.\n",
    "\n",
    "Post-pruning же видит всю картину целиком и принимает более информированное решение о том, какие ветви действительно являются избыточными.\n",
    "\n",
    "В `scikit-learn` основной метод регуляризации — это **pre-pruning** через гиперпараметры. Однако, в `DecisionTreeClassifier` есть параметр `ccp_alpha` (Cost-Complexity Pruning), который реализует один из видов **post-pruning**. Это более продвинутая техника, которая позволяет найти оптимальный баланс между сложностью дерева и его точностью."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 5: Деревья для Регрессии\n",
    "\n",
    "Деревья можно использовать и для задач регрессии.\n",
    "\n",
    "**Принцип работы:**\n",
    "1.  **Критерий разбиения:** Вместо уменьшения неопределенности (Gini/Entropy), дерево ищет такой сплит, который максимально уменьшает **среднеквадратичную ошибку (MSE)**. Для родительского узла $R_m$ и двух его потомков $R_l$ (левый) и $R_r$ (правый), алгоритм максимизирует следующий информационный выигрыш:\n",
    "\n",
    "    $$ Q(R_m, j, t) = H(R_m) - \\frac{|R_l|}{|R_m|}H(R_l) - \\frac{|R_r|}{|R_m|}H(R_r) \\rightarrow \\max $$\n",
    "\n",
    "    Где в качестве меры неопределенности $H(R)$ выступает дисперсия или MSE относительно среднего значения в узле. По сути, дерево ищет сплит, который делает значения в дочерних узлах как можно более \"скученными\".\n",
    "\n",
    "2.  **Предсказание в листе:** В каждом листе $v$ хранится не метка класса, а одно константное значение $b_v$. Это значение является **средним арифметическим** всех целевых переменных $y_i$ объектов, попавших в этот лист:\n",
    "\n",
    "    $$ b_v = \\frac{1}{|R_v|} \\sum_{i \\in R_v} y_i $$\n",
    "\n",
    "    Именно это и приводит к тому, что предсказание дерева регрессии выглядит как **ступенчатая функция**. Каждая \"ступенька\" — это среднее значение в одном из листьев.\n",
    "\n",
    "\n",
    "В результате предсказание дерева регрессии выглядит как **ступенчатая функция**.\n",
    "\n",
    "**Пример:** Аппроксимация функции `cos(x)`.\n",
    "Представим, что у нас есть точки, лежащие на кривой косинуса. Дерево регрессии будет пытаться приблизить эту гладкую кривую с помощью горизонтальных отрезков.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/cos1d.png\" alt=\"1\" width=\"400\"></td>\n",
    "    <td><img src=\"https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/cos2d.png\" alt=\"2\" width=\"400\"></td>\n",
    "    <td><img src=\"https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/cos3.png\" alt=\"3\" width=\"400\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "С увеличением глубины дерева \"ступеньки\" становятся все меньше, и модель начинает подгоняться не только под саму функцию, но и под случайный шум в данных, что также приводит к переобучению."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 6: Практическая реализация в Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. Пример Классификации (Датасет Iris)\n",
    "\n",
    "Давайте построим простое дерево для классической задачи определения вида ириса по его параметрам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Решающее дерево для задачи классификации\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "def get_grid(data):\n",
    "    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "    return np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "train_data = np.c_[iris.data[:, 0].reshape(-1, 1), iris.data[:, 2].reshape(-1, 1)]\n",
    "train_labels = iris.target\n",
    "\n",
    "clf_tree = DecisionTreeClassifier(criterion='entropy', max_depth=2, random_state=5)\n",
    "clf_tree.fit(train_data, train_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "xx, yy = get_grid(train_data)\n",
    "predicted = clf_tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, predicted, cmap='spring', shading='auto')\n",
    "plt.scatter(train_data[:, 0], train_data[:, 1], c=train_labels, s=50, cmap='spring', edgecolors='black', linewidth=1.5)\n",
    "plt.show()\n",
    "plt.figure(figsize=(14,14))\n",
    "plot_tree(clf_tree,feature_names=iris.feature_names,  \n",
    "          class_names=iris.target_names,\n",
    "          filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Пример Регрессии (Аппроксимация косинуса)\n",
    "\n",
    "Теперь реализуем пример, который мы обсуждали в теории: аппроксимация функции `cos(x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Генерируем данные\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 3 * (0.5 - rng.rand(16)) # Добавляем шум\n",
    "\n",
    "# Обучаем дерево регрессии\n",
    "regr = DecisionTreeRegressor(max_depth=2)\n",
    "regr.fit(X, y)\n",
    "\n",
    "# Делаем предсказания\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# Визуализируем результат\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"Данные\")\n",
    "plt.plot(X_test, y_pred, color=\"cornflowerblue\", label=\"Предсказание (max_depth=3)\", linewidth=2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Аппроксимация функции деревом регрессии\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Визуализируем дерево\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(regr, \n",
    "         filled=True)\n",
    "plt.title(\"Дерево решений для задачи регрессии)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 7: Преимущества и недостатки Деревьев Решений\n",
    "\n",
    "#### Преимущества:\n",
    "*   **Высокая интерпретируемость:** Логику дерева легко понять и объяснить даже неспециалисту. Это \"белый ящик\".\n",
    "*   **Не требуют масштабирования данных:** Работают с признаками в их исходном масштабе.\n",
    "*   Могут работать как с числовыми, так и с категориальными признаками.\n",
    "\n",
    "#### Недостатки:\n",
    "*   **Склонность к переобучению:** Требуют тщательной настройки гиперпараметров (регуляризации).\n",
    "*   **Нестабильность:** Небольшие изменения в обучающих данных могут привести к построению совершенно другого дерева."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
