{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 12: K-Means, Иерархическая кластеризация и DBSCAN на практике\n",
    "\n",
    "**Цель семинара:** На простых синтетических данных шаг за шагом разобрать, как работают, где ошибаются и как настраиваются три ключевых алгоритма кластеризации. Мы также научимся интерпретировать полученные кластеры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Решение проблемы с утечкой памяти KMeans на Windows\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1: K-Means — кластеризация на основе центроидов\n",
    "\n",
    "**Датасет:** `make_blobs` — сгенерированные данные, которые идеально подходят для K-Means (три отдельных \"облака\" точек)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_blobs, y_blobs = make_blobs(n_samples=500, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=X_blobs[:,0], y=X_blobs[:,1])\n",
    "plt.title(\"Исходные данные (Blobs)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Комментарий:** *Перед применением K-Means (и любого другого алгоритма, основанного на расстоянии) **крайне важно масштабировать данные**. Иначе признаки с большим диапазоном значений будут непропорционально сильно влиять на результат.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_blobs_scaled = scaler.fit_transform(X_blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Поиск оптимального K с помощью метода локтя\n",
    "\n",
    "**Комментарий:** *Мы не знаем, сколько кластеров в данных. Прогоним K-Means в цикле с разным `k` и будем записывать `inertia_` (WCSS). Затем построим график и найдем \"локоть\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssd = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    model = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "    model.fit(X_blobs_scaled)\n",
    "    ssd.append(model.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(k_range, ssd, 'o-')\n",
    "plt.xlabel(\"Количество кластеров (K)\")\n",
    "plt.ylabel(\"Sum of Squared Distances (WCSS)\")\n",
    "plt.title(\"Метод локтя для K-Means\")\n",
    "plt.xticks(k_range)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** *На графике отчетливо виден \"локоть\" в точке K=3. Это говорит о том, что оптимальное количество кластеров — три.*\n",
    "\n",
    "### 1.2. Финальная модель K-Means и интерпретация\n",
    "\n",
    "**Комментарий:** *Обучим модель с `k=3` и визуализируем результат. Затем посмотрим, как можно интерпретировать полученные кластеры.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init='auto')\n",
    "labels_kmeans = kmeans.fit_predict(X_blobs_scaled)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=X_blobs[:,0], y=X_blobs[:,1], hue=labels_kmeans, palette='viridis')\n",
    "plt.title(\"Результат K-Means (K=3)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Интерпретация кластеров\n",
    "**Комментарий:** *Алгоритм вернул нам метки 0, 1, 2. Сами по себе эти цифры ничего не значат. Наша задача как аналитиков — придать им смысл. Основной способ — сгруппировать исходные данные по меткам кластеров и посмотреть на средние значения признаков.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blobs = pd.DataFrame(X_blobs, columns=['Feature_1', 'Feature_2'])\n",
    "df_blobs['Cluster'] = labels_kmeans\n",
    "\n",
    "# Группируем и считаем средние\n",
    "cluster_profile = df_blobs.groupby('Cluster').mean()\n",
    "print(cluster_profile)\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(cluster_profile, annot=True, cmap='viridis')\n",
    "plt.title('Профиль кластеров K-Means')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** *Мы видим, что:\n",
    "- **Кластер 0** характеризуется высокими значениями обоих признаков.\n",
    "- **Кластер 1** имеет низкое значение Feature_1 и высокое Feature_2.\n",
    "- **Кластер 2** имеет низкое значение Feature_2. \n",
    "Так мы создаем \"профиль\" каждого кластера, что позволяет дать им осмысленные названия.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2: Иерархическая кластеризация\n",
    "\n",
    "**Комментарий:** *Используем небольшой, но понятный датасет, чтобы наглядно построить дендрограмму.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем наглядный датасет (как в лекции)\n",
    "X_hier = np.array([\n",
    "    [8, 150], [9, 160], [8.5, 140], [7.5, 155], # Офисные работники\n",
    "    [3, 40], [4, 50], [2.5, 35], [3.5, 45],    # Студенты\n",
    "    [5, 200], [6, 210], [4.5, 190], [5.5, 220] # Фрилансеры\n",
    "])\n",
    "\n",
    "# Строим дендрограмму\n",
    "linked = linkage(X_hier, method='ward')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram(linked, orientation='top', labels=[f\"Точка {i}\" for i in range(len(X_hier))], distance_sort='descending')\n",
    "plt.title('Дендрограмма иерархической кластеризации')\n",
    "plt.ylabel('Расстояние по Уорду')\n",
    "plt.axhline(y=100, c='k', linestyle='--') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** *Дендрограмма наглядно показывает иерархию объединения точек. Горизонтальный срез на уровне `y=100` пересекает 3 вертикальные линии, что подтверждает нашу гипотезу о наличии 3 кластеров.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3: DBSCAN — кластеризация на основе плотности\n",
    "\n",
    "**Комментарий:** *Теперь перейдем к данным, где K-Means бессилен.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moons, y_moons = make_moons(n_samples=500, noise=0.1, random_state=42)\n",
    "X_moons_scaled = StandardScaler().fit_transform(X_moons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Сравнение K-Means и DBSCAN\n",
    "\n",
    "**Комментарий:** *Применим оба алгоритма к данным `moons` и посмотрим на результат.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init='auto')\n",
    "dbscan = DBSCAN(eps=0.3)\n",
    "\n",
    "labels_kmeans = kmeans.fit_predict(X_moons_scaled)\n",
    "labels_dbscan = dbscan.fit_predict(X_moons_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "sns.scatterplot(ax=axes[0], x=X_moons[:,0], y=X_moons[:,1], hue=labels_kmeans, palette='viridis')\n",
    "axes[0].set_title('Неудачный результат K-Means')\n",
    "\n",
    "sns.scatterplot(ax=axes[1], x=X_moons[:,0], y=X_moons[:,1], hue=labels_dbscan, palette='viridis')\n",
    "axes[1].set_title('Успешный результат DBSCAN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Подбор гиперпараметров для DBSCAN\n",
    "\n",
    "**Комментарий:** *Как мы выбрали `eps=0.3`? Это не магия. Существует полезная эвристика для подбора `eps`. Идея в том, чтобы найти \"точку перегиба\" на графике расстояний до k-го ближайшего соседа.*\n",
    "\n",
    "1.  Выбираем `min_samples`. Хорошее эмпирическое правило: `min_samples = 2 * количество_признаков`. У нас 2 признака, так что возьмем `min_samples=4`.\n",
    "2.  Для **каждой** точки в датасете находим расстояние до ее 4-го ближайшего соседа.\n",
    "3.  Сортируем эти расстояния по возрастанию и строим график.\n",
    "4.  Ищем на графике \"локоть\" — точку, где кривая резко устремляется вверх. Оптимальное значение `eps` будет лежать примерно на этом уровне."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 2 * X_moons_scaled.shape[1]\n",
    "\n",
    "neighbors = NearestNeighbors(n_neighbors=min_samples)\n",
    "neighbors_fit = neighbors.fit(X_moons_scaled)\n",
    "distances, indices = neighbors_fit.kneighbors(X_moons_scaled)\n",
    "\n",
    "# Сортируем расстояния до k-го соседа (k = min_samples)\n",
    "k_distances = np.sort(distances[:, min_samples-1], axis=0)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(k_distances)\n",
    "plt.title('График k-расстояний для подбора Epsilon')\n",
    "plt.xlabel(\"Точки, отсортированные по расстоянию\")\n",
    "plt.ylabel(f\"Расстояние до {min_samples}-го соседа\")\n",
    "plt.axhline(y=0.3, c='r', linestyle='--') # Наш предполагаемый локоть\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод семинара:** *Мы увидели, что нет \"лучшего\" алгоритма. K-Means быстр и хорош для сферических кластеров. Иерархическая кластеризация дает наглядную дендрограмму. DBSCAN, основанный на плотности, отлично справляется с кластерами сложной формы и выбросами, но требует более тонкой настройки гиперпараметров.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
