{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 9: Метрические методы (KNN) и Метод опорных векторов (SVM)\n",
    "\n",
    "**Цель лекции:**\n",
    "1.  Познакомиться с **метрическими методами** классификации на примере **K-ближайших соседей (KNN)**.\n",
    "2.  Изучить один из самых мощных классических алгоритмов — **Метод опорных векторов (SVM)**.\n",
    "3.  Разобрать математические основы, лежащие в основе этих алгоритмов.\n",
    "4.  Научиться применять, настраивать и сравнивать эти модели на задачах бинарной и многоклассовой классификации с использованием Python и библиотеки Scikit-Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1: Метод K-ближайших соседей (K-Nearest Neighbors, KNN)\n",
    "\n",
    "KNN относится к **метрическим алгоритмам**, в основе которых лежит **гипотеза компактности**. Она гласит, что объекты, принадлежащие к одному классу, в пространстве признаков расположены близко друг к другу, образуя \"компактные\" кластеры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Интуиция и математические основы\n",
    "\n",
    "Интуиция KNN очень проста: **класс нового объекта определяется тем, какой класс преобладает среди его ближайших соседей.**\n",
    "\n",
    "![k=3](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/knn_illustration_k3.png)\n",
    "![k=5](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/knn_illustration_k5.png)\n",
    "\n",
    "Чтобы найти \"ближайших\" соседей, нам нужно уметь измерять расстояние. Для этого используются [**метрики**](http://lightcone.ru/manhattan/?ysclid=mhu3mmwtkq673949565)\n",
    "\n",
    "#### Метрика Минковского\n",
    "Это обобщенная метрика расстояния между двумя векторами $x$ и $x_i$ в n-мерном пространстве. Она имеет следующий вид:\n",
    "\n",
    "$$ p(x, x_i) = \\left( \\sum_{j=1}^{n} \\omega_j |x^j - x_i^j|^p \\right)^{1/p} $$\n",
    "\n",
    "где:\n",
    "- $n$ — количество признаков (размерность пространства).\n",
    "- $p > 0$ — параметр метрики, определяющий ее тип.\n",
    "- $\\omega_j$ — веса признаков. Они важны, когда признаки имеют разный масштаб или значимость. На практике, чтобы уравнять влияние признаков, данные почти всегда **масштабируют**.\n",
    "\n",
    "![Разница между метриками расстояния](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/distance_metrics.png)\n",
    "\n",
    "Два наиболее популярных частных случая метрики Минковского:\n",
    "\n",
    "1.  **Евклидово расстояние (при p=2):** Привычное нам \"прямое\" расстояние.\n",
    "$$ p(x, x_i) = \\sqrt{ \\sum_{j=1}^{n} (x^j - x_i^j)^2 } $$\n",
    "\n",
    "2.  **Манхэттенское расстояние (при p=1):** \"Расстояние городских кварталов\", сумма модулей разностей координат.\n",
    "$$ p(x, x_i) = \\sum_{j=1}^{n} |x^j - x_i^j| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Алгоритм KNN\n",
    "\n",
    "Для классификации нового объекта $x$ алгоритм выполняет следующие шаги:\n",
    "1.  **Выбрать K** — количество соседей, на которое мы будем ориентироваться.\n",
    "2.  **Вычислить расстояния** от $x$ до каждого объекта $x_i$ из обучающей выборки с помощью выбранной метрики.\n",
    "3.  **Найти K соседей** — отобрать K объектов из обучающей выборки с минимальным расстоянием до $x$.\n",
    "4.  **Определить класс** — класс объекта $x$ определяется классом, который является преобладающим среди найденных K соседей (мажоритарное голосование).\n",
    "\n",
    "Математически, если $y^{(i)}$ — класс $i$-го соседа из $k$ ближайших, то класс нового объекта $a(x)$ находится так:\n",
    "\n",
    "$$ a(x, X^l) = \\arg\\max_{y \\in Y} \\sum_{i=1}^{k} [y^{(i)} = y] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Преимущества и недостатки KNN\n",
    "\n",
    "#### Преимущества:\n",
    "*   **Простота и интуитивность:** Алгоритм легко понять и реализовать.\n",
    "*   **Гибкость:** Легко адаптируется для задач регрессии (путем усреднения значений соседей, для этого в `scikit-learn` используется модуль **`KNeighborsRegressor`**).\n",
    "*   **Нет этапа обучения:** KNN является \"ленивым\" алгоритмом. Он не строит модель, а просто запоминает всю обучающую выборку. Обучение происходит мгновенно.\n",
    "\n",
    "#### Недостатки:\n",
    "*   **Вычислительная сложность на этапе предсказания:** Для классификации одного нового объекта нужно рассчитать расстояние до всех объектов обучающей выборки, что может быть очень медленно на больших данных.\n",
    "*   **Чувствительность к масштабу признаков:** Требует обязательного масштабирования данных.\n",
    "*   **Чувствительность к выбору K:** Результат сильно зависит от этого гиперпараметра.\n",
    "*   **Требовательность к памяти:** Нужно хранить всю обучающую выборку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2: Метод опорных векторов (Support Vector Machines, SVM)\n",
    "\n",
    "SVM — один из самых мощных и популярных алгоритмов классификации. Его основная идея — найти не просто разделяющую границу, а **оптимальную разделяющую гиперплоскость**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Интуиция и математические основы\n",
    "\n",
    "#### Гиперплоскость\n",
    "Гиперплоскость — это обобщение прямой (для 2D) и плоскости (для 3D) на пространство любой размерности. Это подпространство, размерность которого на единицу меньше исходного.\n",
    "\n",
    "![Примеры гиперплоскостей в разных размерностях](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/hyperplanes.png)\n",
    "\n",
    "Уравнение гиперплоскости задается в виде:\n",
    "\n",
    "$$ w^T x - b = 0 \\quad \\text{или} \\quad \\langle w, x \\rangle - b = 0 $$\n",
    "\n",
    "где:\n",
    "- $w$ — вектор весов (нормаль к гиперплоскости), который задает ее ориентацию.\n",
    "- $x$ — вектор признаков объекта.\n",
    "- $b$ — смещение (bias), которое определяет положение гиперплоскости.\n",
    "\n",
    "#### Максимизация зазора (Margin)\n",
    "SVM ищет такую гиперплоскость, которая находится на максимальном удалении от ближайших объектов каждого класса. Этот зазор называется **margin**. Объекты, лежащие на границах этого зазора, называются **опорными векторами** (support vectors), так как именно они \"поддерживают\" гиперплоскость и определяют ее положение.\n",
    "\n",
    "![Оптимальная разделяющая гиперплоскость](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/оптимальная_разделяющая_гиперплоскость.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача SVM для **линейно разделимого** случая сводится к задаче квадратичного программирования: мы хотим **минимизировать** норму вектора весов (что эквивалентно максимизации зазора), при условии, что все точки классифицированы правильно.\n",
    "\n",
    "$$ \\frac{1}{2} ||w||^2 \\rightarrow \\min_{w,b} $$\n",
    "При ограничении:\n",
    "$$ y_i(\\langle w, x_i \\rangle - b) \\ge 1, \\quad i=1, \\dots, l $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Давайте разберем эту формулу:**\n",
    "1.  **Выражение $\\langle w, x_i \\rangle - b$** — это \"счет\" для точки $x_i$. Его знак показывает, с какой стороны от центральной линии находится точка.\n",
    "2.  **Умножение на $y_i$** (метка класса, `+1` или `-1`) — это трюк. Если точка классифицирована правильно, результат этого умножения всегда будет положительным.\n",
    "3.  **Требование $\\ge 1$** — это самое главное. Мы требуем, чтобы каждая точка находилась не просто на правильной стороне от центра (`> 0`), а на своей границе зазора (`= 1`) или еще дальше (`> 1`). Это и создает \"пустую зону\" между классами, ширину которой мы и стремимся максимизировать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Что такое \"Опорные Векторы\"?\n",
    "\n",
    "Название \"Метод Опорных Векторов\" говорит само за себя. **Опорные векторы** — это ключевые точки из обучающей выборки, которые определяют положение и ориентацию оптимальной разделяющей гиперплоскости.\n",
    "\n",
    "**Концепция:** Представьте, что вы разделяете два вида монет на столе линейкой. Чтобы зазор был максимальным, вы будете двигать линейку, пока она не упрется в несколько монет с обеих сторон. Эти монеты, которые \"подпирают\" линейку, и есть **опорные векторы**. Все остальные монеты, которые находятся далеко, на положение линейки не влияют.\n",
    "\n",
    "**Определение:**\n",
    "1.  **В идеальном случае (Hard Margin):** Опорными векторами являются только те точки, которые лежат **ровно на границах зазора** (где $ y_i(\\langle w, x_i \\rangle - b) = 1 $).\n",
    "2.  **В реальном случае (Soft Margin):** Опорными векторами являются **все точки-нарушители**: те, что лежат на границе зазора, внутри него или были классифицированы неверно.\n",
    "\n",
    "**Почему это важно?**\n",
    "- **Эффективность:** После обучения модель SVM хранит в памяти **только опорные векторы**, а не всю обучающую выборку. Для предсказания нового объекта она сравнивает его **только с этими опорными векторами**, что делает SVM очень быстрым на этапе предсказания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Мягкий зазор (Soft Margin) и неразделимые данные\n",
    "В реальности данные часто нельзя разделить идеально. Для таких случаев вводится концепция \"мягкого зазора\". Мы разрешаем некоторым точкам нарушать границы зазора или даже быть неверно классифицированными. \n",
    "\n",
    "![Пример мягкого зазора (Soft Margin)](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/пример_мягкого_зазора_(soft_margin).png)\n",
    "\n",
    "Для этого вводятся **слабые переменные** (slack variables) $\\xi_i \\ge 0$, которые показывают, насколько $i$-й объект нарушает границу.\n",
    "\n",
    "Оптимизационная задача усложняется: теперь мы минимизируем не только норму вектора весов, но и суммарную ошибку (сумму $\\xi_i$):\n",
    "\n",
    "$$ \\frac{1}{2} ||w||^2 + C \\sum_{i=1}^{l} \\xi_i \\rightarrow \\min_{w,b,\\xi} $$\n",
    "Ограничение ослабляется:\n",
    "$$ y_i(\\langle w, x_i \\rangle - b) \\ge 1 - \\xi_i, \\quad \\xi_i \\ge 0, \\quad i=1, \\dots, l $$\n",
    "\n",
    "*   Если $\\xi_i = 0$, точка находится за пределами зазора (все в порядке).\n",
    "*   Если $0 < \\xi_i \\le 1$, точка попала внутрь зазора, но классифицирована верно.\n",
    "*   Если $\\xi_i > 1$, точка была классифицирована неверно.\n",
    "\n",
    "Модель пытается одновременно **максимизировать зазор** (минимизировать $||w||^2$) и **минимизировать суммарную ошибку** (минимизировать $\\sum \\xi_i$). Баланс между этими двумя целями контролируется **гиперпараметром `C`**.\n",
    "\n",
    "**Гиперпараметр `C`** — это параметр регуляризации. Он контролирует баланс между максимизацией зазора и минимизацией количества ошибок:\n",
    "- **Маленький `C`**: Мы предпочитаем широкий зазор, даже если это приведет к большему числу ошибок на обучающей выборке. Модель более простая, меньше склонна к переобучению (высокое смещение, низкая дисперсия).\n",
    "- **Большой `C`**: Мы сильно штрафуем за ошибки, поэтому модель пытается классифицировать каждую точку правильно, даже ценой сужения зазора. Модель более сложная и склонна к переобучению (низкое смещение, высокая дисперсия)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Трюк с ядром (Kernel Trick)\n",
    "\n",
    "Что делать, если данные неразделимы даже с мягким зазором (например, один класс находится внутри другого)? Идея состоит в том, чтобы перевести данные в пространство признаков более высокой размерности, где они, возможно, станут линейно разделимыми.\n",
    "\n",
    "![Визуализация трюка с ядром](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/kernel_trick_visualization.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Как SVM делает предсказания (Двойственная форма)\n",
    "\n",
    "Оказывается, формулу для предсказания SVM можно записать в виде, который зависит не от вектора $w$, а напрямую от опорных векторов:\n",
    "\n",
    "$$ \\text{предсказание}(x) = \\text{sign} \\left( \\sum_{i \\in SV} \\alpha_i y_i \\langle x_i, x \\rangle - b \\right) $$\n",
    "\n",
    "где:\n",
    "- $x$ — новый объект, который мы хотим классифицировать.\n",
    "- $x_i$ — $i$-й **опорный вектор**, который модель запомнила во время обучения.\n",
    "- $\\alpha_i$ и $y_i$ — вес и класс этого опорного вектора.\n",
    "- $\\langle x_i, x \\rangle$ — скалярное произведение, которое измеряет \"схожесть\" нового объекта с $i$-м опорным вектором.\n",
    "\n",
    "**Простыми словами:** чтобы классифицировать новый объект, SVM измеряет его схожесть с каждым из запомненных опорных векторов, суммирует эти схожести с весами и на основе итогового знака выносит вердикт."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Что такое ядро?\n",
    "\n",
    "**Ядро — это вычислительный трюк.** Это функция $K(x_i, x)$, которая позволяет нам получить результат скалярного произведения в новом, очень сложном пространстве, **не выполняя само преобразование в это пространство.**\n",
    "\n",
    "Мы просто заменяем $\\langle x_i, x \\rangle$ на $K(x_i, x)$ в формуле предсказания:\n",
    "\n",
    "$$ \\text{предсказание}(x) = \\text{sign} \\left( \\sum_{i \\in SV} \\alpha_i y_i K(x_i, x) - b \\right) $$\n",
    "\n",
    "**Популярные ядра:**\n",
    "1.  **Линейное:** $K(x_i, x) = \\langle x_i, x \\rangle$.\n",
    "2.  **Полиномиальное:** $K(x_i, x_j) = (\\gamma \\langle x_i, x_j \\rangle + r)^d$. Гиперпараметры: степень $d$, коэффициент $\\gamma$, свободный член $r$.\n",
    "3.  **Радиальная базисная функция (RBF):** $K(x_i, x) = \\exp(-\\gamma ||x_i - x||^2)$. Вычисляет \"схожесть\" на основе Гауссовой функции. Контролируется параметром `gamma`.  Маленькая `gamma` означает большое влияние (гладкая граница), большая `gamma` — локальное влияние (очень сложная, извилистая граница)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Демонстрация 'Трюка с ядром' на практике\n",
    "\n",
    "Давайте на простом, наглядном примере посмотрим, почему ядра так важны. Мы создадим синтетический набор данных, который невозможно разделить прямой линией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Создаем данные: один класс (0) в виде круга внутри другого класса (1)\n",
    "X, y = make_circles(n_samples=100, noise=0.1, factor=0.5, random_state=42)\n",
    "\n",
    "# Визуализируем\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='winter')\n",
    "plt.title('Линейно неразделимые данные (круги)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, разделить эти данные одной прямой линией невозможно. Попробуем обучить SVM с двумя разными ядрами.\n",
    "\n",
    "**Попытка №1: Линейное ядро**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Вспомогательная функция для отрисовки разделяющей поверхности\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                           np.linspace(y_min, y_max, 100))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='winter')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='winter', edgecolors='k')\n",
    "    plt.title(f'Ядро: {model.kernel}')\n",
    "\n",
    "# Обучаем модель с линейным ядром\n",
    "linear_svm = SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "# Визуализируем результат\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_decision_boundary(linear_svm, X, y)\n",
    "plt.show()\n",
    "print(f\"Точность линейного SVM: {linear_svm.score(X,y):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейное ядро потерпело неудачу. Модель провела прямую линию, которая плохо разделяет классы, и точность очень низкая.\n",
    "\n",
    "**Попытка №2: Нелинейное ядро RBF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем модель с ядром RBF (используется по умолчанию)\n",
    "rbf_svm = SVC(kernel='rbf', C=1, gamma='auto').fit(X, y)\n",
    "\n",
    "# Визуализируем результат\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_decision_boundary(rbf_svm, X, y)\n",
    "plt.show()\n",
    "print(f\"Точность RBF SVM: {rbf_svm.score(X,y):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** Ядро RBF успешно справилось с задачей! Оно построило нелинейную, круговую границу, которая идеально разделила классы, и точность стала 100%. Этот пример наглядно демонстрирует, как \"трюк с ядром\" позволяет SVM решать сложные, нелинейные задачи, просто изменив один гиперпараметр."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Преимущества и недостатки SVM\n",
    "\n",
    "#### Преимущества:\n",
    "*   **Эффективность в пространствах высокой размерности:** Отлично работает, когда признаков много.\n",
    "*   **Эффективность по памяти:** Использует только часть обучающих точек для построения модели (опорные векторы).\n",
    "*   **Гибкость:** Благодаря ядрам может строить очень сложные нелинейные разделяющие поверхности.\n",
    "*   **Имеет прочное математическое обоснование** в виде задачи выпуклой оптимизации, что гарантирует нахождение глобального минимума.\n",
    "\n",
    "#### Недостатки:\n",
    "*   **Чувствительность к выбору ядра и его гиперпараметров (`C`, `gamma`):** Требует тщательного подбора с помощью `GridSearchCV`.\n",
    "*   **Вычислительная сложность:** Обучение может быть долгим на очень больших наборах данных.\n",
    "*   **Сложность интерпретации:** Модель с нелинейным ядром является \"черным ящиком\", ее решения сложно интерпретировать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Применение SVM для задач регрессии (SVR)\n",
    "\n",
    "Метод опорных векторов можно элегантно адаптировать и для решения задач регрессии. Этот подход называется **Support Vector Regression (SVR)**.\n",
    "\n",
    "#### Интуиция: от \"максимального зазора\" к \"максимально широкой улице\"\n",
    "\n",
    "Вспомним, что в классификации SVM искал гиперплоскость, которая бы имела **максимальный зазор** между классами. В регрессии классов нет, у нас есть непрерывная целевая переменная.\n",
    "\n",
    "Основная идея SVR — сделать обратное. Вместо того чтобы раздвигать точки, мы хотим найти такую функцию (гиперплоскость), вокруг которой можно построить \"улицу\" или \"коридор\" с как можно большим количеством точек **внутри** этого коридора.\n",
    "\n",
    "*   **Цель:** Провести линию регрессии так, чтобы она была как можно ближе к большинству точек.\n",
    "*   **\"Улица\" (Коридор):** Мы определяем коридор шириной `2ε` (два эпсилон) вокруг нашей линии регрессии.\n",
    "*   **Правило:** Мы **не штрафуем** модель за ошибки, если точки данных попадают **внутрь** этого коридора. Штраф начисляется только за те точки, которые оказались **снаружи**.\n",
    "\n",
    "![Принцип работы SVR](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/svr_illustration.png)\n",
    "\n",
    "#### Математическая постановка\n",
    "\n",
    "Задача SVR, как и у SVM, сводится к задаче выпуклой оптимизации. Мы снова хотим минимизировать норму вектора весов $||w||^2$ (что делает модель более \"гладкой\"), но при других ограничениях.\n",
    "\n",
    "Мы хотим найти такую функцию $f(x) = \\langle w, x \\rangle + b$, которая имеет наименьшее отклонение от реальных значений $y_i$, \"прощая\" отклонения меньше, чем $\\epsilon$.\n",
    "\n",
    "Это приводит к следующей задаче минимизации (\"примальная\" форма):\n",
    "\n",
    "$$ \\frac{1}{2} ||w||^2 + C \\sum_{i=1}^{l} (\\xi_i + \\xi_i^*) \\rightarrow \\min_{w,b,\\xi,\\xi^*} $$\n",
    "\n",
    "При ограничениях:\n",
    "$$ y_i - (\\langle w, x_i \\rangle + b) \\le \\epsilon + \\xi_i \\quad (\\text{для точек выше коридора}) $$\n",
    "$$ (\\langle w, x_i \\rangle + b) - y_i \\le \\epsilon + \\xi_i^* \\quad (\\text{для точек ниже коридора}) $$\n",
    "$$ \\xi_i, \\xi_i^* \\ge 0 $$\n",
    "\n",
    "*   $\\epsilon$ (эпсилон) — ширина \"зоны нечувствительности\" в одну сторону от линии.\n",
    "*   $\\xi_i$ и $\\xi_i^*$ — слабые переменные, измеряющие, **насколько** точка $x_i$ \"вылетела\" за пределы коридора вверх или вниз.\n",
    "*   `C` — параметр регуляризации, контролирующий баланс между \"гладкостью\" модели и количеством ошибок.\n",
    "\n",
    "#### Двойственная форма и роль опорных векторов в SVR\n",
    "\n",
    "Как и в классификации, для практических вычислений (и использования ядер) используется \"двойственная\" форма. Итоговая формула для предсказания выглядит так:\n",
    "\n",
    "$$ \\text{предсказание}(x) = \\left( \\sum_{i \\in SV} (\\alpha_i - \\alpha_i^*) K(x_i, x) \\right) + b $$\n",
    "\n",
    "**Давайте расшифруем эту ключевую формулу:**\n",
    "*   **$K(x_i, x)$** — это ядро, измеряющее \"схожесть\" нового объекта $x$ с опорным вектором $x_i$.\n",
    "*   **$SV$** — это множество **опорных векторов**, то есть только тех точек, которые лежат на границе $\\epsilon$-коридора или за его пределами.\n",
    "*   **$\\alpha_i$ и $\\alpha_i^*$** — это **множители Лагранжа**, которые можно интуитивно представить как \"силы\", с которыми опорные векторы \"тянут\" на себя линию регрессии.\n",
    "    *   **$\\alpha_i > 0$** (и $\\alpha_i^*=0$) для точек, оказавшихся **выше** коридора. Они тянут линию **вверх**.\n",
    "    *   **$\\alpha_i^* > 0$** (и $\\alpha_i=0$) для точек, оказавшихся **ниже** коридора. Они тянут линию **вниз**.\n",
    "    *   Для всех точек **внутри** коридора и $\\alpha_i=0$, и $\\alpha_i^*=0$. Они **не влияют** на итоговую модель.\n",
    "*   Выражение **$(\\alpha_i - \\alpha_i^*)$** — это итоговый **\"вес\"** $i$-го опорного вектора в модели. Он положителен, если точка тянет вверх, и отрицателен, если точка тянет вниз.\n",
    "\n",
    "**Простыми словами:** итоговая регрессионная кривая строится как взвешенная сумма \"схожестей\" нового объекта со всеми точками-нарушителями (опорными векторами).\n",
    "\n",
    "#### Реализация в Scikit-Learn\n",
    "\n",
    "В `scikit-learn` есть несколько классов для регрессии методом опорных векторов:\n",
    "\n",
    "1.  **`sklearn.svm.SVR`**:\n",
    "    *   Самая универсальная реализация, поддерживающая все виды ядер (`linear`, `poly`, `rbf`).\n",
    "    *   **Ключевые гиперпараметры:**\n",
    "        *   `kernel`: Тип ядра. По умолчанию `'rbf'`.\n",
    "        *   `C`: Параметр регуляризации.\n",
    "        *   `gamma`: Коэффициент для `rbf` и `poly` ядер.\n",
    "        *   `epsilon`: Ширина $\\epsilon$-коридора.\n",
    "\n",
    "2.  **`sklearn.svm.LinearSVR`**:\n",
    "    *   Специализированная и быстрая реализация **только для линейного ядра**.\n",
    "    *   Значительно быстрее, чем `SVR(kernel='linear')` на больших данных.\n",
    "    *   **Когда использовать:** Когда зависимость в данных линейная, и важна скорость.\n",
    "\n",
    "3.  **`sklearn.svm.NuSVR`**:\n",
    "    *   Альтернативная реализация, использующая параметр `nu` вместо `C`.\n",
    "    *   `nu` (от 0 до 1) контролирует долю опорных векторов.\n",
    "\n",
    "**Основное отличие `SVR` от `LinearSVR`** — `SVR` позволяет строить нелинейные модели благодаря ядрам, в то время как `LinearSVR` оптимизирован исключительно для линейных задач."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3: Схема программирования модели (Workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Независимо от выбранного алгоритма (KNN, SVM или другой), процесс его применения в коде следует стандартной схеме. Для этого мы используем мощные библиотеки Python: `pandas` для работы с данными, `matplotlib` и `seaborn` для визуализации, и `scikit-learn` для машинного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Шаг 1: Загрузка и подготовка данных\n",
    "*   **Что делаем:** Загружаем данные (например, из CSV) и проводим разведочный анализ (EDA).\n",
    "*   **Библиотеки:** `import pandas as pd`, `import seaborn as sns`\n",
    "*   **Пример:** `df = pd.read_csv('data.csv')`, `sns.countplot(df['target'])`\n",
    "\n",
    "#### Шаг 2: Определение признаков (X) и цели (y)\n",
    "*   **Что делаем:** Разделяем наш датафрейм на матрицу признаков `X` и вектор целевой переменной `y`.\n",
    "*   **Пример:** `X = df.drop('target', axis=1)`, `y = df['target']`\n",
    "\n",
    "#### Шаг 3: Разделение на обучающую и тестовую выборки\n",
    "*   **Что делаем:** Отделяем часть данных, на которой модель не будет обучаться, чтобы честно оценить ее качество.\n",
    "*   **Библиотеки:** `from sklearn.model_selection import train_test_split`\n",
    "*   **Пример:** `X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)`\n",
    "\n",
    "#### Шаг 4: Масштабирование признаков\n",
    "*   **Что делаем:** Приводим все признаки к единому масштабу. **Критически важно для KNN и SVM!**\n",
    "*   **Библиотеки:** `from sklearn.preprocessing import StandardScaler`\n",
    "*   **Пример:** `scaler = StandardScaler()`, `X_train_scaled = scaler.fit_transform(X_train)`, `X_test_scaled = scaler.transform(X_test)`\n",
    "*   **Важно:** Метод `fit` (вычисление среднего и стандартного отклонения) вызывается **только на обучающих данных**, чтобы избежать утечки информации из теста!\n",
    "\n",
    "#### Шаг 5: Создание и обучение модели\n",
    "*   **Что делаем:** Создаем экземпляр модели и обучаем его на масштабированных обучающих данных.\n",
    "*   **Библиотеки:** `from sklearn.neighbors import KNeighborsClassifier`, `from sklearn.svm import SVC`\n",
    "*   **Пример:** `model = SVC(C=1.0, kernel='rbf')`, `model.fit(X_train_scaled, y_train)`\n",
    "\n",
    "#### Шаг 6: Предсказание и оценка качества\n",
    "*   **Что делаем:** Делаем предсказания на тестовых данных и сравниваем их с реальными значениями.\n",
    "*   **Библиотеки:** `from sklearn.metrics import classification_report, confusion_matrix`\n",
    "*   **Пример:** `predictions = model.predict(X_test_scaled)`, `print(classification_report(y_test, predictions))`\n",
    "\n",
    "#### Оптимизация: Pipeline и GridSearchCV\n",
    "Чтобы автоматизировать шаги 4-6 и найти лучшие гиперпараметры, используются `Pipeline` (объединяет шаги в конвейер) и `GridSearchCV` (перебирает параметры по сетке)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 4: Практические примеры на Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем все необходимые библиотеки\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Настройки для лучшей визуализации\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 1: Бинарная классификация (Подделка вина)\n",
    "\n",
    "**Задача:** На основе химического анализа определить, является ли вино настоящим (`Legit`) или поддельным (`Fraud`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем данные\n",
    "df_fraud = pd.read_csv('https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/data/wine_fraud.csv')\n",
    "\n",
    "# EDA\n",
    "print('Информация о датасете:')\n",
    "df_fraud.info()\n",
    "print('\\nБаланс классов:')\n",
    "print(df_fraud['quality'].value_counts())\n",
    "sns.countplot(x='quality', data=df_fraud)\n",
    "plt.title('Баланс классов в датасете о вине')\n",
    "plt.show()\n",
    "\n",
    "# Подготовка данных\n",
    "X = df_fraud.drop('quality', axis=1)\n",
    "# Конвертируем 'Legit'/'Fraud' в 0/1 для удобства\n",
    "y = df_fraud['quality'].map({'Legit': 0, 'Fraud': 1})\n",
    "\n",
    "# Обработка категориального признака 'type'\n",
    "X = pd.get_dummies(X, columns=['type'], drop_first=True)\n",
    "\n",
    "# Разделение данных\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модель KNN для бинарной классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем Pipeline: сначала масштабирование, потом модель KNN\n",
    "knn_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Задаем сетку параметров для перебора: ищем лучшее k\n",
    "k_values = list(range(1, 20))\n",
    "param_grid_knn = {'knn__n_neighbors': k_values}\n",
    "\n",
    "# Создаем и обучаем GridSearchCV\n",
    "grid_knn = GridSearchCV(knn_pipe, param_grid_knn, cv=5, scoring='accuracy')\n",
    "grid_knn.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Лучший параметр для KNN: {grid_knn.best_params_}\")\n",
    "\n",
    "# Оценка модели KNN\n",
    "knn_preds = grid_knn.predict(X_test)\n",
    "print(\"\\n--- Отчет по качеству модели KNN ---\")\n",
    "print(classification_report(y_test, knn_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    " \n",
    "cm = confusion_matrix(y_test, knn_preds)\n",
    " \n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Legit', 'Fraud'], yticklabels=['Legit', 'Fraud'])\n",
    "plt.xlabel('Предсказанный класс')\n",
    "plt.ylabel('Истинный класс')\n",
    "plt.title('Матрица ошибок для модели KNN')\n",
    "plt.savefig(\"confusion_matrix_heatmap.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модель SVM для бинарной классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем Pipeline для SVM\n",
    "svm_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    # Учитываем дисбаланс классов с помощью class_weight='balanced'\n",
    "    ('svm', SVC(class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Сетка параметров для SVM. C и gamma - ключевые параметры RBF ядра\n",
    "param_grid_svm = {\n",
    "    'svm__C': [0.1, 1, 10],\n",
    "    'svm__gamma': ['scale', 'auto', 0.1]\n",
    "}\n",
    "\n",
    "# Создаем и обучаем GridSearchCV\n",
    "grid_svm = GridSearchCV(svm_pipe, param_grid_svm, cv=5, scoring='accuracy')\n",
    "grid_svm.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Лучшие параметры для SVM: {grid_svm.best_params_}\")\n",
    "\n",
    "# Оценка модели SVM\n",
    "svm_preds = grid_svm.predict(X_test)\n",
    "print(\"\\n--- Отчет по качеству модели SVM ---\")\n",
    "print(classification_report(y_test, svm_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    " \n",
    "cm = confusion_matrix(y_test, svm_preds)\n",
    " \n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Legit', 'Fraud'], yticklabels=['Legit', 'Fraud'])\n",
    "plt.xlabel('Предсказанный класс')\n",
    "plt.ylabel('Истинный класс')\n",
    "plt.title('Матрица ошибок для модели SVM')\n",
    "plt.savefig(\"confusion_matrix_heatmap.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравнение KNN и SVM на задаче бинарной классификации\n",
    "\n",
    "**Выводы:**\n",
    "1.  **Метрики:** Сравните отчеты `classification_report`. Какая модель дает более высокий `f1-score` для класса `Fraud` (метка 1)? В задачах поиска мошенничества или дефектов часто важнее **полнота (recall)**, чтобы найти как можно больше случаев мошенничества, даже ценой ложных срабатываний.\n",
    "2.  **Производительность:** На этом датасете SVM, скорее всего, покажет себя лучше. Благодаря своей способности строить сложную границу с максимальным зазором, он эффективнее разделяет классы. \n",
    "3.  **Сложность:** Настройка SVM сложнее из-за большего количества гиперпараметров (`C`, `gamma`, тип ядра), но `GridSearchCV` автоматизирует этот процесс."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 2: Многоклассовая классификация (Сорта вин)\n",
    "\n",
    "**Задача:** Классифицировать вино по одному из трех сортов на основе 13 химических признаков.\n",
    "\n",
    "#### Как алгоритмы решают многоклассовую задачу?\n",
    "-   **KNN:** Естественным образом. Он просто находит K ближайших соседей и смотрит, какой из 3+ классов преобладает.\n",
    "-   **SVM:** SVM по своей природе бинарный классификатор. Для решения многоклассовой задачи он использует одну из стратегий:\n",
    "    -   **One-vs-Rest (OvR):** Обучается N классификаторов, где N — число классов. Каждый классификатор учится отличать \"свой\" класс от всех остальных (`Rest`).\n",
    "    -   **One-vs-One (OvO):** Обучается N * (N-1) / 2 классификаторов для каждой возможной пары классов. Новый объект классифицируется тем классом, который \"победил\" в большинстве \"дуэлей\". **Scikit-learn использует эту стратегию по умолчанию для `SVC`**, так как она часто эффективнее.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных из sklearn\n",
    "wine_data = load_wine()\n",
    "X = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)\n",
    "y = pd.Series(wine_data.target)\n",
    "\n",
    "# EDA\n",
    "print(\"Размеры датасета сортов вин:\", X.shape)\n",
    "print(\"\\nБаланс классов:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Разделение данных\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модель KNN для многоклассовой классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем тот же Pipeline, что и раньше\n",
    "grid_knn_multi = GridSearchCV(knn_pipe, param_grid_knn, cv=5, scoring='accuracy')\n",
    "grid_knn_multi.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Лучший параметр для KNN: {grid_knn_multi.best_params_}\")\n",
    "\n",
    "# Оценка\n",
    "knn_preds_multi = grid_knn_multi.predict(X_test)\n",
    "print(\"\\n--- Отчет по качеству модели KNN (многоклассовая) ---\")\n",
    "print(classification_report(y_test, knn_preds_multi))\n",
    "\n",
    "print(\"Матрица ошибок:\")\n",
    "print(confusion_matrix(y_test, knn_preds_multi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модель SVM для многоклассовой классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline для SVM без class_weight, т.к. классы сбалансированы\n",
    "svm_pipe_multi = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "param_grid_svm_multi = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__gamma': ['scale', 'auto', 0.1, 0.01]\n",
    "}\n",
    "\n",
    "grid_svm_multi = GridSearchCV(svm_pipe_multi, param_grid_svm_multi, cv=5, scoring='accuracy')\n",
    "grid_svm_multi.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Лучшие параметры для SVM: {grid_svm_multi.best_params_}\")\n",
    "\n",
    "svm_preds_multi = grid_svm_multi.predict(X_test)\n",
    "print(\"\\n--- Отчет по качеству модели SVM (многоклассовая) ---\")\n",
    "print(classification_report(y_test, svm_preds_multi))\n",
    "\n",
    "print(\"Матрица ошибок:\")\n",
    "print(confusion_matrix(y_test, svm_preds_multi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравнение на задаче многоклассовой классификации\n",
    "\n",
    "**Выводы:**\n",
    "1.  **Интерпретация метрик:** В отчете теперь есть строки для каждого из трех классов (0, 1, 2) и усредненные значения (`macro avg`, `weighted avg`).\n",
    "2.  **Матрица ошибок:** Теперь это матрица 3x3. Элемент на пересечении строки `i` и столбца `j` показывает, сколько объектов реального класса `i` было предсказано как класс `j`. Диагональные элементы — это правильно классифицированные объекты. Внедиагональные — ошибки. Анализ этой матрицы помогает понять, какие классы модель путает между собой.\n",
    "3.  **Результат:** На этом классическом датасете обе модели, скорее всего, покажут очень высокие результаты, близкие к 100% точности, но SVM с правильно подобранными параметрами часто оказывается немного точнее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итоговое заключение\n",
    "\n",
    "Мы рассмотрели два мощных, но очень разных по своей природе алгоритма классификации.\n",
    "\n",
    "| Характеристика | K-ближайших соседей (KNN) | Метод опорных векторов (SVM) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Основная идея** | Классификация по большинству голосов соседей | Поиск оптимальной разделяющей гиперплоскости |\n",
    "| **Тип модели** | Метрический, \"ленивый\" (не строит модель) | Геометрический, строит явную модель |\n",
    "| **Ключевые параметры**| `n_neighbors` (количество соседей) | `C` (регуляризация), `kernel`, `gamma` (для RBF) |\n",
    "| **Требования** | **Обязательное** масштабирование данных | **Обязательное** масштабирование данных |\n",
    "| **Лучше всего подходит** | Для быстрых прототипов, когда данные \"чистые\" и хорошо сгруппированы | Для сложных задач с нелинейными границами, в пространствах высокой размерности |\n",
    "| **Интерпретируемость** | Высокая (можно посмотреть на соседей) | Низкая (особенно с нелинейными ядрами) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
