{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 12: Кластеризация. Поиск структуры в данных без учителя\n",
    "\n",
    "**Цель лекции:**\n",
    "1. Понять разницу между обучением с учителем и **обучением без учителя**.\n",
    "2. Изучить **K-Means** — самый популярный алгоритм кластеризации на основе центроидов.\n",
    "3. Кратко познакомиться с **иерархической кластеризацией** и дендрограммами.\n",
    "4. Изучить **DBSCAN** — алгоритм кластеризации на основе плотности.\n",
    "5. Сравнить подходы, чтобы понимать, какой алгоритм для какой задачи подходит лучше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1: Введение в Обучение без учителя\n",
    "\n",
    "До сих пор мы работали в парадигме **обучения с учителем (Supervised Learning)**. У нас всегда была размеченная обучающая выборка с признаками `X` и правильными ответами `y`. Наша цель была — построить модель, которая предсказывает `y` по `X`.\n",
    "\n",
    "Теперь мы переходим к **обучению без учителя (Unsupervised Learning)**. Главное отличие — у нас есть **только признаки `X`**, а **правильных ответов `y` нет**. Задача — не предсказать метку, а найти скрытую структуру, закономерности или группы в самих данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Типы задач обучения без учителя\n",
    "\n",
    "Обучение без учителя — это целое семейство алгоритмов. Кластеризация — самый известный, но далеко не единственный его представитель.\n",
    "\n",
    "1.  **Кластеризация (Clustering):**\n",
    "    *   **Цель:** Сгруппировать похожие объекты вместе, а непохожие — разнести по разным группам (кластерам).\n",
    "    *   **Пример:** Сегментация клиентов магазина по их покупательскому поведению.\n",
    "    *   **Алгоритмы:** K-Means, DBSCAN, Иерархическая кластеризация.\n",
    "\n",
    "2.  **Уменьшение размерности (Dimensionality Reduction):**\n",
    "    *   **Цель:** Сократить количество признаков в датасете, сохранив при этом как можно больше полезной информации. Это помогает бороться с \"проклятием размерности\", ускорять обучение других моделей и визуализировать данные.\n",
    "    *   **Пример:** Преобразовать датасет с 50 признаками в 2 признака, чтобы нарисовать его на 2D-графике.\n",
    "    *   **Алгоритмы:** Метод главных компонент (PCA), t-SNE.\n",
    "\n",
    "3.  **Поиск ассоциативных правил (Association Rule Learning):**\n",
    "    *   **Цель:** Найти интересные взаимосвязи и закономерности между переменными в больших наборах данных.\n",
    "    *   **Пример:** Анализ чеков в супермаркете для поиска правил вида \"Если покупатель купил {Хлеб, Молоко}, то он с высокой вероятностью купит и {Масло}\".\n",
    "    *   **Алгоритмы:** Apriori, Eclat.\n",
    "\n",
    "В этой лекции мы сосредоточимся на **кластеризации**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 2: Алгоритм K-Means (Кластеризация на основе центроидов)\n",
    "\n",
    "K-Means — один из старейших и самых популярных алгоритмов кластеризации. Его идея очень проста: найти `K` центров, вокруг которых группируются данные.\n",
    "\n",
    "#### Интуиция: Алгоритм Ллойда\n",
    "Алгоритм работает итеративно:\n",
    "1.  **Шаг 0: Выбрать `K`** — количество кластеров.\n",
    "2.  **Шаг 1: Инициализация.** Случайно разместить `K` центроидов.\n",
    "3.  **Шаг 2: Присвоение.** Каждую точку отнести к ближайшему центроиду.\n",
    "4.  **Шаг 3: Обновление.** Пересчитать положение каждого центроида как среднее всех точек в его кластере.\n",
    "5.  **Повторение:** Повторять шаги 2 и 3 до сходимости.\n",
    "\n",
    "> **Интерактивная визуализация:** Поиграть с алгоритмом K-Means вживую можно здесь: \n",
    "> [https://www.naftaliharris.com/blog/visualizing-k-means-clustering/](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)\n",
    "\n",
    "Математически, K-Means пытается минимизировать **сумму квадратов внутрикластерных расстояний Within-Cluster Sum of Squares (WCSS)**, также известную как `inertia`:\n",
    "$$ \\Phi_0 = \\sum_{a=1}^{K} \\sum_{i: a_i=a} ||x_i - \\mu_a||^2 \\rightarrow \\min $$\n",
    "где $\\mu_a$ — это центр кластера $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means в Scikit-Learn\n",
    "\n",
    "*   **Класс:** `sklearn.cluster.KMeans`\n",
    "*   **Ключевые гиперпараметры:**\n",
    "    *   `n_clusters`: То самое `K`, количество кластеров. **Самый важный параметр.**\n",
    "    *   `init`: Метод инициализации центроидов.\n",
    "        *   `'random'`: Выбирает `K` случайных точек из датасета в качестве начальных центроидов. Этот метод очень быстрый, но может привести к неудачной сходимости, если начальные центры окажутся близко друг к другу.\n",
    "        *   `'k-means++'` (по умолчанию): Это \"умный\" метод инициализации. Он работает так:\n",
    "            1.  Первый центроид выбирается случайно.\n",
    "            2.  Каждый следующий центроид выбирается из оставшихся точек с вероятностью, пропорциональной **квадрату расстояния** до ближайшего из уже выбранных центроидов.\n",
    "            **Простыми словами:** `k-means++` старается выбрать начальные центроиды как можно **дальше друг от друга**. Это значительно повышает шансы на быструю и качественную сходимость алгоритма.\n",
    "    *   `n_init`: Сколько раз алгоритм будет запущен с разными случайными начальными центроидами. Модель вернет лучший результат (с наименьшим `inertia_`) из всех запусков.\n",
    "        *   **`'auto'` (по умолчанию с версии 1.4):** Это умная настройка. Если `init='k-means++'`, то будет выполнен только **один** запуск (так как этот метод детерминирован). Если `init='random'`, то будет выполнено **10** запусков. Это значение по умолчанию является отличным выбором в большинстве случаев.\n",
    "    *   `random_state`: Число для инициализации генератора случайных чисел. Используется для воспроизводимости результатов, так как `k-means++` и `'random'` содержат элемент случайности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Как выбрать `K`? Метод локтя (Elbow Method)\n",
    "\n",
    "Главный недостаток K-Means — мы должны заранее задать `K`. **Метод локтя** — это популярная эвристика для его поиска:\n",
    "1.  Мы запускаем K-Means для разного количества кластеров (например, от `K=2` до `10`).\n",
    "2.  Для каждого `K` мы вычисляем итоговое значение `WCSS` (атрибут `model.inertia_`).\n",
    "3.  Строим график зависимости `WCSS` от `K`.\n",
    "4.  Ищем на графике точку, похожую на \"локоть\" — место, где кривая начинает выполаживаться. Это и есть наш кандидат на оптимальное `K`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Устанавливаем несколько переменных окружения, чтобы покрыть разные библиотеки\n",
    "# Устанавливаем значение '1', чтобы гарантированно отключить проблемный параллелизм для этой операции\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "\n",
    "# Теперь импортируем все остальное\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Генерируем данные\n",
    "X, y = make_blobs(n_samples=500, centers=4, random_state=42, cluster_std=1.0)\n",
    "\n",
    "ssd = []\n",
    "k_range = range(1, 11)\n",
    "for k in k_range:\n",
    "    model = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    model.fit(X)\n",
    "    ssd.append(model.inertia_)\n",
    "\n",
    "# Рисуем график\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(k_range, ssd, 'o-')\n",
    "plt.xlabel(\"Количество кластеров (K)\")\n",
    "plt.ylabel(\"Sum of Squared Distances (WCSS)\")\n",
    "plt.title(\"Метод локтя для K-Means\")\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация результата с оптимальным K\n",
    "\n",
    "Из графика \"метода локтя\" мы видим, что оптимальное количество кластеров K=4.\n",
    "Теперь давайте обучим финальную модель с этим значением и посмотрим на результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Обучаем финальную модель с K=4\n",
    "final_model = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "labels = final_model.fit_predict(X)\n",
    "\n",
    "# Метки кластеров — это массив, где каждой точке присвоен номер кластера (от 0 до 3)\n",
    "print(\"Примеры меток кластеров для первых 10 точек:\")\n",
    "print(labels[:10])\n",
    "\n",
    "# 2. Визуализируем кластеры\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Рисуем точки, раскрашивая их в соответствии с метками кластеров\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=labels, palette='viridis', s=60)\n",
    "\n",
    "# Отдельно рисуем центры кластеров, чтобы они были хорошо видны\n",
    "centers = final_model.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.8, marker='X', label='Центроиды')\n",
    "\n",
    "plt.title(\"Результат кластеризации K-Means (K=4)\")\n",
    "plt.xlabel(\"Признак 1\")\n",
    "plt.ylabel(\"Признак 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Сначала нужно вычислить матрицу связей (linkage matrix)\n",
    "linked = linkage(X, method='ward')\n",
    "\n",
    "# Теперь строим дендрограмму\n",
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram(linked,\n",
    "            orientation='top',\n",
    "            distance_sort='descending',\n",
    "            show_leaf_counts=True)\n",
    "plt.title('Дендрограмма иерархической кластеризации')\n",
    "plt.ylabel('Расстояние по Уорду')\n",
    "\n",
    "# Добавим линию среза для выбора K\n",
    "plt.axhline(y=35, c='k', linestyle='--') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведя горизонтальную линию там, где мы видим самые длинные вертикальные отрезки, мы пересекаем 4 линии, что также указывает на 4 кластера. Это - иерархическая кластеризация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 3: Альтернативный подход — Иерархическая кластеризация\n",
    "\n",
    "Помимо K-Means, существует другой популярный метод, основанный на расстоянии, — **иерархическая кластеризация**. Ее главная особенность в том, что она не просто делит данные на `K` кластеров, а строит целое \\\"дерево\\\" вложенных друг в друга кластеров.\n",
    "\n",
    "#### Интуиция: Агломеративный подход\n",
    "\n",
    "Самый распространенный вид иерархической кластеризации — **агломеративный** (объединительный). Алгоритм работает \"снизу вверх\":\n",
    "1.  **Начало:** Каждая точка данных считается отдельным кластером.\n",
    "2.  **Шаг 1:** Находятся два **самых близких** друг к другу кластера и объединяются в один.\n",
    "3.  **Шаг 2:** Снова находятся два самых близких кластера и объединяются.\n",
    "4.  **Повторение:** Процесс повторяется до тех пор, пока все точки не окажутся в одном большом кластере.\n",
    "\n",
    "#### Иерархическая кластеризация в Scikit-Learn и SciPy\n",
    "*   **Класс:** `sklearn.cluster.AgglomerativeClustering`\n",
    "*   **Ключевые гиперпараметры:**\n",
    "    *   `n_clusters`: Количество кластеров, которое мы хотим получить в итоге (если `distance_threshold=None`).\n",
    "    *   `metric` (или `affinity`): Метрика расстояния между точками (`euclidean`, `manhattan`).\n",
    "    *   `linkage`: **Самый важный параметр.** Правило, по которому измеряется расстояние **между кластерами**.\n",
    "\n",
    "#### Как работают разные методы `linkage`?\n",
    "\n",
    "Пусть у нас есть два кластера, $U$ и $V$, и мы хотим измерить расстояние между ними, $D(U, V)$.\n",
    "\n",
    "![linkage](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/linkage.png)\n",
    "\n",
    "*   **`'single'` (Расстояние ближнего соседа):**\n",
    "    *   **Интуиция:** Расстояние между кластерами — это расстояние между **двумя самыми близкими** точками из этих кластеров. Это \"оптимистичный\" подход, который хорошо находит вытянутые, ленточные кластеры.\n",
    "    $$ D(U, V) = \\min_{u \\in U, v \\in V} d(u, v) $$\n",
    "\n",
    "*   **`'complete'` (Расстояние дальнего соседа):**\n",
    "    *   **Интуиция:** Расстояние между кластерами — это расстояние между **двумя самыми далекими** точками из этих кластеров. Этот \"пессимистичный\" подход стремится создавать более компактные, шарообразные кластеры.\n",
    "    $$ D(U, V) = \\max_{u \\in U, v \\in V} d(u, v) $$\n",
    "\n",
    "*   **`'average'` (Групповое среднее расстояние):**\n",
    "    *   **Интуиция:** Расстояние — это **среднее арифметическое** всех возможных попарных расстояний между точками из одного кластера и точками из другого. Это компромисс между `single` и `complete`.\n",
    "    $$ D(U, V) = \\frac{1}{|U| \\cdot |V|} \\sum_{u \\in U} \\sum_{v \\in V} d(u, v) $$\n",
    "\n",
    "*   **`'ward'` (Метод Уорда):**\n",
    "    *   **Интуиция:** Этот метод работает иначе. Он объединяет те два кластера, слияние которых приведет к **минимальному увеличению суммарной внутрикластерной дисперсии (WCSS)**. По сути, на каждом шаге он выбирает такое объединение, которое создает наиболее компактный из всех возможных новых кластеров.\n",
    "    *   Этот метод стремится создавать кластеры примерно одинакового размера и часто является лучшим выбором по умолчанию, если вы ожидаете найти сферические кластеры.\n",
    "\n",
    "#### Визуализация: Дендрограмма\n",
    "\n",
    "Весь этот процесс объединения можно визуализировать с помощью **дендрограммы**, которую удобнее всего строить с помощью библиотеки `SciPy`.\n",
    "\n",
    "**Как читать дендрограмму:**\n",
    "*   По **вертикали** отложена высота — расстояние (согласно выбранному методу `linkage`), на котором произошло объединение. Длинные вертикальные линии означают, что объединялись очень далекие друг от друга кластеры.\n",
    "*   **Дендрограмма** — это еще один способ выбрать оптимальное `K`. Мы можем провести горизонтальную линию-срез в том месте, где она пересечет самую длинную вертикальную линию. Количество пересечений и будет рекомендованным числом кластеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Визуализация: Дендрограмма на практическом примере\n",
    "\n",
    "Давайте посмотрим, как работает иерархическая кластеризация и дендрограмма на простом и наглядном примере. Создадим небольшой датасет, где есть 12 человек и два признака: \"Средние часы работы в день\" и \"Месячный доход (в тыс.)\". Интуитивно мы ожидаем увидеть 3 группы: офисные работники, студенты и фрилансеры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Создаем наглядный датасет\n",
    "X = np.array([\n",
    "    # Группа 1: Офисные работники (много часов, средний доход)\n",
    "    [8, 150], [9, 160], [8.5, 140], [7.5, 155],\n",
    "    # Группа 2: Студенты (мало часов, низкий доход)\n",
    "    [3, 40], [4, 50], [2.5, 35], [3.5, 45],\n",
    "    # Группа 3: Фрилансеры (гибкие часы, высокий доход)\n",
    "    [5, 200], [6, 210], [4.5, 190], [5.5, 220]\n",
    "])\n",
    "\n",
    "# 1. Визуализируем исходные данные\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], s=100)\n",
    "plt.title('Исходные данные: 12 человек')\n",
    "plt.xlabel('Часы работы в день')\n",
    "plt.ylabel('Доход (в тыс.)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 2. Строим дендрограмму\n",
    "# linkage вычисляет матрицу связей, 'ward' минимизирует дисперсию при объединении\n",
    "linked = linkage(X, method='ward')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram(linked,\n",
    "            orientation='top',\n",
    "            labels=[f\"Точка {i}\" for i in range(len(X))], # Подписываем точки\n",
    "            distance_sort='descending',\n",
    "            show_leaf_counts=True)\n",
    "\n",
    "plt.title('Дендрограмма иерархической кластеризации')\n",
    "plt.ylabel('Расстояние по Уорду')\n",
    "plt.axhline(y=100, c='k', linestyle='--') # Добавим линию среза\n",
    "plt.show()\n",
    "\n",
    "# 3. Применяем AgglomerativeClustering с K=3\n",
    "agg_cluster = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "labels = agg_cluster.fit_predict(X)\n",
    "\n",
    "# 4. Визуализируем результат кластеризации\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=labels, palette='viridis', s=100)\n",
    "plt.title('Результат иерархической кластеризации (K=3)')\n",
    "plt.xlabel('Часы работы в день')\n",
    "plt.ylabel('Доход (в тыс.)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Интерпретация результатов:**\n",
    "\n",
    "1.  **Исходные данные:** На первом графике мы четко видим 3 визуальных кластера.\n",
    "2.  **Дендрограмма:**\n",
    "    *   **Нижние уровни:** Алгоритм сначала объединяет самые близкие точки внутри каждой группы (например, двух студентов с похожими параметрами).\n",
    "    *   **Средние уровни:** Затем он объединяет эти небольшие подгруппы в три больших кластера. Высота \"перемычек\" внутри этих кластеров относительно небольшая.\n",
    "    *   **Верхний уровень:** Чтобы объединить эти 3 больших кластера, алгоритму приходится делать \"прыжки\" на очень большое расстояние (длинные вертикальные линии).\n",
    "3.  **Выбор K:** Проведя горизонтальную линию среза (`y=100`) через самую длинную вертикальную линию, мы пересекаем ровно **3** ветви. Это подтверждает нашу интуицию, что оптимальное количество кластеров — **три**.\n",
    "4.  **Итоговая кластеризация:** Финальный график показывает, что `AgglomerativeClustering` с `n_clusters=3` идеально справился с задачей, в точности выделив те группы, которые мы и ожидали увидеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 4: Где K-Means терпит неудачу?\n",
    "\n",
    "K-Means очень эффективен, но у него есть фундаментальное ограничение: он предполагает, что кластеры имеют **сферическую (выпуклую) форму** и примерно одинаковый размер. Он терпит неудачу на кластерах сложной формы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Генерируем данные\n",
    "# Датасет \"полумесяцы\"\n",
    "X_moons, y_moons = make_moons(n_samples=500, noise=0.1, random_state=42)\n",
    "# Датасет \"кольца\"\n",
    "X_circles, y_circles = make_circles(n_samples=500, factor=0.5, noise=0.05, random_state=42)\n",
    "\n",
    "# Масштабируем данные, так как K-Means основан на расстоянии\n",
    "X_moons_scaled = StandardScaler().fit_transform(X_moons)\n",
    "X_circles_scaled = StandardScaler().fit_transform(X_circles)\n",
    "\n",
    "# 2. Обучаем K-Means для каждого набора данных\n",
    "# Мы знаем, что в обоих случаях должно быть 2 кластера, поэтому n_clusters=2\n",
    "kmeans_moons = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "kmeans_circles = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "\n",
    "# Получаем метки кластеров\n",
    "labels_moons = kmeans_moons.fit_predict(X_moons_scaled)\n",
    "labels_circles = kmeans_circles.fit_predict(X_circles_scaled)\n",
    "\n",
    "# 3. Визуализируем результаты\n",
    "# Создаем область для двух графиков рядом\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# График для \"полумесяцев\"\n",
    "sns.scatterplot(ax=axes[0], x=X_moons[:,0], y=X_moons[:,1], hue=labels_moons, palette='viridis', s=50)\n",
    "axes[0].set_title('Неудачный результат K-Means на \\\"полумесяцах\\\"')\n",
    "axes[0].set_xlabel('Признак 1')\n",
    "axes[0].set_ylabel('Признак 2')\n",
    "\n",
    "\n",
    "# График для \"колец\"\n",
    "sns.scatterplot(ax=axes[1], x=X_circles[:,0], y=X_circles[:,1], hue=labels_circles, palette='viridis', s=50)\n",
    "axes[1].set_title('Неудачный результат K-Means на \\\"кольцах\\\"')\n",
    "axes[1].set_xlabel('Признак 1')\n",
    "axes[1].set_ylabel('Признак 2')\n",
    "\n",
    "\n",
    "plt.suptitle('Ограничения K-Means: кластеры нелинейной формы', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 5: Алгоритм DBSCAN (Кластеризация на основе плотности)\n",
    "\n",
    "DBSCAN предлагает совершенно другой подход. Вместо поиска центров, он ищет **плотные скопления точек**.\n",
    "\n",
    "> **Интерактивная визуализация:** Поиграть с алгоритмом DBSCAN вживую можно здесь: \n",
    "> [https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)\n",
    "\n",
    "#### Ключевые понятия и гиперпараметры\n",
    "DBSCAN определяется двумя гиперпараметрами:\n",
    "1.  **`eps` (эпсилон):** Радиус окрестности. Это расстояние, в пределах которого мы ищем соседей.\n",
    "2.  **`min_samples`:** Минимальное число соседей (включая саму точку) в `eps`-окрестности, чтобы точка считалась \"ядром плотности\".\n",
    "\n",
    "#### DBSCAN в Scikit-Learn\n",
    "*   **Класс:** `sklearn.cluster.DBSCAN`\n",
    "*   **Ключевые гиперпараметры:** `eps` и `min_samples`.\n",
    "  \n",
    "На основе этих параметров все точки делятся на три типа:\n",
    "\n",
    "![dbscan](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/dbscan.png)\n",
    "\n",
    "*   **Core Point (Ядровая):** Точка, у которой в `eps`-окрестности есть не менее `min_samples` соседей. Это \"сердце\" кластера.\n",
    "*   **Border Point (Граничная):** Точка, у которой соседей меньше, чем `min_samples`, но она сама является соседом какой-либо ядровой точки. Это \"край\" кластера.\n",
    "*   **Noise (Шум/Выброс):** Точка, которая не является ни ядровой, ни граничной. Она лежит в разреженной области.\n",
    "\n",
    "#### Алгоритм на пальцах\n",
    "1.  Выбирается случайная, еще не посещенная точка.\n",
    "2.  Если она **ядровая**, то она начинает **новый кластер**.\n",
    "3.  Этот кластер начинает \"расти\": все ее соседи добавляются в этот же кластер. Если кто-то из соседей тоже оказывается ядровой точкой, то и его соседи тоже присоединяются. Процесс продолжается, пока кластер не перестанет расти.\n",
    "4.  Если точка **не ядровая**, она временно помечается как \"шум\". Позже она может стать граничной, если окажется в окрестности какого-то другого кластера.\n",
    "5.  Процесс повторяется для всех точек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример, где K-Means плох, а DBSCAN хорош\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X_moons, y_moons = make_moons(n_samples=500, noise=0.1, random_state=42)\n",
    "X_moons_scaled = StandardScaler().fit_transform(X_moons)\n",
    "\n",
    "# Модели\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "dbscan = DBSCAN(eps=0.3)\n",
    "\n",
    "# Предсказания\n",
    "labels_kmeans = kmeans.fit_predict(X_moons_scaled)\n",
    "labels_dbscan = dbscan.fit_predict(X_moons_scaled)\n",
    "\n",
    "# Визуализация\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "sns.scatterplot(ax=axes[0], x=X_moons[:,0], y=X_moons[:,1], hue=labels_kmeans, palette='viridis')\n",
    "axes[0].set_title('Неудачный результат K-Means')\n",
    "\n",
    "sns.scatterplot(ax=axes[1], x=X_moons[:,0], y=X_moons[:,1], hue=labels_dbscan, palette='viridis')\n",
    "axes[1].set_title('Успешный результат DBSCAN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_circles  # Заменяем make_moons на make_circles\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Генерируем данные в виде двух колец\n",
    "# factor=0.5 означает, что внутреннее кольцо будет в два раза меньше внешнего\n",
    "X_circles, y_circles = make_circles(n_samples=500, noise=0.05, factor=0.5, random_state=42)\n",
    "\n",
    "# Масштабируем данные, что важно для DBSCAN, так как eps - это мера расстояния\n",
    "X_circles_scaled = StandardScaler().fit_transform(X_circles)\n",
    "\n",
    "# 2. Инициализируем модели\n",
    "# Для K-Means мы знаем, что кластеров должно быть 2\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "# Для DBSCAN подбираем eps. Для компактных колец он должен быть меньше, чем для лун.\n",
    "dbscan = DBSCAN(eps=0.3)\n",
    "\n",
    "# 3. Обучаем модели и получаем метки кластеров\n",
    "labels_kmeans = kmeans.fit_predict(X_circles_scaled)\n",
    "labels_dbscan = dbscan.fit_predict(X_circles_scaled)\n",
    "\n",
    "# 4. Визуализируем результаты\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# График для K-Means\n",
    "sns.scatterplot(ax=axes[0], x=X_circles[:,0], y=X_circles[:,1], hue=labels_kmeans, palette='viridis')\n",
    "axes[0].set_title('Неудачный результат K-Means на \"кольцах\"')\n",
    "axes[0].set_xlabel(\"Признак 1\")\n",
    "axes[0].set_ylabel(\"Признак 2\")\n",
    "\n",
    "# График для DBSCAN\n",
    "sns.scatterplot(ax=axes[1], x=X_circles[:,0], y=X_circles[:,1], hue=labels_dbscan, palette='viridis')\n",
    "axes[1].set_title('Успешный результат DBSCAN на \"кольцах\"')\n",
    "axes[1].set_xlabel(\"Признак 1\")\n",
    "axes[1].set_ylabel(\"Признак 2\")\n",
    "\n",
    "plt.suptitle('Сравнение K-Means и DBSCAN на данных в виде колец', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 6: Интерпретация кластеров — нетривиальная задача\n",
    "\n",
    "Важно понимать, что алгоритмы кластеризации возвращают лишь **номера кластеров** (0, 1, 2, ...). Они не говорят нам, **что означают** эти группы. Присвоение бизнес-смысла полученным кластерам — это отдельная и очень важная задача, которая называется **пост-обработкой** или **интерпретацией**.\n",
    "\n",
    "**Основной подход:**\n",
    "1.  **Добавить метки:** Добавить полученные метки кластеров в исходный, **немасштабированный** DataFrame.\n",
    "2.  **Профилирование кластеров:** Использовать `groupby('cluster').mean()` или `.describe()` для анализа средних значений и распределений признаков внутри каждого кластера.\n",
    "3.  **Создание \"персон\":** На основе анализа дать каждому кластеру осмысленное название. Например, если мы кластеризуем клиентов интернет-магазина, у нас могут получиться:\n",
    "    *   **Кластер 0 (\"Лояльные VIP-клиенты\"):** Высокий средний чек, высокая частота покупок, большой `total_spent`.\n",
    "    *   **Кластер 1 (\"Охотники за скидками\"):** Низкий средний чек, покупки только в период распродаж.\n",
    "    *   **Кластер 2 (\"Новички\"):** Мало покупок, низкий `total_spent`.\n",
    "\n",
    "Эта интерпретация позволяет превратить математический результат в **полезный для бизнеса инструмент**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 7: Итоговые выводы по лекции\n",
    "\n",
    "1.  Мы перешли от **Обучения с учителем** к **Обучению без учителя**, где главная задача — найти **скрытую структуру** в данных, а не предсказать известную метку.\n",
    "\n",
    "2.  Мы рассмотрели **K-Means** — быстрый и простой алгоритм, который ищет **сферические кластеры** на основе центроидов. Его главный недостаток — необходимость заранее задавать количество кластеров `K`, которое можно оценить с помощью **метода локтя**.\n",
    "\n",
    "3.  Мы кратко коснулись **Иерархической кластеризации** и **дендрограмм** — мощного визуального инструмента, который также помогает в определении числа кластеров.\n",
    "\n",
    "4.  Мы изучили **DBSCAN** — алгоритм, основанный на **плотности**. Он не требует заранее знать число кластеров, умеет находить кластеры **произвольной формы** и автоматически определяет **выбросы**.\n",
    "\n",
    "5.  Главный вывод: **не существует универсально \"лучшего\" алгоритма кластеризации**. Выбор метода зависит от структуры ваших данных и целей исследования."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
