{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11-дәріс: Ағаштар ансамбльдері. Кездейсоқ Орман\n",
    "\n",
    "**Дәріс мақсаты:**\n",
    "1.  Модельдердің сапасы мен тұрақтылығын арттыру тәсілі ретінде **ансамбльдік әдістер** тұжырымдамасымен танысу.\n",
    "2.  Кездейсоқ орманның негізі ретінде **бэггинг (Bagging)** әдісін талдау.\n",
    "3.  **Кездейсоқ Орман (Random Forest)** алгоритмін, оның артықшылықтарын және практикалық қолданылуын зерттеу.\n",
    "4.  Балама тәсіл — **бустингке (Boosting)** қысқаша тоқталу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-бөлім: Неліктен бір ағаш жеткіліксіз? Көпшілік даналығы\n",
    "\n",
    "Өткен дәрісте біз шешімдер ағаштарының елеулі кемшілігі бар екенін анықтадық: олар **тұрақсыз** және **артық оқытуға** қатты бейім. Деректердегі шамалы өзгеріс ағаштың құрылымын түбегейлі өзгертуі мүмкін.\n",
    "\n",
    "**Бұл мәселені қалай шешуге болады?** Күнделікті өмірдегі даналыққа жүгінейік. Маңызды шешім қабылдағымыз келгенде, біз сирек бір сарапшының пікіріне сүйенеміз. Біз бірнеше пікір жинап, қорытынды шешімді консенсус негізінде қабылдаймыз. Бұл принцип **\"көпшілік даналығы\"** деп аталады.\n",
    "\n",
    "Машиналық оқытуда бұл тәсіл **ансамбльдеу** деп аталады. Идеясы — көптеген әртүрлі (әлсіз немесе тұрақсыз) модельдерді оқытып, олардың болжамдарын біріктіріп, бір, бірақ дәлірек және робасты болжам алу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ensamble](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/ensamble.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-бөлім: Бэггинг (Bagging = Bootstrap Aggregating)\n",
    "\n",
    "Бэггинг — ең танымал ансамбльдік әдістердің бірі және кездейсоқ орманның негізі. Оның атауы екі бөліктен тұрады.\n",
    "\n",
    "#### 1-қадам: Bootstrap (Бутстрэп)\n",
    "\n",
    "Біздің модельдеріміз (ағаштар) әртүрлі болуы үшін, оларды **әртүрлі деректерде** оқытуымыз керек. Бірақ бізде тек бір ғана оқыту таңдамасы бар. Басқаларын қайдан аламыз?\n",
    "\n",
    "Бутстрэп — бұл `N` өлшемді бір таңдамадан сол `N` өлшемді көптеген **ішкі таңдамалар** генерациялауға мүмкіндік беретін статистикалық әдіс. Бұл **бастапқы таңдамадан объектілерді қайтару арқылы кездейсоқ таңдау** жолымен жасалады.\n",
    "\n",
    "![bootstrap](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/bootstrap1.png)\n",
    "Тағы бір мысал\n",
    "![bootstrap1](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/bootstrapping.png)\n",
    "\n",
    "Нәтижесінде әрбір ішкі таңдама бірегей болады: кейбір бастапқы объектілер онда қайталанады, ал кейбіреулері (орташа есеппен 37%-ға жуығы) мүлдем түспейді.\n",
    "\n",
    "#### 2-қадам: Aggregation (Агрегаттау)\n",
    "\n",
    "Алынған `T` бутстрэп-ішкі таңдамалардың әрқайсысында біз **өз тәуелсіз шешімдер ағашын** $b_t(x)$ оқытамыз. Әдетте бұл ағаштарды өте терең етіп жасайды, оларға өз деректер бөлігінде әдейі артық оқытуға мүмкіндік береді.\n",
    "\n",
    "Жаңа объект үшін болжам жасау қажет болғанда, біз барлық `T` болжамды жинап, оларды агрегаттаймыз.\n",
    "\n",
    "**Классификация үшін — мажоритарлық дауыс беру:**\n",
    "Қорытынды болжам $a(x)$ ретінде ағаштардың көпшілігі дауыс берген класс алынады.\n",
    "$$ a(x) = \\text{sign} \\left( \\frac{1}{T} \\sum_{t=1}^{T} b_t(x) \\right) $$\n",
    "\n",
    "**Регрессия үшін — орташалау:**\n",
    "Қорытынды болжам $a(x)$ ретінде барлық ағаштардың болжамдарының арифметикалық ортасы алынады.\n",
    "$$ a(x) = \\frac{1}{T} \\sum_{t=1}^{T} b_t(x) $$\n",
    "\n",
    "Нәтижесінде жекелеген артық оқытылған ағаштардың қателері мен \"қызықтары\" бір-бірін өтейді, және ансамбльдің қорытынды болжамы әлдеқайда дәлірек және тұрақты болады."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Математикалық негіздеме: орташалау қатені қалай азайтады\n",
    "\n",
    "Неліктен \"көпшілік даналығы\" жұмыс істейді? Бұл әсерді ықтималдықтар теориясының негіздері арқылы оңай түсіндіруге болады.\n",
    "\n",
    "Регрессия есебін шешіп жатырмыз делік. Айталық:\n",
    "*   **$\\theta$** — біз болжағымыз келетін шын, дұрыс мән.\n",
    "*   **$x_j$** — $j$-ші \"сарапшының\" (немесе біздің ансамбльдегі $j$-ші ағаштың) жауабы.\n",
    "\n",
    "Әрбір жауапты аддитивті модель түрінде көрсетуге болады:\n",
    "$$ x_j = \\theta + \\eta_j $$\n",
    "мұндағы $\\eta_j$ — $j$-ші сарапшының кездейсоқ қатесі.\n",
    "\n",
    "Қарапайымдылық үшін екі қисынды болжам жасаймыз:\n",
    "1.  **Қателер ығыспаған:** Орташа есеппен, сарапшылар жүйелі түрде бір жаққа қателеспейді. Математикалық тұрғыдан: қатенің орташа мәні нөлге тең, $E[\\eta_j] = 0$.\n",
    "2.  **Қателер тәуелсіз:** Бір сарапшының қатесі екіншісінің қатесімен байланысты емес. Математикалық тұрғыдан: $i \\neq j$ үшін $E[\\eta_i \\eta_j] = 0$. Бұл — біз бутстрэп пен `max_features` қолданатын негізгі шарт!\n",
    "\n",
    "Әрбір жеке сарапшы қатесінің **дисперсиясы** $\\sigma^2$ болсын. Дисперсия — бұл болжамдардың \"шашырау\" немесе тұрақсыздық өлшемі.\n",
    "$$ \\sigma^2 = E[\\eta_j^2] $$\n",
    "\n",
    "Енді $T$ сарапшының жауаптарын орташалау арқылы ансамбльіміздің болжамын табайық:\n",
    "$$ y = \\frac{1}{T} \\sum_{j=1}^{T} x_j $$\n",
    "\n",
    "Осы орташаланған жауаптың қатесінің дисперсиясы қандай болады?\n",
    "$$ E[(\\theta - y)^2] = E\\left[\\left(\\theta - \\frac{1}{T} \\sum_{j=1}^{T} x_j\\right)^2\\right] = E\\left[\\left(\\frac{1}{T} \\sum_{j=1}^{T} (\\theta - x_j)\\right)^2\\right] = E\\left[\\left(\\frac{1}{T} \\sum_{j=1}^{T} \\eta_j\\right)^2\\right] $$\n",
    "\n",
    "Қосындының квадратын ашып, қателердің тәуелсіздігі қасиетін пайдалансақ, мынаны аламыз:\n",
    "$$ \\frac{1}{T^2} \\sum_{j=1}^{T} E[\\eta_j^2] = \\frac{1}{T^2} \\cdot T \\cdot \\sigma^2 = \\frac{\\sigma^2}{T} $$\n",
    "\n",
    "**Қорытынды:**\n",
    "> Орташаланған жауаптың (ансамбльдің) дисперсиясы жеке бір ағаштың жауабының дисперсиясынан **$T$ есе аз**.\n",
    "\n",
    "**Қарапайым сөзбен айтқанда:** $T$ тәуелсіз, бірақ шулы модельдердің болжамдарын орташалау арқылы біз қорытынды \"шуды\" (қатені) $T$ есе азайтамыз. Дәл осы себепті бэггинг пен кездейсоқ орман жалғыз ағаштардың тұрақсыздығымен және артық оқытылуымен тиімді күреседі. Біз қаншалықты көп ағаш қоссақ, олардың жеке қателері соншалықты қатты тегістеледі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-бөлім: Кездейсоқ Орман (Random Forest) — \"ерекшелігі\" бар Бэггинг\n",
    "\n",
    "**Кездейсоқ орман** — бұл ағаштар бэггингінің жетілдірілген нұсқасы. Ол ағаштарды бір-бірінен одан да тәуелсіз ету үшін тағы бір кездейсоқтық элементін қосады.\n",
    "\n",
    "**Бэггингтен айырмашылығы:**\n",
    "Әрбір ағаштағы **әрбір түйінді** құру кезінде алгоритм ең жақсы сплитті **барлық** қолжетімді белгілерден емес, тек сол белгілердің **кездейсоқ ішкі жиынынан** таңдайды.\n",
    "\n",
    "Бұл параметр `scikit-learn`-де `max_features` деп аталады.\n",
    "\n",
    "**Бұл не үшін қажет?**\n",
    "Деректеріңізде бір өте күшті, басым белгі бар деп елестетіңіз. Кәдімгі бэггингте көптеген ағаштар түбірдегі бірінші бөлу үшін дәл сол белгіні таңдайды. Нәтижесінде барлық ағаштар бір-біріне өте ұқсас болады (\"корреляцияланған\"), және орташалаудан болатын әсер әлсірейді.\n",
    "\n",
    "Кездейсоқ орман кейбір ағаштарды осы күшті белгіге **қол жеткізбестен** құруға мәжбүрлейді, оларды деректердегі басқа, баламалы заңдылықтарды табуға итермелейді. Бұл ансамбльдегі \"пікірлердің әртүрлілігін\" арттырады және әдетте қорытынды дәлдікті жоғарылатады."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![randomforest1](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/random_forest1.png)\n",
    "![randomforest1](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/randomforest1.png)\n",
    "![randomforest1](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/randomforest2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-бөлім: Тарихи анықтама және Scikit-Learn-дегі іске асырылуы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Негізгі тұлғалар мен идеялар\n",
    "*   **CART (Classification and Regression Trees):** `scikit-learn`-дегі жеке ағаштарды құру негізінде жатқан алгоритмді **Лео Брейман** және т.б. 1984 жылы ұсынған.\n",
    "*   **Random Subspace Method:** Классификаторларды белгілердің кездейсоқ ішкі жиындарында оқыту идеясын **Тин Кам Хо** 1998 жылы ұсынған. Бұл `max_features` параметрінің негізі.\n",
    "*   **Бэггинг және Кездейсоқ Орман:** **Лео Брейман** 1994 жылы бэггингті ұсынды, ал 2001 жылы оны кездейсоқ ішкі кеңістіктер әдісімен біріктіріп, **Random Forest** алгоритмін ресімдеп, танымал етті.\n",
    "\n",
    "#### Scikit-Learn-дегі іске асырылуы\n",
    "`scikit-learn`-де екі негізгі класс бар:\n",
    "- `sklearn.ensemble.RandomForestClassifier` — классификация есептері үшін.\n",
    "- `sklearn.ensemble.RandomForestRegressor` — регрессия есептері үшін.\n",
    "\n",
    "**Негізгі гиперпараметрлер:**\n",
    "- `n_estimators`: Ормандағы ағаштар саны. Ең маңызды параметр. Неғұрлым көп болса, соғұрлым жақсы (белгілі бір шекке дейін), бірақ оқыту ұзағырақ болады.\n",
    "- `max_features`: Ең жақсы сплитті іздеу кезінде қарастырылатын белгілер саны. Әдеттегі мәндер: `'sqrt'` (жалпы белгілер санының квадрат түбірі), `'log2'` немесе нақты сан.\n",
    "- `criterion`: Сплит сапасын бағалау критерийі (`'gini'` немесе `'entropy'` классификация үшін, `'squared_error'` регрессия үшін).\n",
    "- `max_depth`, `min_samples_split`, `min_samples_leaf`: Әрбір жеке ағашты реттеуге арналған параметрлер (`DecisionTreeClassifier`-ге ұқсас).\n",
    "- `bootstrap`: `True` (әдепкі) немесе `False`. Бутстрэп механизмін қосады немесе өшіреді.\n",
    "- `oob_score`: `True` немесе `False`. Out-of-Bag бағалауын қосады немесе өшіреді."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-Bag (OOB) Score дегеніміз не?\n",
    "\n",
    "Бұл Кездейсоқ Орманның ең талғампаз ерекшеліктерінің бірі.\n",
    "\n",
    "**Идея:** №1 ағашты оқыту үшін бутстрэп-ішкі таңдамасын жасағанда, бастапқы деректердің шамамен 37%-ы оған түспеді. Осы \"қалдырылған\" деректерді дәл осы №1 ағаш үшін **валидациялық жиын** ретінде пайдалануға болады.\n",
    "\n",
    "**Процесс:**\n",
    "1.  Бастапқы оқыту таңдамасындағы **әрбір** $x_i$ объектісі үшін біз осы объект **пайдаланылмаған** (яғни \"out-of-bag\" болған) барлық ағаштарды табамыз.\n",
    "2.  Біз сол ағаштардан $x_i$ объектісі үшін болжам жасауды сұраймыз.\n",
    "3.  $x_i$ объектісі үшін қорытынды \"OOB-болжамын\" алу үшін олардың болжамдарын орташалаймыз.\n",
    "4.  Барлық OOB-болжамдарды нақты $y_i$ белгілерімен салыстырып, **Out-of-Bag Score** аламыз — бұл бөлек валидациялық жиын құруды қажет етпестен алынған модель сапасының шынайы бағасы!\n",
    "\n",
    "Бұл мүмкіндікті пайдалану үшін `bootstrap=True` (бұл әдепкі мән) және `oob_score=True` деп орнату керек."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ensamble](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/randomforest4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тереңірек үңілу: `max_features` не үшін қажет және оның мәндері қайдан шыққан?\n",
    "\n",
    "`max_features` параметрі — Кездейсоқ Орманды жай ғана ағаштар Бэггингінен ерекшелейтін \"айрықша белгі\". Оның не үшін қажет екенін және неліктен оған $\\sqrt{n_{features}}$ сияқты таңқаларлық мәндер қолданылатынын қарастырайық.\n",
    "\n",
    "**Басты мақсат: Ағаштарды декорреляциялау**\n",
    "\n",
    "Есімізде болса, \"көпшілік даналығы\" көпшіліктің пікірлері **тәуелсіз** және **әртүрлі** болғанда ең жақсы жұмыс істейді. Егер барлық \"сарапшылар\" (ағаштар) бірдей ойласа, онда ансамбль бір сарапшыдан ақылды болмайды.\n",
    "\n",
    "**Мәселе:** Деректеріңізде бір өте күшті, басым белгі бар деп елестетіңіз (мысалы, қымбат тауарды сатып алуды болжау кезінде \"табыс\"). Егер біз ағаштарды қарапайым бэггинг әдісімен құратын болсақ, онда әрбір ағаш дерлік ең бірінші, түбірлік бөлу үшін дәл осы белгіні таңдауы мүмкін. Нәтижесінде ормандағы барлық ағаштар бір-біріне өте ұқсас болады — олар **корреляцияланады**. Бұдан ансамбльдің тиімділігі қатты төмендейді.\n",
    "\n",
    "**Шешім:** `max_features` параметрі бұл мәселені әрбір түйін үшін \"таңдауды\" жасанды түрде шектеу арқылы шешеді. Кезекті ағаштағы кезекті түйінді құру кезінде алгоритм мынаны орындайды:\n",
    "1.  Деректер жиынтығындағы **барлық** қолжетімді белгілерді алады.\n",
    "2.  Олардан `max_features` өлшеміндегі **кездейсоқ ішкі жиынды** таңдайды.\n",
    "3.  Ең жақсы сплитті **тек осы ішкі жиынның белгілері арасынан** іздейді.\n",
    "\n",
    "Бұл кейбір ағаштарды басым белгіні \"көрмей-ақ\" құрылуға мәжбүрлейді және оларды деректердегі басқа, айқын емес заңдылықтарды табуға итермелейді. Бұл — **декорреляция**.\n",
    "\n",
    "**Негізгі ымыра (Trade-off)**\n",
    "\n",
    "`max_features` таңдауы — бұл жеке ағаштардың күші мен олардың әртүрлілігі арасындағы тепе-теңдік:\n",
    "\n",
    "*   **`max_features`-тің кіші мәні:**\n",
    "    *   **Артықшылығы:** Ағаштар өте әртүрлі болып шығады (төмен корреляция). Бұл \"көпшілік даналығы\" үшін жақсы.\n",
    "    *   **Кемшілігі:** Жеке ағаштар өте \"әлсіз\" болуы мүмкін, себебі олардың маңызды белгілерге қол жеткізуі болмауы мүмкін. Олардың жеке болжамдық күші төмендейді.\n",
    "\n",
    "*   **`max_features`-тің үлкен мәні:**\n",
    "    *   **Артықшылығы:** Әрбір ағаш күшті болады, себебі оның барлық дерлік белгілерге қол жеткізуі бар.\n",
    "    *   **Кемшілігі:** Ағаштар бір-біріне өте ұқсас болып кетеді (жоғары корреляция), және ансамбль өз тиімділігін жоғалтады. Егер `max_features` жалпы белгілер санына тең болса, онда біз кәдімгі Бэггинг аламыз.\n",
    "\n",
    "**$\\sqrt{n}$ және $\\log_2(n)$ мәндері қайдан алынған?**\n",
    "\n",
    "Бұл мәндер қатаң математикалық теореманың нәтижесі емес, **Лео Брейман** өзінің Кездейсоқ Орман туралы түпнұсқа жұмысында ұсынған **эвристикалар** (тәжірибелік ережелер) болып табылады. Олар әртүрлі деректер жиындарында көптеген эксперименттер барысында эмпирикалық жолмен табылған және өздерін тамаша бастапқы нүкте ретінде көрсеткен.\n",
    "\n",
    "*   `max_features='sqrt'` (немесе $\\sqrt{n_{features}}$):\n",
    "    *   **Неліктен бұл жақсы?** Бұл жақсы ымыра. Таңдалатын белгілердің саны жалпы белгілер санымен бірге өседі, бірақ баяу өседі. Бұл ағаштардың әртүрлілігін сақтауға мүмкіндік береді, бірақ сонымен бірге оларға маңызды предикторларды \"көруге\" жеткілікті жақсы мүмкіндік береді.\n",
    "    *   **Қашан қолданылады?** Бұл `scikit-learn`-де **классификация есептері** үшін әдепкі мән болып табылады, және көп жағдайда оны өзгерту қажет емес.\n",
    "\n",
    "*   `max_features='log2'` (немесе $\\log_2(n_{features})$):\n",
    "    *   **Неліктен бұл жақсы?** Бұл одан да қатаң шектеу. Таңдалатын белгілердің саны өте баяу өседі.\n",
    "    *   **Қашан қолданылады?** Бұл нұсқа сізде **өте көп белгілер** (мыңдаған) болғанда және олардың көбі не ақпаратсыз (шу), не бір-бірімен қатты корреляцияланған деп күдіктенгенде пайдалы болуы мүмкін.\n",
    "\n",
    "*   `max_features=1.0` (немесе `n_features`):\n",
    "    *   **Бұл не?** Бұл кәдімгі Бэггингке тең. Алгоритм әр қадамда барлық белгілерді көреді.\n",
    "    *   **Қашан қолданылады?** `scikit-learn`-де **регрессия есептері** үшін әдепкі бойынша қолданылады. Регрессия үшін күштірек жеке ағаштардың болуы маңыздырақ деп саналады, және корреляция мәселесі соншалықты өткір емес."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-бөлім: Бустинг (Boosting) туралы қысқаша\n",
    "\n",
    "Бустинг — бұл бэггингтен түбегейлі ерекшеленетін ансамбльдеудің тағы бір қуатты тәсілі.\n",
    "\n",
    "| Бэггинг (Random Forest) | Бустинг (Gradient Boosting, AdaBoost) |\n",
    "| :--- | :--- |\n",
    "| Ағаштар **тәуелсіз** және **параллель** құрылады. | Ағаштар **бірізді** құрылады. |\n",
    "| Әрбір ағаш басқалары туралы \"білмейді\". | Әрбір келесі ағаш алдыңғысының **қателерінен үйренеді**. |\n",
    "| Мақсат — орташалау арқылы **дисперсияны (variance)** азайту. | Мақсат — қателерді түзету арқылы **ығысуды (bias)** азайту. |\n",
    "| Терең, артық оқытылған ағаштар қолданылады. | Өте қарапайым, \"әлсіз\" ағаштар (жиі тереңдігі 1-3 болатын \"түбіртектер\") қолданылады. |\n",
    "\n",
    "#### Бустинг интуициясы: Қателермен жұмыс\n",
    "\n",
    "**Интуиция:** Күрделі есепті шешу үшін сарапшылар тобын жинап жатырсыз делік. Оларды параллель жұмыс істеуге сұраудың орнына, сіз мұны бірізді түрде жасайсыз:\n",
    "1.  Бірінші сарапшы өз әрекетін жасайды.\n",
    "2.  Сіз оның қателеріне қарап, екінші сарапшыдан біріншісі сәтсіздікке ұшыраған жағдайларға назар аударуды сұрайсыз.\n",
    "3.  Үшінші сарапшы алғашқы екеуінің жұмысынан кейін қалған қателерге шоғырланады.\n",
    "4.  Осылайша жалғаса береді. Қорытынды шешім — барлық сарапшылардың салмақталған \"пікірі\".\n",
    "\n",
    "#### Мысал: AdaBoost (Adaptive Boosting) алгоритмі\n",
    "\n",
    "AdaBoost — бустингтің алғашқы және ең көрнекі алгоритмдерінің бірі.\n",
    "\n",
    "1.  **Бастапқы орнату:** Ең басында оқыту таңдамасының барлық объектілері бірдей **салмаққа** ие болады: $w_i = 1/N$.\n",
    "\n",
    "2.  **Итеративті құру:** Әрбір $t=1, \\dots, T$ қадамы үшін:\n",
    "    a. Ағымдағы $w_i$ салмақтарын ескере отырып, деректерде \"әлсіз\" модель (мысалы, ағаш-түбіртек) $b_t(x)$ оқытылады. Модель үлкен салмағы бар объектілерді дұрыс жіктеуге тырысады.\n",
    "    б. Осы модельдің $\\epsilon_t$ қатесі есептеледі.\n",
    "    в. Осы модельдің **\"дауыс салмағы\"** $\\alpha_t$ есептеледі. Модельдің қатесі неғұрлым аз болса, оның соңғы ансамбльдегі дауысы соғұрлым \"қатты\" болады.\n",
    "       $$ \\alpha_t = \\frac{1}{2} \\ln{\\frac{1 - \\epsilon_t}{\\epsilon_t}} $$\n",
    "    г. **Объектілердің салмақтары жаңартылады:**\n",
    "       *   **Дұрыс емес** жіктелген объектілердің салмақтары **артады**.\n",
    "       *   **Дұрыс** жіктелген объектілердің салмақтары **азаяды**.\n",
    "       Бұл келесі $b_{t+1}(x)$ моделін ең \"күрделі\" объектілерге назар аударуға мәжбүрлейді.\n",
    "\n",
    "3.  **Соңғы болжам:** Қорытынды модель — барлық әлсіз модельдердің салмақталған дауыс беруі.\n",
    "\n",
    "    $$ a(x) = \\text{sign} \\left( \\sum_{t=1}^{T} \\alpha_{t} b_{t}(x) \\right) $$\n",
    "\n",
    "**Gradient Boosting** бұл идеяны дамытады: әрбір келесі модель жай ғана қателерді емес, алдыңғы модельдің **шығын функциясының градиентін** болжауға үйренеді. Бұл жалпырақ және әдетте қуаттырақ тәсіл.\n",
    "\n",
    "Бұл өте қуатты, бірақ баптау мен түсінуге күрделірек әдістер. Танымал іске асырулар — Gradient Boosting, XGBoost, LightGBM. Біздің курс аясында біз оларды егжей-тегжейлі қарастырмаймыз, бірақ бұл әрі қарай зерттеу үшін маңызды бағыт."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![baggingboosting](https://raw.githubusercontent.com/yuliya-sabirova/ml-course/main/figs/baggingboosting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1-бөлім: Кездейсоқ Орманның \"тегін түскі асы\": неге ол ағаштар санының өсуімен артық оқытылмайды?\n",
    "\n",
    "Бір шешімдер ағашының басты мәселесін еске түсірейік: ағаш неғұрлым терең (күрделі) болса, артық оқытылу қаупі соғұрлым жоғары. Интуитивті түрде ансамбльге барған сайын күрделі, артық оқытылған ағаштарды қосу арқылы біз қорытынды модельді одан да артық оқытылған етуіміз керек сияқты көрінеді.\n",
    "\n",
    "Бірақ Кездейсоқ Орманмен таңғажайып нәрсе болады: **ағаштар санының (`n_estimators`) артуымен тесттік таңдамадағы қате (жалпылау қабілеті) өсуін тоқтатып, платоға шығады.**\n",
    "\n",
    "**Бұл неліктен орын алады?**\n",
    "\n",
    "Бұл әсер **Үлкен сандар заңының** салдары болып табылады. Ормандағы әрбір ағаш — бұл, шын мәнінде, өз болжамын жасайтын кездейсоқ шама. Оның жоғары ығысуы (variance) бар, яғни ол өзінің бутстрэп-таңдамасына қатты бейімделген.\n",
    "\n",
    "Біз көптеген **тәуелсіз (немесе әлсіз корреляцияланған)** кездейсоқ шамалардың болжамдарын орташалағанда, олардың жеке қателері (дисперсия) бір-бірін жоя бастайды.\n",
    "\n",
    "*   **Алғашқы ағаштар (1-ден ~20-ға дейін):** Әрбір жаңа ағаш көптеген жаңа, бірегей \"ақпарат\" әкеледі. Тесттік таңдамадағы қате күрт төмендейді.\n",
    "*   **Келесі ағаштар (~20-дан ~100-200-ге дейін):** Жаңа ағаштар әлі де өз үлесін қосып, нәтижені жақсартады, бірақ әсері соншалықты күшті емес. Қате біртіндеп төмендей береді.\n",
    "*   **Ағаштар санын одан әрі арттыру:** Ансамбль жеткілікті \"дана\" болады. Әрбір жаңа ағаш енді принципті түрде жаңа ақпарат қоспайды, тек жалпы көріністі аздап \"нақтылайды\". Тесттік таңдамадағы қате тұрақталып, **асимптотикалық платоға** шығады. Ол енді нашарламайды!\n",
    "\n",
    "**Қорытынды:** Күрделілікпен ымыраға келуді қажет ететін көптеген басқа модельдерден айырмашылығы, Кездейсоқ Ормандағы `n_estimators` үшін ереже қарапайым: **\"неғұрлым көп болса, соғұрлым жақсы\"** (немесе, кем дегенде, жаман емес). Жалғыз шектеу — оқыту уақыты.\n",
    "\n",
    "Мұны тәжірибеде көрейік."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Күрделірек деректер жиынтығын генерациялау\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=5,\n",
    "    n_redundant=0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Тексеруге арналған ағаштар санының диапазоны\n",
    "n_estimators_range = range(1, 201)\n",
    "\n",
    "# Қателерді сақтауға арналған тізімдер\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for n_estimators in n_estimators_range:\n",
    "    # Модельді ағымдағы ағаштар санымен оқыту\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Болжамдар жасау\n",
    "    train_preds = rf.predict(X_train)\n",
    "    test_preds = rf.predict(X_test)\n",
    "    \n",
    "    # Қатені есептеу (1 - accuracy) және тізімдерге қосу\n",
    "    train_errors.append(1 - accuracy_score(y_train, train_preds))\n",
    "    test_errors.append(1 - accuracy_score(y_test, test_preds))\n",
    "\n",
    "# График салу\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(n_estimators_range, train_errors, label=\"Оқыту таңдамасындағы қате (Train Error)\")\n",
    "plt.plot(n_estimators_range, test_errors, label=\"Тесттік таңдамадағы қате (Test Error)\", linewidth=2)\n",
    "\n",
    "plt.ylabel(\"Қате деңгейі (1 - Accuracy)\")\n",
    "plt.xlabel(\"Ағаштар саны (n_estimators)\")\n",
    "plt.title(\"Кездейсоқ Ормандағы ағаштар санына қателіктің тәуелділігі\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-бөлім: Кездейсоқ Орман іс жүзінде (практикалық мысал)\n",
    "\n",
    "Бір ағаш пен кездейсоқ орманның бөлу беттерін бірдей деректерде салыстырайық."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Есеп күрделі болуы үшін жоғары шу деңгейі бар деректерді генерациялаймыз\n",
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "\n",
    "# Деректерді оқыту және тесттік таңдамаларға бөлеміз\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# --- ТҮЗЕТУ: Модельдерді ТЕК оқыту таңдамасында оқытамыз ---\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# --- Бөлу беттерін визуализациялау ---\n",
    "def plot_boundary(ax, model, X_data, y_data, title):\n",
    "    x_min, x_max = X_data[:, 0].min() - 0.5, X_data[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_data[:, 1].min() - 0.5, X_data[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='winter')\n",
    "    # Модельдердің оларды қалай жіктейтінін көру үшін тек тесттік нүктелерді көрсетеміз\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=30, edgecolor='k', cmap='winter')\n",
    "    ax.set_title(title)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "plot_boundary(axes[0], tree_clf, X, y, \"Бір Шешімдер Ағашы\")\n",
    "plot_boundary(axes[1], rf_clf, X, y, \"Кездейсоқ Орман (100 ағаш)\")\n",
    "plt.suptitle(\"Бөлу беттерін салыстыру (тесттік деректерде)\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Модельдерді тесттік таңдамада бағалау ---\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Classification Report: Бір Шешімдер Ағашы\")\n",
    "print(classification_report(y_test, y_pred_tree))\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Classification Report: Кездейсоқ Орман\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "# --- Қателер матрицалары ---\n",
    "cm_tree = confusion_matrix(y_test, y_pred_tree)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "disp_tree = ConfusionMatrixDisplay(confusion_matrix=cm_tree)\n",
    "disp_tree.plot(ax=axes[0], cmap='Blues', values_format='d')\n",
    "axes[0].set_title(\"Қателер матрицасы - Ағаш\")\n",
    "\n",
    "disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf)\n",
    "disp_rf.plot(ax=axes[1], cmap='Blues', values_format='d')\n",
    "axes[1].set_title(\"Қателер матрицасы - Кездейсоқ орман\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Жоғары шу деңгейі бар деректерді генерациялау\n",
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "\n",
    "# Деректерді оқыту және тесттік таңдамаларға бөлу\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 1. \"Жеке тұрған\" Шешімдер Ағашын оқыту\n",
    "# Ол артық оқытылуы үшін тереңдігін шектемейміз\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# 2. Кездейсоқ Орманды оқыту\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# --- АҒАШТАРДЫ ВИЗУАЛИЗАЦИЯЛАУ ---\n",
    "\n",
    "# 3. \"Жеке тұрған\" Шешімдер Ағашын салу\n",
    "# График оқылатын болуы үшін көрсету тереңдігін шектейміз (max_depth=3),\n",
    "# бірақ шын мәнінде ағаштың әлдеқайда терең екенін ұмытпаймыз.\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(tree_clf, \n",
    "          max_depth=3, # Тек визуализация үшін шектеу\n",
    "          feature_names=['feature_0', 'feature_1'], \n",
    "          class_names=['Класс 0', 'Класс 1'],\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(\"'Жеке тұрған' Шешімдер Ағашының жоғарғы деңгейлері\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 4. Орманнан бір ағашты \"алып шығып\", оны салу\n",
    "# rf_clf.estimators_ - бұл ормандағы барлық ағаштардың тізімі. Біріншісін алайық.\n",
    "first_tree_from_forest = rf_clf.estimators_[0]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(first_tree_from_forest, \n",
    "          max_depth=3, # Тек визуализация үшін шектеу\n",
    "          feature_names=['feature_0', 'feature_1'],\n",
    "          class_names=['Класс 0', 'Класс 1'],\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(\"Кездейсоқ Орманнан АЛЫНҒАН бір ағаштың жоғарғы деңгейлері\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Қорытынды:** Бір ағаштың шекарасы — \"жыртылған\" және күрделі, ол әрбір нүктеге мінсіз бейімделуге тырысады. Кездейсоқ Орманның шекарасы — әлдеқайда тегіс және жалпыланған. Ол шуды емес, деректердегі жалпы тенденцияны көрсетеді. Кездейсоқ орманнан алынған бір ағаштың құрылымы, бутстрэпті қолдану мен әрбір түйінде белгілерді кездейсоқ таңдау себебінен, барлық деректерде оқытылған ағаштың құрылымынан айтарлықтай ерекшеленеді. Дәл осы ағаштар арасындағы мәжбүрлі \"пікірлердің әртүрлілігі\" ансамбльге жоғары дәлдікке және артық оқытылуға төзімділікке қол жеткізуге мүмкіндік береді."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-бөлім: Кездейсоқ Орманның Артықшылықтары мен Кемшіліктері\n",
    "\n",
    "#### Артықшылықтары:\n",
    "*   **Болжамдардың жоғары дәлдігі:** Көптеген есептерде сызықтық алгоритмдерден жақсы жұмыс істейді; дәлдігі бустингтің дәлдігімен салыстырмалы.\n",
    "*   **Шығарындылар мен шуға төзімділік:** Бутстрэп әдісімен таңдамаларды кездейсоқ сэмплингтеу арқылы деректердегі шығарындыларға іс жүзінде сезімтал емес.\n",
    "*   **Масштабтауға** және белгі мәндерінің басқа монотонды түрлендірулеріне **сезімтал еместігі**.\n",
    "*   **«Қораптан шыққандай» жұмыс істейді:** Жақсы нәтиже алу үшін параметрлерді мұқият баптауды қажет етпейді.\n",
    "*   **Көптеген белгілермен және кластармен тиімді жұмыс істейді.**\n",
    "*   **Сирек артық оқытылады:** Тәжірибеде ағаштарды қосу композицияны әрқашан дерлік жақсартады.\n",
    "*   **Жоғалған деректермен жақсы жұмыс істейді.**\n",
    "*   **Параллельдеу** және масштабтау оңай.\n",
    "\n",
    "#### Кемшіліктері:\n",
    "*   **Интерпретациялаудың күрделілігі:** Бір ағаштан айырмашылығы, кездейсоқ орманның нәтижелерін интерпретациялау қиынырақ.\n",
    "*   **Сирек деректермен проблемалар:** Таңдамада өте көп сирек белгілер болғанда (мысалы, мәтіндерді талдау есептерінде) көптеген сызықтық әдістерден нашар жұмыс істейді.\n",
    "*   **Экстраполяция жасай алмайды:** Жалғыз ағаштар сияқты, орман оқыту таңдамасында көрген диапазонынан тыс мәндерді болжай алмайды.\n",
    "*   **Категориялық белгілерге қатысты бейімділік:** Көптеген деңгейлері (категориялары) бар категориялық белгілерге артықшылық беруге бейім.\n",
    "*   **Модельдің үлкен өлшемі:** Сақтау үшін O(N·K) жады қажет, мұндағы N - ағаштар саны, ал K - олардағы түйіндер саны."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
